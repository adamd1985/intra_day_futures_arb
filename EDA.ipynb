{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","id":"oaDoHbxVH0CW"},"source":["# Data Cleanup for Various Future Datafeeds"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","id":"z_cBqdYOoY5S"},"source":["# Notebook's Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","collapsed":false,"id":"eETPYJLiMU-b","jupyter":{"outputs_hidden":false},"outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"outputs":[],"source":["INSTALL_DEPS = False\n","if INSTALL_DEPS:\n","  %pip installnumpy==1.26.4\n","  %pip installpandas==2.2.1\n","  %pip installpandas_market_calendars==4.4.0\n","  %pip installpytz==2024.1\n","  %pip installscipy==1.12.0\n","\n","!python --version"]},{"cell_type":"markdown","metadata":{},"source":["# Cloud Environment Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","collapsed":false,"id":"Q4-GoceIIfT_","jupyter":{"outputs_hidden":false},"outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","import pandas as pd\n","import numpy as np\n","import glob\n","from datetime import datetime\n","from pandas.tseries.offsets import BDay, Day\n","import pandas_market_calendars as mcal\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from tqdm import tqdm\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","CLEAN_DATA_PATH = f\".{os.sep}data\"\n","DATA_PATH = f\".{os.sep}data{os.sep}unstructureddata\"\n","print(\"running localhost!\")"]},{"cell_type":"markdown","metadata":{},"source":["# Get Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class StockFeat:\n","    DATETIME = \"Datetime\"\n","    OPEN = \"Open\"\n","    HIGH = \"High\"\n","    LOW = \"Low\"\n","    CLOSE = \"Close\"\n","    VOLUME = \"Volume\"\n","    list = [OPEN, HIGH, LOW, CLOSE, VOLUME]\n","\n","class YFinanceOptions:\n","    INDEX = \"Datetime\"\n","    MIN1_RANGE = 7 - 1\n","    MIN15_RANGE = 60 - 1\n","    HOUR_RANGE = 730 - 1\n","    DAY_RANGE = 7300 - 1\n","    D1=\"1d\"\n","    H1=\"1h\"\n","    M15=\"15m\"\n","    M1=\"1m\"\n","    DATE_TIME_FORMAT = \"%Y-%m-%d\"\n","    DATE_TIME_HRS_FORMAT = '%Y-%m-%d %H:%M:%S %Z'\n","\n","INTERVAL = YFinanceOptions.M15\n","DATE_TIME_FORMAT = \"%Y-%m-%d\"\n","END_DATE = pd.Timestamp(datetime.now() - Day(1)).strftime(DATE_TIME_FORMAT)\n","START_DATE =  pd.Timestamp(datetime.now() - Day(YFinanceOptions.MIN15_RANGE)).strftime(DATE_TIME_FORMAT)\n","\n","SNP_FUT = \"ES=F\"\n","NSDQ_FUT = \"NQ=F\"\n","GOLD_FUT = \"GC=F\"\n","CRUDOIL_FUT=\"CL=F\"\n","VOLATILITY_FUT= \"^VIX\"\n","RUS_FUT = \"RTY=F\"\n","RATES_FUT = \"2YY=F\"\n","\n","CORN_FUT = \"ZC=F\"\n","SOYOIL_FUT = \"ZL=F\"\n","KCWHEAT_FUT = \"KE=F\"\n","SOYBEAN_FUT = \"ZS=F\"\n","SOYBEANMEAL_FUT = \"ZM=F\"\n","WHEAT_FUT = \"ZW=F\"\n","LIVECATTLE_FUT = \"LE=F\"\n","LEANHOG_FUT = \"HE=F\"\n","FEEDERCATTLE_FUT = \"GF=F\"\n","MILK_FUT = \"DC=F\"\n","\n","TICKER_SYMBOLS = [RATES_FUT, VOLATILITY_FUT, GOLD_FUT, CRUDOIL_FUT, SNP_FUT, RUS_FUT, CORN_FUT, SOYOIL_FUT, KCWHEAT_FUT, SOYBEAN_FUT, SOYBEANMEAL_FUT, WHEAT_FUT, LIVECATTLE_FUT, LEANHOG_FUT, FEEDERCATTLE_FUT, MILK_FUT]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yfinance as yf\n","from scipy.stats import skew, kurtosis\n","\n","def get_yf_tickers_df(tickers_symbols, start, end, interval=INTERVAL, datadir=DATA_PATH):\n","    tickers = {}\n","    earliest_end= pd.to_datetime(datetime.strptime(end,YFinanceOptions.DATE_TIME_FORMAT)).tz_localize(\"UTC\")\n","    latest_start = pd.to_datetime(datetime.strptime(start,YFinanceOptions.DATE_TIME_FORMAT)).tz_localize(\"UTC\")\n","    os.makedirs(datadir, exist_ok=True)\n","    for symbol in tickers_symbols:\n","        cached_file_path = f\"{datadir}/{symbol}-{start.split(' ')[0]}-{end.split(' ')[0]}-{interval}.csv\"\n","        print(f\"Checking file: {cached_file_path}\")\n","        if os.path.exists(cached_file_path):\n","            print(f\"loading from {cached_file_path}\")\n","            df = pd.read_csv(cached_file_path, parse_dates= True, index_col=0)\n","            try:\n","                df.index = pd.to_datetime(df.index).tz_localize('US/Central').tz_convert('UTC')\n","            except Exception as e:\n","                df.index = pd.to_datetime(df.index).tz_convert('UTC')\n","            assert len(df) > 0, \"Empty data\"\n","        else:\n","            df = yf.download(\n","                symbol,\n","                start=start,\n","                end=end,\n","                progress=True,\n","                interval=interval\n","            )\n","            assert len(df) > 0, \"No data pulled\"\n","            try:\n","                df.index = pd.to_datetime(df.index).tz_localize('US/Central').tz_convert('UTC')\n","            except Exception as e:\n","                df.index = pd.to_datetime(df.index).tz_convert('UTC')\n","        min_date = df.index.min()\n","        max_date = df.index.max()\n","        nan_count = df[\"Close\"].isnull().sum()\n","        skewness = round(skew(df[\"Close\"].dropna()), 2)\n","        kurt = round(kurtosis(df[\"Close\"].dropna()), 2)\n","        outliers_count = (df[\"Close\"] > df[\"Close\"].mean() + (3 * df[\"Close\"].std())).sum()\n","        print(\n","            f\"{symbol} => min_date: {min_date}, max_date: {max_date}, kurt:{kurt}, skewness:{skewness}, outliers_count:{outliers_count},  nan_count: {nan_count}\"\n","        )\n","        tickers[symbol] = df\n","\n","        if min_date > latest_start:\n","            latest_start = min_date\n","        if max_date < earliest_end:\n","            earliest_end = max_date\n","\n","    nyse = mcal.get_calendar('CME_Agriculture')\n","    schedule = nyse.schedule(start_date=latest_start, end_date=earliest_end)\n","    all_trading_days = mcal.date_range(schedule, frequency='1T')\n","\n","    for symbol, df in tickers.items():\n","        df_filtered = df[(df.index >= latest_start) & (df.index <= earliest_end)]\n","        df_reindexed = df_filtered.reindex(all_trading_days, method='nearest')\n","        df_reindexed.index = pd.to_datetime(df_reindexed.index)\n","        df_reindexed = df_reindexed[~df_reindexed.index.duplicated(keep='first')]\n","        df_reindexed.index.name = 'Date'\n","        tickers[symbol] = df_reindexed\n","\n","\n","        cached_file_path = f\"{datadir}/{symbol}-{start.split(' ')[0]}-{end.split(' ')[0]}-{interval}.csv\"\n","        if not os.path.exists(cached_file_path):\n","            df_reindexed.to_csv(cached_file_path, index=True)\n","\n","    return tickers, latest_start, earliest_end\n","\n","tickers, latest_start, earliest_end = get_yf_tickers_df(TICKER_SYMBOLS, start=START_DATE, end=END_DATE)"]},{"cell_type":"markdown","metadata":{},"source":["# Clean Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_prefix(filename):\n","    prefix = filename.split(os.sep)[-1].split(\"-\")[0].replace(\"=F\", \"\")\n","    return prefix\n","\n","\n","files = glob.glob(DATA_PATH + f\"{os.sep}*-{INTERVAL}.csv\")\n","assert files and len(files) > 0\n","\n","fut_tickers = []\n","df_list = []\n","data_quality_metrics = []\n","\n","for f in files:\n","    print(f)\n","    prefix = get_prefix(f)\n","    fut_tickers.append(prefix)\n","    df_temp = pd.read_csv(f, index_col=\"Date\", parse_dates=True)\n","    # TODO: Use bid ask here.\n","    df_temp[\"Spread\"] = df_temp[\"High\"] - df_temp[\"Low\"]\n","\n","    df_temp.columns = [prefix + \"_\" + col for col in df_temp.columns]\n","    df_temp = df_temp.apply(\n","        pd.to_numeric, errors=\"coerce\"\n","    )  # Force conversion to numeric and coerce errors to NaN\n","\n","\n","    df_list.append(df_temp)\n","    for col in tqdm(df_temp.columns):\n","        metrics = {\n","            \"Column\": col,\n","            \"Total NaNs\": df_temp[col].isnull().sum(),\n","            \"Skewness\": df_temp[col].skew(),\n","            \"Kurtosis\": df_temp[col].kurtosis(),\n","            \"Mean\": df_temp[col].mean(),\n","            \"Standard Deviation\": df_temp[col].std(),\n","            \"Min\": df_temp[col].min(),\n","            \"Max\": df_temp[col].max(),\n","        }\n","        data_quality_metrics.append(metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["futs_df = pd.concat(df_list, axis=1)\n","try:\n","    futs_df.index = futs_df.index.tz_localize(\"GMT\")\n","except Exception as e:\n","    print(e)\n","    # Probably already TZ aware\n","futs_df.sort_index(inplace=True)\n","futs_df = futs_df.iloc[futs_df.notnull().all(axis=1).argmax() :]\n","futs_df.interpolate(method=\"time\", inplace=True)\n","\n","assert not futs_df.isnull().any().any()\n","print(fut_tickers)\n","print(f\"Dataset Shape: {futs_df.shape}\")\n","\n","futs_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PRICE_COLS =[\"Close\", \"Open\", \"High\", \"Low\", \"Spread\"]\n","def create_fut_rets_df(df, price_types = PRICE_COLS):\n","    log_returns_data = {}\n","    pct_change_data = {}\n","\n","    for price_type in tqdm(price_types):\n","        for column in tqdm(df.columns):\n","            if price_type in column:\n","                log_return_col_name = f\"{column}_lr\"\n","                pct_change_col_name = f\"{column}_pc\"\n","                log_returns_data[log_return_col_name] = np.log(\n","                    df[column] / df[column].shift(1)\n","                )\n","                pct_change_data[pct_change_col_name] = df[column].pct_change() * 100\n","    log_fut_rets_df = pd.DataFrame(log_returns_data, index=df.index).bfill()\n","    pct_changes_df = pd.DataFrame(pct_change_data, index=df.index).bfill()\n","    combined_df = pd.concat([log_fut_rets_df, pct_changes_df], axis=1)\n","\n","    return combined_df\n","\n","fut_rets_df = create_fut_rets_df(futs_df)\n","fut_rets_df.head(2)"]},{"cell_type":"markdown","metadata":{},"source":["# EDA\n","\n","## Skew, Kurosis and Outliers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_to_plot = fut_rets_df.filter(regex=\"^.*Close.*\")\n","plt.figure(figsize=(22, 12))\n","ax = sns.violinplot(data=data_to_plot, orient='h', scale='width')\n","\n","plt.title(\"Violin Plot of Returns\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["metrics_df = pd.DataFrame(data_quality_metrics)\n","metrics_df"]},{"cell_type":"markdown","metadata":{},"source":["## Data Mining for Correlations\n","\n","Current findings:\n","| Pairing, lags               | Correlation | P-Value       |\n","|-----------------------------|-------------|---------------|\n","| (ES_Close, GC_Close), 1     | 0.016987    | 1.152765e-08  |\n","| (ES_Close, NQ_Close), 1     | 0.016356    | 3.919657e-08  |\n","| (GC_Close, NQ_Close), 1     | 0.014338    | 1.459589e-06  |\n","| (CL_Close, GC_Close), 1     | 0.006259    | 3.551049e-02  |\n","| (2YY_Close, ES_Close), 5    | -0.008258   | 5.538042e-03  |\n","| (2YY_Close, GC_Close), 5    | -0.009686   | 1.139459e-03  |\n","| (2YY_Close, NQ_Close), 5    | -0.009862   | 9.234257e-04  |\n","| (2YY_Close, NQ_Close), 1    | -0.033188   | 7.051777e-29  |\n","| (2YY_Close, ES_Close), 1    | -0.033695   | 1.026262e-29  |\n","| (2YY_Close, GC_Close), 1    | -0.039427   | 4.567548e-40  |\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from statsmodels.tsa.stattools import adfuller\n","from scipy.stats import pearsonr\n","from itertools import combinations\n","\n","LAGS_IN_MINS = [1, 5] # [1, 5, 15, 60, 240, 480]\n","MAX_DIFF = 2\n","\n","\n","def make_stationary(series, max_diff=MAX_DIFF):\n","    result = adfuller(series.dropna(), autolag=\"AIC\")\n","    p_value = result[1]\n","    if p_value < 0.05:\n","        return series, 0\n","\n","    diff_count = 0\n","    differenced_series = series.copy()\n","    while p_value >= 0.05 and diff_count < max_diff:\n","        differenced_series = differenced_series.diff().dropna()\n","        result = adfuller(differenced_series, autolag=\"AIC\")\n","        p_value = result[1]\n","        diff_count += 1\n","        if p_value < 0.05:\n","            break\n","\n","    return differenced_series, diff_count\n","\n","\n","def calculate_correlations(data, lags=LAGS_IN_MINS):\n","    correlations = {}\n","    rets = data.filter(regex=\"(_Close(_lr|_pc)?)$\")\n","    for col1, col2 in tqdm(combinations(rets.columns, 2), desc=\"calculate_correlations\"):\n","        series1, _ = make_stationary(rets[col1])\n","        series2, _ = make_stationary(rets[col2])\n","\n","        for lag in tqdm(lags, desc=f'calculate_correlations {col1} vs {col2}'):\n","            lagged_series2 = series2.shift(lag).dropna()\n","            truncated_series1 = series1.iloc[lag:]\n","\n","            if len(truncated_series1) == 0 or len(truncated_series1) != len(lagged_series2):\n","                continue\n","\n","            correlation, p_value = pearsonr(truncated_series1, lagged_series2)\n","            if p_value < 0.05:\n","                correlations[((col1, col2), lag)] = (correlation, p_value)\n","\n","    return correlations\n","\n","GET_CORR = False\n","if GET_CORR:\n","    # Also takes time\n","    correlation_results = calculate_correlations(futs_df)\n","    if len(correlation_results) > 0:\n","        correlation_df = pd.DataFrame.from_dict(\n","            correlation_results, orient=\"index\", columns=[\"Correlation\", \"P-Value\"]\n","        )\n","        correlation_df = correlation_df.sort_values(by=\"Correlation\", ascending=False)\n","\n","        print(correlation_df)\n","    else:\n","        print(\"No meaningful coorelations\")"]},{"cell_type":"markdown","metadata":{},"source":["## Autocorrelation\n","\n","Current results:\n","\n","| Series     | Lag | Coefficient | P-Value          |\n","|------------|-----|-------------|------------------|\n","| NQ_High    | 1   | 0.999971    | 0.000000e+00     |\n","| NQ_Low     | 1   | 0.999969    | 0.000000e+00     |\n","| NQ_Open    | 1   | 0.999967    | 0.000000e+00     |\n","| NQ_Close   | 1   | 0.999967    | 0.000000e+00     |\n","| NQ_Volume  | 1   | 0.821516    | 0.000000e+00     |\n","| NQ_Spread  | 1   | 0.727226    | 0.000000e+00     |\n","| NQ_Open    | 5   | -0.017075   | 2.064916e-27     |\n","| NQ_Close   | 5   | -0.017068   | 8.802106e-30     |\n","| NQ_High    | 5   | -0.019068   | 7.274848e-44     |\n","| NQ_Volume  | 5   | 0.116937    | 0.000000e+00     |\n","| NQ_Spread  | 5   | 0.139868    | 0.000000e+00     |\n","| NQ_Low     | 5   | NaN         | NaN              |\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from statsmodels.tsa.arima.model import ARIMA\n","\n","def get_ar_stats(df):\n","    # this to avoid the ARIMA warnings\n","    df.index = pd.DatetimeIndex(df.index).to_period('T')\n","\n","    def test_autoregression(series, lags=LAGS_IN_MINS):\n","        results = {}\n","        for lag in tqdm(lags):\n","            try:\n","                model = ARIMA(series, order=(lag, 0, 0))\n","                fitted_model = model.fit()\n","                coef = fitted_model.params.get(f'ar.L{lag}')\n","                p_value = fitted_model.pvalues.get(f'ar.L{lag}')\n","                if p_value < 0.05:\n","                    results[lag] = {'Coefficient': coef, 'P-Value': p_value}\n","                else:\n","                    raise Exception(\"Not AR\")\n","            except Exception as e:\n","                results[lag] = {'Coefficient': None, 'P-Value': None}\n","        return results\n","\n","    ar_results = {}\n","    for col in tqdm(df.columns, desc=\"get_ar_stats\"):\n","        ar_results[col] = test_autoregression(df[col].dropna())\n","\n","    ar_df = pd.DataFrame.from_dict({(i, j): ar_results[i][j]\n","                                    for i in ar_results.keys()\n","                                    for j in ar_results[i].keys()},\n","                                orient='index')\n","    return ar_df\n","\n","if GET_CORR:\n","    # This takes time\n","    ar_df = get_ar_stats(futs_df.filter(regex='^NQ'))\n","    ar_df"]},{"cell_type":"markdown","metadata":{},"source":["## Cummulative Returns Visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for c in fut_rets_df.columns:\n","    if c.endswith(\"_Close_pc\"):\n","        fut_rets_df[c + \"_c\"] = fut_rets_df[c].cumsum()\n","\n","# Plotting\n","plt.figure(figsize=(22, 12))\n","for c in fut_rets_df.columns:\n","    if c.endswith(\"_Close_pc_c\"):\n","        plt.plot(fut_rets_df.index, fut_rets_df[c], label=c.replace(\"_Close_pc_c\", \"\"), alpha=0.65)\n","plt.title(\"% Cummulative\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Distributions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import scipy.stats\n","from scipy.stats import norm\n","\n","def plot_norm(df):\n","    columns = df.filter(regex=\"(_Close(_lr)?)$\").columns\n","    n_cols = len(columns)\n","    n_cols_adjusted = n_cols // 2 + (n_cols % 2)\n","    fig, axes = plt.subplots(nrows=n_cols_adjusted // 2, ncols=n_cols_adjusted //2, figsize=(22, 18))\n","\n","    axes = axes.flatten()\n","    plt.title(\"Log Returns Gaussian Fit\")\n","    for ax, column in zip(axes, columns):\n","        data = df[column].dropna()\n","        sns.kdeplot(\n","            data,\n","            fill=True,\n","            common_norm=False,\n","            alpha=0.5,\n","            linewidth=0.5,\n","            ax=ax\n","        )\n","\n","        params = norm.fit(data)\n","        ks_stat, ks_p_value = scipy.stats.kstest(data, norm.name, args=params)\n","\n","        mu, std = params\n","        xmin, xmax = ax.get_xlim()\n","\n","        x = np.linspace(xmin, xmax, 100)\n","        p = norm.pdf(x, mu, std)\n","        ax.plot(x, p, 'k', linewidth=2)\n","        title = f\"{column} Gaussian Fit (ks {ks_stat:.02f}, ks_p {ks_p_value:.02f}) \\nMean = {mu:.04f}, Std = {std:.04f}\"\n","        ax.set_title(title)\n","        ax.set_xlabel(\"Log Return Value\")\n","        ax.set_ylabel(\"Density\")\n","\n","    for ax in axes[len(columns):]:\n","        ax.set_visible(False)\n","\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_norm(fut_rets_df)"]},{"cell_type":"markdown","metadata":{},"source":["# Save For Models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["futs_df.to_csv(CLEAN_DATA_PATH + f\"{os.sep}futures_1min_data.csv\")"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":4755137,"sourceId":8061237,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
