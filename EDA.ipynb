{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","id":"oaDoHbxVH0CW"},"source":["# Data Cleanup for Various Future Datafeeds"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","id":"z_cBqdYOoY5S"},"source":["# Notebook's Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","collapsed":false,"id":"eETPYJLiMU-b","jupyter":{"outputs_hidden":false},"outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"outputs":[],"source":["INSTALL_DEPS = False\n","if INSTALL_DEPS:\n","  %pip installnumpy==1.26.4\n","  %pip installpandas==2.2.1\n","  %pip installpandas_market_calendars==4.4.0\n","  %pip installpytz==2024.1\n","  %pip installscipy==1.12.0\n","\n","!python --version"]},{"cell_type":"markdown","metadata":{},"source":["# Cloud Environment Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","collapsed":false,"id":"Q4-GoceIIfT_","jupyter":{"outputs_hidden":false},"outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","import pandas as pd\n","import numpy as np\n","import glob\n","from datetime import datetime\n","from pandas.tseries.offsets import BDay, Day\n","import pandas_market_calendars as mcal\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from tqdm import tqdm\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","CLEAN_DATA_PATH = f\".{os.sep}data\"\n","DATA_PATH = f\".{os.sep}data{os.sep}unstructureddata\"\n","print(\"running localhost!\")"]},{"cell_type":"markdown","metadata":{},"source":["# Get Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from constants import *\n","\n","class StockFeat:\n","    DATETIME = \"Datetime\"\n","    OPEN = \"Open\"\n","    HIGH = \"High\"\n","    LOW = \"Low\"\n","    CLOSE = \"Close\"\n","    VOLUME = \"Volume\"\n","    list = [OPEN, HIGH, LOW, CLOSE, VOLUME]\n","\n","\n","PERIOD_PD_FREQ = {\n","    YFinanceOptions.M1: '1T',\n","    YFinanceOptions.M15: '15T',\n","}\n","\n","INTERVAL = YFinanceOptions.M15\n","DATE_TIME_FORMAT = \"%Y-%m-%d\"\n","END_DATE = pd.Timestamp(datetime.now()).strftime(DATE_TIME_FORMAT)\n","START_DATE =  pd.Timestamp(datetime.now() - Day(YFinanceOptions.MIN15_RANGE)).strftime(DATE_TIME_FORMAT)\n","\n","FUTS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yfinance as yf\n","from scipy.stats import skew, kurtosis\n","\n","def get_yf_tickers_df(tickers_symbols, start, end, interval=INTERVAL, datadir=DATA_PATH):\n","    tickers = {}\n","    earliest_end= pd.to_datetime(datetime.strptime(end,YFinanceOptions.DATE_TIME_FORMAT)).tz_localize(\"UTC\")\n","    latest_start = pd.to_datetime(datetime.strptime(start,YFinanceOptions.DATE_TIME_FORMAT)).tz_localize(\"UTC\")\n","    os.makedirs(datadir, exist_ok=True)\n","    for symbol in tickers_symbols:\n","        cached_file_path = f\"{datadir}/{symbol}-{start.split(' ')[0]}-{end.split(' ')[0]}-{interval}.csv\"\n","        print(f\"Checking file: {cached_file_path}\")\n","        if os.path.exists(cached_file_path):\n","            print(f\"loading from {cached_file_path}\")\n","            df = pd.read_csv(cached_file_path, parse_dates= True, index_col=0)\n","            try:\n","                df.index = pd.to_datetime(df.index).tz_localize('US/Central').tz_convert('UTC')\n","            except Exception as e:\n","                df.index = pd.to_datetime(df.index).tz_convert('UTC')\n","            assert len(df) > 0, \"Empty data\"\n","        else:\n","            df = yf.download(\n","                symbol,\n","                start=start,\n","                end=end,\n","                progress=True,\n","                interval=interval\n","            )\n","            assert len(df) > 0, \"No data pulled\"\n","            try:\n","                df.index = pd.to_datetime(df.index).tz_localize('US/Central').tz_convert('UTC')\n","            except Exception as e:\n","                df.index = pd.to_datetime(df.index).tz_convert('UTC')\n","        # Use adjusted close if available.\n","        if 'Adj Close' in df.columns:\n","            assert 'Close' in df.columns\n","            df.drop(columns=['Adj Close'], inplace=True)\n","            # df.rename(columns={'Adj Close': 'Close'}, inplace=True)\n","        min_date = df.index.min()\n","        max_date = df.index.max()\n","        nan_count = df[\"Close\"].isnull().sum()\n","        skewness = round(skew(df[\"Close\"].dropna()), 2)\n","        kurt = round(kurtosis(df[\"Close\"].dropna()), 2)\n","        outliers_count = (df[\"Close\"] > df[\"Close\"].mean() + (3 * df[\"Close\"].std())).sum()\n","        print(\n","            f\"{symbol} => min_date: {min_date}, max_date: {max_date}, kurt:{kurt}, skewness:{skewness}, outliers_count:{outliers_count},  nan_count: {nan_count}\"\n","        )\n","        tickers[symbol] = df\n","\n","        if min_date > latest_start:\n","            latest_start = min_date\n","        if max_date < earliest_end:\n","            earliest_end = max_date\n","\n","    nyse = mcal.get_calendar('CME_Agriculture')\n","    schedule = nyse.schedule(start_date=latest_start, end_date=earliest_end)\n","    all_trading_days = mcal.date_range(schedule, frequency=PERIOD_PD_FREQ[interval], tz='UTC', normalize=True)\n","\n","    for symbol, df in tickers.items():\n","        df_filtered = df[(df.index >= latest_start) & (df.index <= earliest_end)]\n","        df_reindexed = df_filtered.reindex(all_trading_days, method='nearest')\n","        df_reindexed.index = pd.to_datetime(df_reindexed.index)\n","        df_reindexed = df_reindexed[~df_reindexed.index.duplicated(keep='first')]\n","        df_reindexed.index.name = 'Date'\n","        df_reindexed = df_reindexed.bfill().ffill()\n","        tickers[symbol] = df_reindexed\n","\n","\n","        cached_file_path = f\"{datadir}/{symbol}-{start.split(' ')[0]}-{end.split(' ')[0]}-{interval}.csv\"\n","        if not os.path.exists(cached_file_path):\n","            df_reindexed.to_csv(cached_file_path, index=True)\n","\n","    return tickers, latest_start, earliest_end\n","\n","tickers, latest_start, earliest_end = get_yf_tickers_df(FUTS, start=START_DATE, end=END_DATE)"]},{"cell_type":"markdown","metadata":{},"source":["De-duplicate and combine dataframes."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for ticker in FUTS:\n","    file_pattern = f'{ticker}*.csv'\n","    files = glob.glob(f\"{DATA_PATH}{os.sep}{ticker}*-{INTERVAL}.csv\")\n","    assert files and len(files) > 0\n","\n","    dataframes = []\n","    for file in files:\n","        df = pd.read_csv(file, index_col=0, parse_dates=True)\n","        dataframes.append(df)\n","\n","    combined_df = pd.concat(dataframes)\n","    combined_df = combined_df[~combined_df.index.duplicated(keep='first')]\n","    combined_df.sort_index(inplace=True)\n","    combined_df.to_csv(f'{DATA_PATH}{os.sep}{ticker}-combined-{INTERVAL}.csv')"]},{"cell_type":"markdown","metadata":{},"source":["# Clean Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_prefix(filename):\n","    prefix = filename.split(os.sep)[-1].split(\"-\")[0].replace(\"=F\", \"\")\n","    return prefix\n","\n","files = glob.glob(f'{DATA_PATH}{os.sep}*-combined-{INTERVAL}.csv')\n","assert files and len(files) > 0\n","\n","fut_tickers = []\n","df_list = []\n","\n","for f in files:\n","    print(f)\n","    prefix = get_prefix(f)\n","    fut_tickers.append(prefix)\n","    df_temp = pd.read_csv(f, index_col=\"Date\", parse_dates=True)\n","    # TODO: Use bid ask here.\n","    df_temp[\"Spread\"] = df_temp[\"High\"] - df_temp[\"Low\"]\n","\n","    df_temp.columns = [prefix + \"_\" + col for col in df_temp.columns]\n","    df_temp = df_temp.apply(\n","        pd.to_numeric, errors=\"coerce\"\n","    )\n","\n","    df_list.append(df_temp)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["futs_df = pd.concat(df_list, axis=1)\n","try:\n","    futs_df.index = futs_df.index.tz_localize(\"GMT\")\n","except Exception as e:\n","    print(e)\n","    # Probably already TZ aware\n","futs_df.sort_index(inplace=True)\n","futs_df = futs_df.iloc[futs_df.notnull().all(axis=1).argmax() :]\n","futs_df.interpolate(method=\"time\", inplace=True)\n","\n","assert not futs_df.isnull().any().any()\n","print(fut_tickers)\n","print(f\"Dataset Shape: {futs_df.shape}\")\n","\n","futs_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PRICE_COLS =[\"Close\", \"Open\", \"High\", \"Low\", \"Spread\"]\n","def create_fut_rets_df(df, price_types = PRICE_COLS):\n","    log_data = {}\n","    pct_change_data = {}\n","\n","    for price_type in tqdm(price_types):\n","        for column in tqdm(df.columns):\n","            if price_type in column:\n","                log_data[f\"{column}_lp\"] = np.log(df[column])\n","                pct_change_data[f\"{column}_pc\"] = df[column].pct_change() * 100\n","    log_fut_df = pd.DataFrame(log_data, index=df.index).bfill()\n","    pct_changes_df = pd.DataFrame(pct_change_data, index=df.index).bfill()\n","    combined_df = pd.concat([log_fut_df, pct_changes_df], axis=1)\n","\n","    return combined_df\n","\n","fut_rets_df = create_fut_rets_df(futs_df)\n","fut_rets_df.head(2)"]},{"cell_type":"markdown","metadata":{},"source":["# Save Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["futs_df.to_csv(f\"{CLEAN_DATA_PATH}{os.sep}futures_{INTERVAL}.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["# EDA\n","\n","## Skew, Kurosis and Outliers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fut_lp = fut_rets_df.filter(regex=\"^.*Close_lp\")\n","fut_lr = fut_lp.diff(1).fillna(0)\n","plt.figure(figsize=(14, 12))\n","ax = sns.violinplot(data=fut_lr, orient='h', scale='width')\n","\n","plt.title(\"Violin Plot of Returns\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Data Mining for Correlations\n"]},{"cell_type":"markdown","metadata":{},"source":["## Autoregressive"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from statsmodels.tsa.stattools import pacf\n","from tqdm import tqdm\n","from scipy.stats import norm\n","\n","def get_significant_lags(series, alpha=0.05):\n","    pacf_values = pacf(series.dropna(), nlags=40)\n","    n = len(series.dropna())\n","    z_critical = norm.ppf(1 - alpha / 2)  # two-tailed test\n","    critical_value = z_critical / np.sqrt(n) # the PCF value\n","    significant_lags = [i for i, p in enumerate(pacf_values) if abs(p) > critical_value]\n","    significant_lags = significant_lags[2:] # Drop lag 0,1, its always AR\n","    significant_pacf_values = [p for p in pacf_values if abs(p) > critical_value]\n","    significant_pacf_values = significant_pacf_values[2:]\n","    return significant_lags, significant_pacf_values\n","\n","def get_ar_stats(df, alpha=0.05):\n","    df.index = pd.DatetimeIndex(df.index).to_period('T')\n","    ar_results = []\n","    for col in tqdm(df.columns, desc=\"get_ar_stats\"):\n","        significant_lags, significant_pacf_values = get_significant_lags(df[col], alpha)\n","        ar_results.append({\n","            'Futs': col,\n","            'Lags': significant_lags,\n","            'PACF Values': significant_pacf_values\n","        })\n","\n","    ar_df = pd.DataFrame(ar_results)\n","    return ar_df\n","\n","ar_df = get_ar_stats(futs_df.filter(regex='_Close$'))\n","ar_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def find_common_lags(ar_df):\n","    lag_dict = {}\n","    for index, row in ar_df.iterrows():\n","        for lag in row['Lags']:\n","            if lag in lag_dict:\n","                lag_dict[lag].append(row['Futs'])\n","            else:\n","                lag_dict[lag] = [row['Futs']]\n","\n","    common_lags = {lag: futs for lag, futs in lag_dict.items() if len(futs) > 1}\n","\n","    common_lags_df = pd.DataFrame(list(common_lags.items()), columns=['Lag', 'Futs'])\n","    common_lags_df['Futs'] = common_lags_df['Futs'].apply(lambda futs: ', '.join(futs))\n","\n","    return common_lags_df\n","\n","\n","commmon_ar_df = find_common_lags(ar_df)\n","commmon_ar_df"]},{"cell_type":"markdown","metadata":{},"source":["## Correlations between Futures"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from statsmodels.tsa.stattools import adfuller\n","from scipy.stats import pearsonr\n","from itertools import combinations\n","\n","LAGS = [1, 4, 16, 32]\n","MAX_DIFF = 2\n","\n","\n","def make_stationary(series, max_diff=MAX_DIFF):\n","    result = adfuller(series.dropna(), autolag=\"AIC\")\n","    p_value = result[1]\n","    if p_value < 0.05:\n","        return series, 0\n","\n","    diff_count = 0\n","    differenced_series = series.copy()\n","    while p_value >= 0.05 and diff_count < max_diff:\n","        differenced_series = differenced_series.diff().dropna()\n","        result = adfuller(differenced_series, autolag=\"AIC\")\n","        p_value = result[1]\n","        diff_count += 1\n","        if p_value < 0.05:\n","            break\n","\n","    return differenced_series, diff_count\n","\n","\n","def calculate_correlations(data, lags=LAGS, alpha=0.05, corr_threshold=0.25):\n","    correlations = {}\n","\n","    rets = data.filter(regex=\"(_Close_lp)$\")\n","    pairs = list(combinations(rets.columns, 2))\n","    for col1, col2 in tqdm(pairs, desc=\"calculate_correlations\", total=len(pairs)):\n","        series1, _ = make_stationary(rets[col1])\n","        series2, _ = make_stationary(rets[col2])\n","\n","        for lag in lags:\n","            lagged_series2 = series2.shift(lag).dropna()\n","            truncated_series1 = series1.iloc[lag:]\n","\n","            if len(truncated_series1) == 0 or len(truncated_series1) != len(lagged_series2):\n","                continue\n","\n","            corr, p_value = pearsonr(truncated_series1, lagged_series2)\n","            if p_value < alpha and abs(corr) >= corr_threshold:\n","                correlations[((col1, col2), lag)] = (corr, p_value)\n","\n","    return correlations\n","\n","correlation_df = []\n","correlation_results = calculate_correlations(fut_rets_df)\n","if len(correlation_results) > 0:\n","    correlation_df = pd.DataFrame.from_dict(\n","        correlation_results, orient=\"index\", columns=[\"Correlation\", \"P-Value\"]\n","    )\n","    correlation_df = correlation_df.sort_values(by=\"Correlation\", ascending=False)\n","\n","correlation_df"]},{"cell_type":"markdown","metadata":{},"source":["## Data Mining for Co-Integrations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from statsmodels.tsa.vector_ar.vecm import coint_johansen\n","import itertools\n","from numpy.linalg import LinAlgError\n","\n","subsets = {\n","        'Market': MARKET_FUTS,\n","        'Metals': METALS_FUTS,\n","        'Energy': ENERGY_FUTS,\n","        'Agri': AGRI_FUTS,\n","        'All': FUTS\n","    }\n","\n","def johansen_cointegration_test(df, alpha=0.05):\n","    assert df.shape[1] > 0\n","\n","    data = df.values\n","    johansen_result = coint_johansen(data, det_order=0, k_ar_diff=1)\n","\n","    trace_stat = johansen_result.lr1\n","    trace_crit_values = johansen_result.cvt\n","\n","    def get_crit_index(alpha):\n","        alpha_levels = [0.10, 0.05, 0.01]\n","        if alpha in alpha_levels:\n","            return alpha_levels.index(alpha)\n","        else:\n","            raise ValueError(f\"Alpha {alpha} out of valid range {alpha_levels}\")\n","\n","    crit_index = get_crit_index(alpha)\n","    num_cointegrated_vectors = sum(trace_stat > trace_crit_values[:, crit_index])\n","    return num_cointegrated_vectors, johansen_result\n","\n","def run_johansen_on_subsets(fut_rets_df):\n","    results = []\n","\n","    # Test each subset\n","    for subset_name, tickers in subsets.items():\n","        cols = [t.replace(\"=F\", \"\")+\"_Close_lp\" for t in tickers]\n","        filtered_df = fut_rets_df[cols]\n","        if len(filtered_df.columns) < 2:\n","            continue\n","        try:\n","            num_coint_vectors, johansen_result = johansen_cointegration_test(filtered_df)\n","            results.append({\n","                'Subset': subset_name,\n","                'Coints': num_coint_vectors\n","            })\n","        except LinAlgError as e:\n","            results.append({\n","                'Subset': subset_name,\n","                'Coints': None,\n","            })\n","\n","    # Test combinations of subsets\n","    subset_keys = list(subsets.keys())\n","    for i in range(2, len(subset_keys) + 1):\n","        for combination in itertools.combinations(subset_keys, i):\n","            combined_name = ' + '.join(combination)\n","            combined_tickers = list(itertools.chain.from_iterable(subsets[key] for key in combination))\n","            cols = [t.replace(\"=F\", \"\")+\"_Close_lp\" for t in combined_tickers]\n","            filtered_df = fut_rets_df[cols]\n","            if len(filtered_df.columns) < 2 or len(filtered_df.columns) > 12:\n","                continue\n","            try:\n","                num_coint_vectors, johansen_result = johansen_cointegration_test(filtered_df)\n","                results.append({\n","                    'Subset': combined_name,\n","                    'Coints': num_coint_vectors\n","                })\n","            except LinAlgError as e:\n","                results.append({\n","                    'Subset': combined_name,\n","                    'Coints': 0,\n","                })\n","\n","    return pd.DataFrame(results)\n","\n","results = run_johansen_on_subsets(fut_rets_df)\n","results[results[\"Coints\"] > 1].sort_values(\"Coints\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_all_futs_in_coint_subsets():\n","    coint_subset = set()\n","    for subset in results[results[\"Coints\"] > 1]['Subset']:\n","        futures = subset.split(' + ')\n","        for future in futures:\n","            coint_subset.update(subsets[future])\n","\n","    return coint_subset\n","\n","# We can get a subset of all coint futures across industries, though we\n","# know there is strong coint in the equities and agri futures,\n","COINT_FUTS = get_all_futs_in_coint_subsets()\n","COINT_FUTS = MARKET_FUTS + AGRI_FUTS\n","print(COINT_FUTS)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from statsmodels.tsa.stattools import coint\n","\n","def pairwise_cointegration_test(df, coint_cols, alpha=0.05):\n","    tickers = [t.replace(\"=F\", \"\")+\"_Close_lp\" for t in coint_cols]\n","    results = []\n","\n","    for i in range(len(tickers)):\n","        for j in range(i + 1, len(tickers)):\n","            series1 = df[tickers[i]]\n","            series2 = df[tickers[j]]\n","            score, p_value, _ = coint(series1, series2)\n","            if p_value < alpha:\n","                results.append({\n","                    'Ticker1': tickers[i].split('_')[0],\n","                    'Ticker2': tickers[j].split('_')[0],\n","                    'P-Value': p_value\n","                })\n","\n","    results_df = pd.DataFrame(results)\n","    return results_df\n","\n","\n","pairwise_coint_results = pairwise_cointegration_test(fut_rets_df, COINT_FUTS)\n","\n","if not pairwise_coint_results.empty:\n","    pairwise_coint_results = pairwise_coint_results.sort_values(by='P-Value')\n","    print(pairwise_coint_results)\n","else:\n","    print(\"No meaningful pairwise cointegration within the supplied alpha\")"]},{"cell_type":"markdown","metadata":{},"source":["## Cummulative Returns Visualizations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for c in fut_rets_df.columns:\n","    if c.endswith(\"_Close_pc\"):\n","        fut_rets_df[c + \"_c\"] = fut_rets_df[c].cumsum()\n","\n","# Plotting\n","plt.figure(figsize=(22, 12))\n","for c in fut_rets_df.columns:\n","    if c.endswith(\"_Close_pc_c\"):\n","        plt.plot(fut_rets_df.index, fut_rets_df[c], label=c.replace(\"_Close_pc_c\", \"\"), alpha=0.65)\n","plt.title(\"% Cummulative\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Distributions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from scipy.stats import norm, skew, kurtosis, kstest\n","\n","def plot_norm(df):\n","    columns = df.filter(regex=\"(_Close(_lp)?)$\").columns\n","    n_cols = len(columns)\n","    n_cols_adjusted = n_cols // 2 + (n_cols % 2)\n","    assert len(columns) > 0 and n_cols_adjusted > 0\n","\n","    fig, axes = plt.subplots(nrows=max(n_cols_adjusted // 2, 1), ncols=max(n_cols_adjusted // 2, 1), figsize=(22, 18))\n","\n","    axes = axes.flatten()\n","    fig.suptitle(\"Log Price Gaussian\", fontsize=16)\n","    for ax, column in zip(axes, columns):\n","        data = df[column].dropna()\n","        sns.kdeplot(\n","            data,\n","            fill=True,\n","            common_norm=False,\n","            alpha=0.5,\n","            linewidth=0.5,\n","            ax=ax\n","        )\n","\n","        params = norm.fit(data)\n","        # Kolmogorov-Smirnov Test (KS Test)\n","        ks_stat, ks_p_value = kstest(data, norm.name, args=params)\n","\n","        mu, std = params\n","        xmin, xmax = ax.get_xlim()\n","\n","        x = np.linspace(xmin, xmax, 100)\n","        p = norm.pdf(x, mu, std)\n","        ax.plot(x, p, 'k', linewidth=2)\n","\n","        data_skew = skew(data)\n","        data_kurtosis = kurtosis(data)\n","\n","        title = (\n","            f\"{column.split('_')[0]} (ks {ks_stat:.02f}, ks_p {ks_p_value:.05f}) \\n\"\n","            f\"Mean = {mu:.04f}, Std = {std:.04f} \\n\"\n","            f\"Skew = {data_skew:.04f}, Kurtosis = {data_kurtosis:.04f}\"\n","        )\n","        ax.set_title(title)\n","        ax.set_xlabel(\"Log Price\")\n","        ax.set_ylabel(\"Density\")\n","\n","    for ax in axes[len(columns):]:\n","        ax.set_visible(False)\n","\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","    plt.show()\n","\n","plot_norm(fut_rets_df)"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":4755137,"sourceId":8061237,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
