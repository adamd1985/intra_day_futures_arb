{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","id":"oaDoHbxVH0CW"},"source":["# CNN"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","id":"z_cBqdYOoY5S"},"source":["## Notebook's Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","collapsed":false,"execution":{"iopub.execute_input":"2024-05-31T19:14:59.632537Z","iopub.status.busy":"2024-05-31T19:14:59.631985Z"},"id":"eETPYJLiMU-b","jupyter":{"outputs_hidden":false},"outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"outputs":[],"source":["INSTALL_DEPS = False\n","if INSTALL_DEPS:\n","  %pip install hurst==0.0.5\n","  %pip install imbalanced_learn==0.12.3\n","  %pip install imblearn==0.0\n","  %pip install protobuf==5.27.0\n","  %pip install pykalman==0.9.7\n","  %pip install tqdm==4.66.4\n","\n","!python --version"]},{"cell_type":"markdown","metadata":{},"source":["## Cloud Environment Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","collapsed":false,"id":"Q4-GoceIIfT_","jupyter":{"outputs_hidden":false},"outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","IN_KAGGLE = IN_COLAB = False\n","try:\n","    # https://www.tensorflow.org/install/pip#windows-wsl2\n","    import google.colab\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/drive\")\n","    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n","    MODEL_PATH = \"/content/drive/MyDrive/models\"\n","    IN_COLAB = True\n","    print(\"Colab!\")\n","except:\n","    IN_COLAB = False\n","if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and not IN_COLAB:\n","    print(\"Running in Kaggle...\")\n","    for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n","        for filename in filenames:\n","            print(os.path.join(dirname, filename))\n","    MODEL_PATH = \"./models\"\n","    DATA_PATH = \"/kaggle/input/intra-day-agriculture-futures-trades-2023-2024\"\n","    IN_KAGGLE = True\n","    print(\"Kaggle!\")\n","elif not IN_COLAB:\n","    IN_KAGGLE = False\n","    MODEL_PATH = \"./models\"\n","    DATA_PATH = \"./data/\"\n","    print(\"running localhost!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import mixed_precision\n","\n","print(f'Tensorflow version: [{tf.__version__}]')\n","\n","tf.get_logger().setLevel('INFO')\n","\n","#tf.config.set_soft_device_placement(True)\n","#tf.config.experimental.enable_op_determinism()\n","#tf.random.set_seed(1)\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","\n","  tf.config.experimental_connect_to_cluster(tpu)\n","  tf.tpu.experimental.initialize_tpu_system(tpu)\n","  strategy = tf.distribute.TPUStrategy(tpu)\n","except Exception as e:\n","  gpus = tf.config.experimental.list_physical_devices('GPU')\n","  if len(gpus) > 0:\n","    try:\n","        strategy = tf.distribute.MirroredStrategy()\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","    except RuntimeError as e:\n","        print(e)\n","    finally:\n","        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n","  else:\n","    # CPU is final fallback\n","    strategy = tf.distribute.get_strategy()\n","    print(\"Running on CPU\")\n","\n","def is_tpu_strategy(strategy):\n","    return isinstance(strategy, tf.distribute.TPUStrategy)\n","\n","print(\"Number of accelerators:\", strategy.num_replicas_in_sync)\n","os.getcwd()"]},{"cell_type":"markdown","metadata":{},"source":["# Instruments"]},{"cell_type":"markdown","metadata":{},"source":["## Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from algo_trading_utility_script import *\n","\n","filename = f\"{DATA_PATH}{os.sep}futures_{INTERVAL}.csv\"\n","print(filename)\n","futs_df = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n","\n","print(futs_df.shape)\n","\n","HALF_LIFE, HURST = get_ou(futs_df, f'{TARGET_FUT}_Close')\n","\n","print(\"Half-Life:\", HALF_LIFE)\n","print(\"Hurst:\", HURST)\n","\n","futs_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 4))\n","\n","plt.plot(futs_df[f'{TARGET_FUT}_Close'], label=f'{TARGET_FUT} Close', alpha=0.7)\n","plt.title(f'{TARGET_FUT} Price')\n","plt.xlabel('Date')\n","plt.ylabel('Price')\n","plt.legend()\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pickle\n","from tqdm import tqdm\n","\n","from sklearn.preprocessing import normalize\n","\n","TEST_SPLIT = 0.8\n","TRAIN_SIZE = int(len(futs_df) * TEST_SPLIT)\n","CACHE = True\n","FUTURES_TMP_FILE = \"./tmp/futures.pkl\"\n","os.makedirs(\"./tmp/\", exist_ok=True)\n","\n","def oversample_mean_reversions(train_agri_ts, window, period=INTERVAL, hurst=HURST):\n","    samples = []\n","    for df in tqdm(train_agri_ts, desc=\"oversample_mean_reversions\"):\n","        bb_df = df.copy()\n","        results_df = param_search_bbs(bb_df, StockFeatExt.CLOSE, period, initial_window=window * 2, window_min=window // 2, hurst=hurst)\n","        results_df = results_df[results_df[\"Metric\"] == \"Sharpe\"]\n","        bb_df, _ = bollinger_band_backtest(bb_df, StockFeatExt.CLOSE, results_df[\"Window\"].iloc[0], period, std_factor=results_df[\"Standard_Factor\"].iloc[0])\n","\n","        samples.append(bb_df[train_agri_ts[0].columns].reset_index(drop=True))\n","    return train_agri_ts + samples\n","\n","def normalize_and_label_data(ts, meta_label=META_LABEL, cols_to_scale=COLS_TO_SCALE):\n","    y0 = 0\n","    y1 = 0\n","    process_data = []\n","    for df in tqdm(ts, desc=\"label_data\"):\n","        df = aug_metalabel_mr(df)\n","        y0  += (df[meta_label] == 0).sum()\n","        y1  += (df[meta_label] > 0).sum()\n","\n","        if cols_to_scale is not None:\n","            aug_df_scaled = normalize(df[cols_to_scale])\n","            aug_df_scaled = pd.DataFrame(aug_df_scaled, columns=cols_to_scale)\n","            df[cols_to_scale] = aug_df_scaled\n","            df = df.loc[:, ~df.columns.duplicated(keep=\"first\")]\n","\n","        process_data.append(df)\n","\n","    total = y0 + y1\n","    class_weight_0 = total / y0 if y0 != 0 else 0\n","    class_weight_1 = total / y1 if y1 != 0 else 0\n","    class_weights = {0: class_weight_0, 1: class_weight_1}\n","\n","    return process_data, class_weights\n","\n","with strategy.scope():\n","    if not os.path.exists(FUTURES_TMP_FILE):\n","        futs_exog_df = process_exog(MARKET_FUTS, futs_df)\n","        train_agri_ts, val_agri_ts = process_futures(FUTS, futs_df, futs_exog_df, TRAIN_SIZE, INTERVAL)\n","        train_agri_ts = oversample_mean_reversions(train_agri_ts, HALF_LIFE)\n","        val_agri_ts = oversample_mean_reversions(val_agri_ts, HALF_LIFE)\n","\n","        if CACHE:\n","            with open(FUTURES_TMP_FILE, 'wb') as f:\n","                pickle.dump((train_agri_ts, val_agri_ts), f)\n","    else:\n","        with open(FUTURES_TMP_FILE, 'rb') as f:\n","            train_agri_ts, val_agri_ts = pickle.load(f)\n","\n","train_agri_ts, CLASS_WEIGHTS = normalize_and_label_data(train_agri_ts)\n","val_agri_ts, val_weights = normalize_and_label_data(val_agri_ts)\n","\n","print(f\"train: {CLASS_WEIGHTS}\")\n","print(f\"test: {val_weights}\")\n","np.shape(train_agri_ts)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["FEATURES = KF_COLS + BB_COLS + MOM_COLS + StockFeatExt.list\n","\n","sample = val_agri_ts[0]\n","print(sample[META_LABEL].value_counts())\n","\n","sampled_pattenrs = sample[sample[META_LABEL] > 0]\n","sampled_pattenrs[FEATURES + [META_LABEL, \"Ret\"]].tail(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["WINDOW = 511  # window is the k+k-1xd-1 or the sum i=0..n of 1+sum(receptive feild)x2^i\n","WINDOW_TMP_PATH = \"./tmp/\"\n","# TPU see: https://github.com/tensorflow/tensorflow/issues/41635\n","BATCH_SIZE = 8  * strategy.num_replicas_in_sync # Default 8\n","print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n","\n","def prepare_windows(data_df, label_df, window_size=WINDOW):\n","    \"\"\"\n","    Prepare windows of features and corresponding labels for classification.\n","    IMPORTANT: There is no padding, incomplete timewindows are discarded!\n","\n","    Parameters:\n","    - data_df: DataFrame containing the features.\n","    - label_df: DataFrame containing the labels.\n","    - window_size: The size of the input window.\n","\n","    Returns:\n","    - X: Array of input windows.\n","    - y: Array of corresponding labels.\n","    \"\"\"\n","    X, y = [], []\n","    for i in range(len(data_df) - window_size):\n","        input_window = data_df.iloc[i : i + window_size].values\n","        assert not np.isnan(input_window).any(), \"NaN values found in input window\"\n","        X.append(input_window)\n","        if label_df is not None:\n","            target_label = label_df.iloc[i + window_size]\n","            y.append([target_label])\n","            assert not np.isnan(target_label).any(), \"NaN values found in target label\"\n","    return np.array(X), np.array(y)\n","\n","def prepare_window_with_disjoint_ts(ts_list, window_size=WINDOW):\n","    \"\"\"\n","    Generator function to yield windows of features and corresponding labels from multiple time series.\n","\n","    Parameters:\n","    - ts_list: List of DataFrames, each containing a time series.\n","    - window_size: The size of the input window.\n","\n","    Yields:\n","    - features: The input window of features.\n","    - labels: The corresponding label.\n","    \"\"\"\n","    for data_df in ts_list:\n","        X, y = prepare_windows(data_df[FEATURES], data_df[META_LABEL], window_size=window_size)\n","        for features, labels in zip(X, y):\n","            yield features, labels\n","\n","def create_windowed_dataset_from_generator(ts_list, window_size=WINDOW, batch_size=BATCH_SIZE):\n","    \"\"\"\n","    Create a TensorFlow dataset from a generator.\n","\n","    Parameters:\n","    - ts_list: List of DataFrames, each containing a time series.\n","    - window_size: The size of the input window.\n","    - batch_size: The batch size for the dataset.\n","\n","    Returns:\n","    - dataset: A TensorFlow dataset.\n","    \"\"\"\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: prepare_windows_with_disjoint_ts(ts_list, window_size=window_size),\n","        output_signature=(\n","            tf.TensorSpec(shape=(window_size, len(FEATURES)), dtype=tf.float32),\n","            tf.TensorSpec(shape=(1,), dtype=tf.float32)  # Assuming labels are floats for binary classification\n","        )\n","    )\n","    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","    return dataset\n","\n","def create_dataset_from_generator(ts_list, batch_size):\n","    def generator(ts_list):\n","        full_df = pd.concat(ts_list)\n","        for i, row in full_df.iterrows():\n","            yield row[FEATURES].values, row[META_LABEL]  # Reshape to match (1,)\n","\n","    output_signature = (\n","        tf.TensorSpec(shape=(len(FEATURES),), dtype=tf.float32),\n","        tf.TensorSpec(shape=(), dtype=tf.float32)\n","    )\n","\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: generator(ts_list),\n","        output_signature=output_signature\n","    )\n","    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","    return dataset\n","\n","with strategy.scope():\n","    train_dataset = create_dataset_from_generator(train_agri_ts, batch_size=BATCH_SIZE)\n","    val_dataset = create_dataset_from_generator(val_agri_ts, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# INPUT_SHAPE = (len(FEATURES), ) # The expected shape, where the None shape is BATCH_SIZE\n","\n","sampled_dataset = val_dataset.shuffle(buffer_size=250).take(1)\n","for features, labels in train_dataset.take(1):\n","    INPUT_SHAPE = features.numpy().shape[1:]  # Assuming the shape is (batch_size, len(FEATURES))\n","    print(\"Features:\", features.numpy())\n","    print(\"Labels:\", labels.numpy())\n","\n","print(\"INPUT_SHAPE:\", INPUT_SHAPE)"]},{"cell_type":"markdown","metadata":{},"source":["# CNN Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Conv1D, Add, Multiply, Input, Flatten, Dense, AveragePooling1D, MaxPooling1D, SpatialDropout1D, Activation, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2, l1, l1_l2\n","\n","MODEL_NAME = \"WAVENET\"\n","MAX_DILATION = 8\n","DILATION_RATE = 1\n","EPOCHS = 3\n","PATIENCE_EPOCHS = 1\n","FILTERS = 32\n","DROPRATE = 0.4\n","KERNEL_SIZE = 3\n","REG_WEIGHTS = 0.05\n","CONVOLUTIONS = 10\n","LEARN_RATTE =1e-3\n","\n","def wavenet_block(inputs, layer_id, filters, kernel_size=KERNEL_SIZE,\n","                  dilation_rate=DILATION_RATE, reg_param=REG_WEIGHTS, dropout_rate=DROPRATE):\n","    # Filter = is the output, kernel is the input or scope.\n","    # Dilation expands the receptive field of the kernel.\n","    conv_f = Conv1D(filters, kernel_size,\n","                    dilation_rate=dilation_rate,\n","                    padding='causal',\n","                    kernel_regularizer=l2(reg_param),\n","                    name=f'conv_f_{layer_id}')(inputs)\n","    conv_g = Conv1D(filters, kernel_size,\n","                    dilation_rate=dilation_rate,\n","                    padding='causal',\n","                    kernel_regularizer=l2(reg_param),\n","                    name=f'conv_g_{layer_id}')(inputs)\n","    tanh_out = Activation('tanh')(conv_f)\n","    sigmoid_out = Activation('sigmoid')(conv_g)\n","    merged = Multiply()([tanh_out, sigmoid_out])\n","    # merged = SpatialDropout1D(dropout_rate)(merged)\n","\n","    skip_out = Conv1D(filters, 1, padding='same',\n","                      kernel_regularizer=l2(reg_param),\n","                      name=f'skip_{layer_id}')(merged)\n","    residual_out = Conv1D(filters, 1, padding='same',\n","                          kernel_regularizer=l2(reg_param),\n","                          name=f'residual_{layer_id}')(inputs)\n","    residual_out = Add()([residual_out, skip_out])\n","\n","    return residual_out, skip_out\n","\n","\n","def build_wavenet_model(input_shape, filters=FILTERS, kernel_size=KERNEL_SIZE,\n","                        reg_param=REG_WEIGHTS, dropout_rate=DROPRATE, convolutions=CONVOLUTIONS,\n","                        max_dilation=MAX_DILATION):\n","    inputs = Input(shape=input_shape)\n","    x = inputs\n","    skip_connections = []\n","\n","    for i in range(convolutions):\n","        dilation_rate = min(2 ** i, max_dilation)\n","        x, skip = wavenet_block(x, layer_id=i,\n","                                filters=filters,\n","                                kernel_size=kernel_size,\n","                                dilation_rate=dilation_rate,\n","                                reg_param=reg_param,\n","                                dropout_rate=dropout_rate)\n","        skip_connections.append(skip)\n","\n","    x = Add()(skip_connections)\n","    x = Activation('relu')(x)\n","\n","    # Downsample the output to coarser frames\n","    x = AveragePooling1D(pool_size=2)(x)\n","\n","    x = Conv1D(filters, 1, activation='relu', kernel_regularizer=l2(reg_param), name='post_conv_1')(x)\n","    x = Conv1D(filters, 1, activation='relu', kernel_regularizer=l2(reg_param), name='post_conv_2')(x)\n","    x = Flatten()(x)\n","    x = Dense(32, activation='relu', kernel_regularizer=l2(reg_param), name='fc1')(x)\n","    x = Dropout(dropout_rate, name='fcdo1')(x)\n","    outputs = Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_param), name='output_dense')(x)\n","\n","    model = Model(inputs, outputs, name=MODEL_NAME)\n","    return model\n","\n","def build_baseline_model(input_shape, filters=FILTERS, kernel_size=KERNEL_SIZE,\n","                        reg_param=REG_WEIGHTS, dropout_rate=DROPRATE, convolutions=CONVOLUTIONS,\n","                        max_dilation=MAX_DILATION):\n","    inputs = Input(shape=input_shape)\n","\n","    x = Dense(1024, activation='relu', kernel_regularizer=l1_l2(reg_param))(inputs)\n","    x = Dropout(dropout_rate)(x)\n","    x = Dense(1024, activation='relu', kernel_regularizer=l1_l2(reg_param))(x)\n","    x = Dropout(dropout_rate)(x)\n","    x = Dense(1024, activation='relu', kernel_regularizer=l1_l2(reg_param))(x)\n","    x = Dropout(dropout_rate)(x)\n","    outputs = Dense(1, activation='sigmoid', name='output_dense', kernel_regularizer=l1_l2(reg_param))(x)\n","\n","    return Model(inputs, outputs, name=f'{MODEL_NAME}_BASELINE')"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","from tensorflow.keras.losses import BinaryCrossentropy\n","from tensorflow.keras.metrics import AUC, BinaryAccuracy\n","from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n","from tensorflow.keras.optimizers import Adam\n","\n","import shutil\n","\n","MODEL_DIR = f\"models/{MODEL_NAME}\"\n","IMAGES_DIR = f\"images/{MODEL_NAME}/images\"\n","LOG_BASEPATH = f\"logs/{MODEL_NAME}/tb\"\n","TARGET_METRIC = \"val_auc\"\n","\n","if os.path.exists(LOG_BASEPATH):\n","    # Remove tensorboard logs\n","    shutil.rmtree(LOG_BASEPATH)\n","\n","def build_cnn(input_shape, train_dataset, test_dataset=None, lr=LEARN_RATTE):\n","    model = build_baseline_model(\n","        input_shape=input_shape,\n","        filters=FILTERS,\n","        kernel_size=KERNEL_SIZE,\n","        reg_param=REG_WEIGHTS,\n","        dropout_rate=DROPRATE,\n","        convolutions=CONVOLUTIONS,\n","    )\n","\n","    optimizer = Adam(learning_rate=lr)\n","    loss = BinaryCrossentropy(from_logits=False,\n","                              reduction='sum_over_batch_size',\n","                              name='bce')\n","    model.compile(\n","        loss=loss,\n","        optimizer=optimizer,\n","        metrics=[BinaryAccuracy(name='ba'), AUC(name='auc')]\n","    )\n","    callbacks = [\n","        EarlyStopping(\n","            patience=PATIENCE_EPOCHS,\n","            monitor=TARGET_METRIC,\n","            restore_best_weights=True,\n","        ),\n","        ReduceLROnPlateau(\n","            monitor=TARGET_METRIC,\n","            factor=0.5,\n","            patience=2,\n","            verbose=1,\n","            min_lr=1e-5\n","        ),\n","        TensorBoard(\n","            log_dir=LOG_BASEPATH,\n","            histogram_freq=1,\n","            write_images=True\n","        )\n","    ]\n","\n","    history = model.fit(\n","        train_dataset,\n","        validation_data=test_dataset,\n","        epochs=EPOCHS,\n","        batch_size=BATCH_SIZE,\n","        callbacks=callbacks,\n","        verbose=1\n","    )\n","\n","    return model, history\n","\n","with strategy.scope():\n","    print(f\"input_shape: {INPUT_SHAPE}\")\n","\n","    model, history = build_cnn(INPUT_SHAPE, train_dataset=train_dataset, test_dataset=val_dataset)\n","    model.save(MODEL_PATH)\n","    model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### Visualize Outputs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(14, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], label='Train Loss')\n","plt.plot(history.history['val_loss'], label='Val Loss')\n","plt.title('Model Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend(loc='upper right')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['ba'], label='Train Accuracy')\n","plt.plot(history.history['val_ba'], label='Val Accuracy')\n","plt.plot(history.history['auc'], label='Train AUC')\n","plt.plot(history.history['val_auc'], label='Val AUC')\n","plt.title('Model Accuracy and AUC')\n","plt.xlabel('Epoch')\n","plt.ylabel('Metric')\n","plt.legend(loc='lower right')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Explain"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import shap\n","shap.initjs()\n","\n","RANDOMIZE_SIZE = 32\n","SAMPLE_SIZE = 100\n","\n","train_dataset = create_dataset_from_generator(train_agri_ts, batch_size=BATCH_SIZE)\n","sampled_dataset = train_dataset.shuffle(buffer_size=RANDOMIZE_SIZE*SAMPLE_SIZE *2).take(SAMPLE_SIZE *2)\n","sampled_dataset = list(sampled_dataset.as_numpy_iterator())\n","\n","test_dataset = create_dataset_from_generator(train_agri_ts, batch_size=BATCH_SIZE)\n","sampled_testdataset = test_dataset.shuffle(buffer_size=RANDOMIZE_SIZE*SAMPLE_SIZE).take(SAMPLE_SIZE)\n","sampled_testdataset = list(sampled_testdataset.as_numpy_iterator())\n","\n","\n","background_features = np.concatenate([batch[0] for batch in sampled_dataset], axis=0)\n","background = background_features[np.random.choice(background_features.shape[0], SAMPLE_SIZE, replace=False)]\n","\n","test_features = np.concatenate([batch[0] for batch in sampled_testdataset], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["e = shap.DeepExplainer(model, background)\n","\n","shap_values = e.shap_values(test_features)\n","if isinstance(shap_values, list):\n","    shap_values = shap_values[0]\n","shap_values = np.squeeze(shap_values)\n","\n","print(f\"SHAP values shape: {shap_values.shape}\")\n","print(f\"Test features shape: {test_features.shape}\")\n","assert shap_values.shape == test_features.shape\n","shap.summary_plot(shap_values, test_features, feature_names=FEATURES)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_index = 23\n","e = shap.KernelExplainer(model, background)\n","\n","shap_features = test_features[sample_index:sample_index+1]\n","shap_values = e.shap_values(shap_features, nsamples=SAMPLE_SIZE)\n","print(f\"SHAP values shape: {shap_values.shape}\")\n","\n","if isinstance(shap_values, list):\n","    shap_values = shap_values[0]\n","shap_values = np.squeeze(shap_values)\n","print(f\"Explainer expected value: {e.expected_value}\")\n","\n","with warnings.catch_warnings():\n","    warnings.simplefilter(\"ignore\")\n","    shap_values = e.shap_values(shap_features)[1]\n","    shap_interaction_values = e.shap_interaction_values(shap_features)\n","if isinstance(shap_interaction_values, list):\n","    shap_interaction_values = shap_interaction_values[1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["e = shap.KernelExplainer(model, background)\n","\n","shap_features = test_features[sample_index:SAMPLE_SIZE+1]\n","shap_values = e.shap_values(shap_features, nsamples=SAMPLE_SIZE)\n","if isinstance(shap_values, list):\n","    shap_values = shap_values[0]\n","shap_values = np.squeeze(shap_values)\n","\n","print(f\"SHAP values shape: {shap_values.shape}\")\n","print(f\"Test features shape: {test_features.shape}\")\n","\n","shap.force_plot(e.expected_value, shap_values, shap_features, feature_names=FEATURES, out_names=[\"RW\", \"MR\"])"]},{"cell_type":"markdown","metadata":{},"source":["## Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score, roc_auc_score\n","\n","def print_metrics():\n","    ypred_raw = model.predict([X_t])\n","    pred = (ypred_raw > 0.5).astype(int)\n","    metrics = {}\n","\n","    metrics = {\n","        \"Accuracy\": accuracy_score(y_t.flatten(), pred.flatten()),\n","        \"Precision\": precision_score(y_t.flatten(), pred.flatten()),\n","        \"Recall\": recall_score(y_t.flatten(), pred.flatten()),\n","        \"F1b Score\": fbeta_score(y_t.flatten(), pred.flatten(), average=\"weighted\", beta=0.1),\n","        \"ROC AUC\": roc_auc_score(y_t.flatten(), ypred_raw.flatten(), average='weighted')  # Using raw probabilities\n","    }\n","\n","    metrics_unseen_df = pd.DataFrame.from_dict(metrics, orient='index')\n","    metrics_unseen_df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.math import confusion_matrix\n","import seaborn as sns\n","\n","def plot_confusion_matrix(labels, cm2=None, labels2=None):\n","    cm = confusion_matrix(y_t.flatten(), pred)\n","\n","    plt.figure(figsize=(8 if cm2 is not None else 4, 4))\n","    if cm2 is not None:\n","        plt.subplot(1, 2, 1)\n","    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Accent)\n","\n","    df_cm = pd.DataFrame((cm / np.sum(cm, axis=1)[:, None])*100, index=[i for i in labels], columns=[i for i in labels])\n","    cm_plot1 = sns.heatmap(df_cm, annot=True,  fmt=\".2f\", cmap='Blues', xticklabels=labels, yticklabels=labels).get_figure()\n","    plt.xlabel('Predicted Labels')\n","    plt.ylabel('True Labels')\n","    plt.title('Confusion Matrix 1')\n","    tick_marks = np.arange(len(labels))\n","    plt.xticks(tick_marks, labels, rotation=45)\n","    plt.yticks(tick_marks, labels)\n","    plt.tight_layout()\n","\n","    return cm_plot1\n","\n","figure = plot_confusion_matrix(labels=[1,0])"]},{"cell_type":"markdown","metadata":{},"source":["# Grid Search"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n","from tensorboard.plugins.hparams import api as hp\n","\n","import json\n","\n","HP_KERNEL_SIZE = hp.HParam(\"kernel_size\", hp.Discrete([KERNEL_SIZE * 2, KERNEL_SIZE]))\n","HP_BATCH_SIZE = hp.HParam(\"batch_size\", hp.Discrete([BATCH_SIZE]))\n","HP_EPOCHS = hp.HParam(\"epochs\", hp.Discrete([EPOCHS]))\n","HP_DILATION_RATE = hp.HParam(\"dilation_rate\", hp.Discrete([DILATION_RATE]))\n","HP_DROPOUT_RATE = hp.HParam(\"dropout_rate\", hp.Discrete([DROPRATE, DROPRATE*2]))\n","HP_REG_WEIGHTS = hp.HParam(\"reg_weight\", hp.Discrete([REG_WEIGHTS, REG_WEIGHTS*2]))\n","HP_LEARNING_RATE = hp.HParam(\"learning_rate\", hp.Discrete([LEARN_RATTE]))\n","HP_PATIENCE = hp.HParam(\"patience\", hp.Discrete([PATIENCE_EPOCHS]))\n","HP_HIDDEN_DENSE = hp.HParam(\"dense_units\", hp.Discrete([\n","    f\"{WINDOW}\",\n","    f\"{WINDOW*2}_{WINDOW}\",\n","    f\"{WINDOW*2}_{WINDOW}_{WINDOW//2}\",\n","    f\"{WINDOW*4}_{WINDOW*2}\",\n","]))\n","HP_FILTERS = hp.HParam(\"filters\", hp.Discrete([FILTERS //2 ,FILTERS, FILTERS * 2]))\n","HPARAMS = [\n","    HP_FILTERS,\n","    HP_KERNEL_SIZE,\n","    HP_BATCH_SIZE,\n","    HP_EPOCHS,\n","    HP_DILATION_RATE,\n","    HP_DROPOUT_RATE,\n","    HP_REG_WEIGHTS,\n","    HP_LEARNING_RATE,\n","    HP_PATIENCE,\n","    HP_HIDDEN_DENSE\n","    ]\n","\n","def grid_search_build_cnn(input_shape, X, y, Xt=None, yt=None, hparams=HPARAMS, file_name=f\"best_params.json\", checkpoint_file = f\"checkpoint.json\"):\n","    def _decode_arrays(config_str):\n","        return [int(unit) for unit in config_str.split('_')]\n","\n","    def _save_best_params(best_params, best_loss, best_metric, other_metrics = None, file_name=\"best_params.json\"):\n","        os.makedirs(MODEL_DIR, exist_ok=True)\n","        with open(f\"{MODEL_DIR}/{file_name}\", \"w\") as file:\n","            json.dump({\"best_params\": best_params, \"best_loss\": best_loss, \"best_metric\": best_metric, 'other_metrics': other_metrics}, file)\n","\n","    def _load_checkpoint(file_name):\n","        json = None\n","        try:\n","            os.makedirs(MODEL_DIR, exist_ok=True)\n","            with open(f\"{MODEL_DIR}/{file_name}\", \"r\") as file:\n","                json = json.load(file)\n","        except Exception as e:\n","            print(f\"File {MODEL_DIR}/{file_name} not found or error {e}\")\n","        return json\n","\n","    def _save_checkpoint(state, file_name):\n","        os.makedirs(MODEL_DIR, exist_ok=True)\n","        with open(f\"{MODEL_DIR}/{file_name}\", \"w\") as file:\n","            json.dump(state, file)\n","\n","    with create_file_writer(f\"{LOG_BASEPATH}/hparam_tuning\").as_default():\n","        hp.hparams_config(\n","            hparams=hparams,\n","            metrics=[hp.Metric(TARGET_METRIC, display_name=TARGET_METRIC)],\n","        )\n","\n","    start_index = 0\n","    best_loss = np.inf\n","    best_metric = -np.inf\n","    best_params = None\n","    checkpoint = _load_checkpoint(checkpoint_file)\n","    if checkpoint:\n","        start_index = checkpoint['next_index']\n","        best_loss = checkpoint['best_loss']\n","        best_metric = checkpoint['best_metric']\n","        best_params = checkpoint['best_params']\n","\n","    grid = list(ParameterGrid({h.name: h.domain.values for h in hparams}))\n","    for index, hp_values in enumerate(tqdm(grid[start_index:], desc=\"Grid Search..\"), start=start_index):\n","        dense_units = _decode_arrays(hp_values[\"dense_units\"])\n","        filters = _decode_arrays(hp_values[\"filters\"])\n","        b = hp_values[\"bias\"]\n","        k = hp_values[\"kernel_size\"]\n","        d = hp_values[\"dilation_rate\"]\n","        rw = hp_values[\"reg_weight\"]\n","        drop = hp_values[\"dropout_rate\"]\n","\n","        ERROR_ALPHA = hp_values[\"alpha\"]\n","        ERROR_GAMMA = hp_values[\"gamma\"]\n","        print(f\"Shapes{input_shape}: x{X[0].shape}xg{X[1].shape}y{y.shape}, filters {filters}, dense {dense_units}, k: {k}, d: {d}, rw: {rw}, drop: {drop}, b: {b}, alpha: {ERROR_ALPHA},  gamma: {ERROR_GAMMA}\")\n","\n","        model, history = build_cnn(input_shape, X, y,\n","                                    Xt=Xt, yt=yt,\n","                                    filters=filters,\n","                                    kernel_size=k,\n","                                    b_cv=True)\n","        loss = history.history[f\"val_loss\"][-1]\n","        metric = history.history[f\"val_{TARGET_METRIC}\"][-1]\n","        if (metric > best_metric):\n","            best_history = history\n","            best_loss = loss\n","            best_metric = metric\n","            best_model = model\n","            best_params = hp_values\n","            other_metrics = {\n","                f\"{TARGET_METRIC}\": history.history[f\"{TARGET_METRIC}\"][-1],\n","                f\"v_{TARGET_METRIC}\": history.history[f\"val_{TARGET_METRIC}\"][-1],\n","                'ba': history.history[\"ba\"][-1],\n","                'v_ba': history.history[\"val_ba\"][-1],\n","            }\n","            _save_best_params(best_params, best_loss, best_metric, other_metrics, file_name)\n","        checkpoint_state = {\n","            'next_index': index + 1,\n","            'best_loss': best_loss,\n","            'best_metric': best_metric,\n","            'best_params': best_params\n","        }\n","        _save_checkpoint(checkpoint_state, checkpoint_file)\n","    return best_model, best_history, best_params, best_loss, best_metric\n","\n","PARAM_SEARCH = False\n","if PARAM_SEARCH:\n","    with strategy.scope():\n","        assert not np.any(pd.isna(X)) and not np.any(np.isnan(X_t))\n","        print(f\"{X.shape}\")\n","        input_shape = (\n","            WINDOW,\n","            1 if len(X.shape) < 3 else X.shape[2],\n","        )\n","\n","        model, history, best_params, best_loss, best_metric = grid_search_build_cnn(input_shape, X=X, y=y, Xt=X_t, yt=y_t, hparams=HPARAMS)\n","        print(best_params)\n","        print(best_metric)"]},{"cell_type":"markdown","metadata":{},"source":["# CV"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import TimeSeriesSplit\n","\n","CV_MODEL = True\n","CV_SPLITS = 3\n","\n","def train_cv_model(X, y, input_shape, n_splits=5, perturb=True, window=WINDOW):\n","    def _perturb_gaussiannoise(X, noise_level=0.1):\n","        sigma = noise_level * np.std(X)\n","        noise = np.random.normal(0, sigma, X.shape)\n","        return X + noise\n","\n","    if perturb:\n","        X = _perturb_gaussiannoise(X)\n","\n","    results = []\n","    tscv = TimeSeriesSplit(n_splits=n_splits)\n","    global metrics_col\n","    metrics_col = None\n","\n","    for train_index, test_index in tqdm(tscv.split(X), desc=f\"CV Testing for n_splits: {n_splits}\"):\n","        X_train = X.iloc[train_index]\n","        y_train = y.iloc[train_index]\n","        X_test = X.iloc[test_index]\n","        y_test = y.iloc[test_index]\n","\n","        X_train_windows, y_train_windows = prepare_windows(X_train, y_train, window_size=window)\n","        X_test_windows, y_test_windows = prepare_windows(X_test, y_test, window_size=window)\n","\n","        try:\n","            cv_model, _ = build_cnn(input_shape, X=X_train_windows, y=y_train_windows, Xt=X_test_windows, yt=y_test_windows)\n","\n","            result = cv_model.evaluate([X_test_windows], y_test_windows, verbose=0)\n","            results.append(result)\n","\n","            if metrics_col is None:\n","                metrics_col = cv_model.metrics\n","        except Exception as e:\n","            print(f\"CV error on fold with exception: {e}\")\n","\n","    if metrics_col is None:\n","        raise ValueError(\"No successful model training; metrics_col is None\")\n","\n","    metrics_names = [metric.name for metric in metrics_col]\n","    results_df = pd.DataFrame(results, columns=metrics_names)\n","\n","    return results_df\n","\n","# results_df = train_cv_model(train_ts_df.drop(columns=[META_LABEL]), train_ts_df[META_LABEL], input_shape)\n","# results_df"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":4755137,"sourceId":8061237,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
