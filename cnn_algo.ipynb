{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","id":"oaDoHbxVH0CW"},"source":["# CNN"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","id":"z_cBqdYOoY5S"},"source":["## Notebook's Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","collapsed":false,"id":"eETPYJLiMU-b","jupyter":{"outputs_hidden":false},"outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"outputs":[],"source":["INSTALL_DEPS = False\n","if INSTALL_DEPS:\n","  %pip install hurst==0.0.5\n","  %pip install imbalanced_learn==0.12.3\n","  %pip install imblearn==0.0\n","  %pip install lime==0.2.0.1\n","  %pip install matplotlib==3.8.3\n","  %pip install numpy==1.26.4\n","  %pip install pandas==2.2.2\n","  %pip install pandas_market_calendars==4.4.0\n","  %pip install protobuf==5.27.0\n","  %pip install pykalman==0.9.7\n","  %pip install scikit_learn==1.4.2\n","  %pip install scipy==1.13.1\n","  %pip install seaborn==0.13.2\n","  %pip install shap==0.45.1\n","  %pip install statsmodels==0.14.2\n","  %pip install tensorboard==2.15.2\n","  %pip install tensorflow==2.15.1\n","  %pip install tensorflow_intel==2.15.1\n","  %pip install tqdm==4.66.4\n","  %pip install yfinance==0.2.37\n","\n","\n","!python --version"]},{"cell_type":"markdown","metadata":{},"source":["## Cloud Environment Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","collapsed":false,"id":"Q4-GoceIIfT_","jupyter":{"outputs_hidden":false},"outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","IN_KAGGLE = IN_COLAB = False\n","try:\n","    # https://www.tensorflow.org/install/pip#windows-wsl2\n","    import google.colab\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/drive\")\n","    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n","    MODEL_PATH = \"/content/drive/MyDrive/models\"\n","    IN_COLAB = True\n","    print(\"Colab!\")\n","except:\n","    IN_COLAB = False\n","if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and not IN_COLAB:\n","    print(\"Running in Kaggle...\")\n","    for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n","        for filename in filenames:\n","            print(os.path.join(dirname, filename))\n","    MODEL_PATH = \"./models\"\n","    DATA_PATH = \"/kaggle/input/Intra-Day Agriculture Futures Trades 2023-2024\"\n","    IN_KAGGLE = True\n","    print(\"Kaggle!\")\n","elif not IN_COLAB:\n","    IN_KAGGLE = False\n","    MODEL_PATH = \"./models\"\n","    DATA_PATH = \"./data/\"\n","    print(\"running localhost!\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import mixed_precision\n","\n","print(f'Tensorflow version: [{tf.__version__}]')\n","\n","tf.get_logger().setLevel('INFO')\n","\n","#tf.config.set_soft_device_placement(True)\n","#tf.config.experimental.enable_op_determinism()\n","#tf.random.set_seed(1)\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","\n","  tf.config.experimental_connect_to_cluster(tpu)\n","  tf.tpu.experimental.initialize_tpu_system(tpu)\n","  strategy = tf.distribute.TPUStrategy(tpu)\n","except Exception as e:\n","  # Not an exception, just no TPUs available, GPU is fallback\n","  # https://www.tensorflow.org/guide/mixed_precision\n","  print(e)\n","  policy = mixed_precision.Policy('mixed_float16')\n","  mixed_precision.set_global_policy(policy)\n","  gpus = tf.config.experimental.list_physical_devices('GPU')\n","  if len(gpus) > 0:\n","    try:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=12288)])\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        strategy = tf.distribute.MirroredStrategy()\n","\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","    except RuntimeError as e:\n","        print(e)\n","    finally:\n","        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n","  else:\n","    # CPU is final fallback\n","    strategy = tf.distribute.get_strategy()\n","    print(\"Running on CPU\")\n","\n","def is_tpu_strategy(strategy):\n","    return isinstance(strategy, tf.distribute.TPUStrategy)\n","\n","print(\"Number of accelerators:\", strategy.num_replicas_in_sync)\n","os.getcwd()"]},{"cell_type":"markdown","metadata":{},"source":["# Instruments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from constants import *\n","\n","FEATURES_SELECTED = ['10Y_Barcount', '10Y_Spread', '10Y_Volume', '2YY_Spread', '2YY_Volume',\n","                    'CONTRA', 'Filtered_X', 'KG_X', 'KG_Z1', 'RTY_Spread', 'SD', 'Spread',\n","                    'TSMOM', 'VXM_Open', 'VXM_Spread', 'Volume']\n","TARGET_FUT, INTERVAL"]},{"cell_type":"markdown","metadata":{},"source":["## Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","filename = f\"{DATA_PATH}{os.sep}futures_{INTERVAL}.csv\"\n","print(filename)\n","futs_df = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n","\n","print(futs_df.shape)\n","futs_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from quant_equations import get_ou, get_annualized_factor, calc_annualized_sharpe, deflated_sharpe_ratio\n","\n","HALF_LIFE, HURST = get_ou(futs_df, f'{TARGET_FUT}_Close')\n","\n","print(\"Half-Life:\", HALF_LIFE)\n","print(\"Hurst:\", HURST)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 4))\n","\n","plt.plot(futs_df[f'{TARGET_FUT}_Close'], label=f'{TARGET_FUT} Close', alpha=0.7)\n","plt.title(f'{TARGET_FUT} Price')\n","plt.xlabel('Date')\n","plt.ylabel('Price')\n","plt.legend()\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare the Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from models import process_futures, process_exog\n","import pickle\n","\n","TEST_SPLIT = 0.6\n","TRAIN_SIZE = int(len(futs_df) * TEST_SPLIT)\n","CACHE = True\n","FUTURES_TMP_FILE = \"./tmp/futures.pkl\"\n","os.makedirs(\"./tmp/\", exist_ok=True)\n","\n","with strategy.scope():\n","    if not os.path.exists(FUTURES_TMP_FILE):\n","        futs_exog_df = process_exog(MARKET_FUTS, futs_df)\n","        train_agri_ts, val_agri_ts, scalers = process_futures(FUTS, futs_df, futs_exog_df, TRAIN_SIZE, INTERVAL)\n","        if CACHE:\n","            with open(FUTURES_TMP_FILE, 'wb') as f:\n","                pickle.dump((train_agri_ts, val_agri_ts, scalers), f)\n","    else:\n","        with open(FUTURES_TMP_FILE, 'rb') as f:\n","            train_agri_ts, val_agri_ts, scalers = pickle.load(f)\n","\n","# Stacking the lists of dataframes into single dataframes\n","# train_ts_df = pd.concat([df.reset_index(drop=True) for df in train_agri_ts], axis=0, ignore_index=True).dropna()\n","# test_ts_df = pd.concat([df.reset_index(drop=True) for df in val_agri_ts], axis=0, ignore_index=True).dropna()\n","\n","# test_ts_df.tail(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from models import aug_metalabel_mr, META_LABEL\n","from tqdm import tqdm\n","\n","PREDICTION_HORIZON = 1\n","WINDOW  = HALF_LIFE\n","WINDOW_TMP_PATH = \"./tmp/\"\n","\n","def prepare_windows(\n","    data_df,\n","    label_df,\n","    window_size=WINDOW,\n","    horizon=PREDICTION_HORIZON,\n","):\n","    assert len(data_df) > 1\n","    X, y = [], []\n","    for i in tqdm(\n","        range(len(data_df) - window_size - horizon + 1), desc=f\"Encoding Widows of {window_size} with {horizon} horizon.\"\n","    ):\n","        input_window = data_df.iloc[i : i + window_size].values\n","        X.append(input_window)\n","        if label_df is not None:\n","            target_window = label_df.iloc[i + window_size : i + window_size + horizon].values\n","            y.append(target_window)\n","\n","    return np.array(X),  np.array(y)\n","\n","def prepare_windows_with_disjoint_ts(\n","    ts_list,\n","    window_size=WINDOW,\n","    horizon=PREDICTION_HORIZON,\n","):\n","    Xs, ys = [], []\n","\n","    # Iterate over each disconnected time series and build windows there.\n","    for data_df in ts_list:\n","        assert len(data_df) > 1\n","        data_df = aug_metalabel_mr(data_df)\n","        X, y = prepare_windows(data_df[FEATURES_SELECTED], data_df[META_LABEL], window_size=window_size, horizon=horizon)\n","\n","        Xs.append(X)\n","        ys.append(y)\n","\n","    Xs = np.array(Xs)\n","    ys = np.array(ys)\n","    Xs = np.concatenate(Xs, axis=0)\n","    ys = np.concatenate(ys, axis=0)\n","    return Xs,  ys\n","\n","with strategy.scope():\n","    if not os.path.exists(WINDOW_TMP_PATH+ \"X.pkl\"):\n","        X, y = prepare_windows_with_disjoint_ts(train_agri_ts)\n","        X_t, y_t = prepare_windows_with_disjoint_ts(val_agri_ts)\n","        if CACHE:\n","            with open(WINDOW_TMP_PATH + \"X.pkl\", 'wb') as f:\n","                pickle.dump(X, f)\n","            with open(WINDOW_TMP_PATH + \"y.pkl\", 'wb') as f:\n","                pickle.dump(y, f)\n","            with open(WINDOW_TMP_PATH + \"X_t.pkl\", 'wb') as f:\n","                pickle.dump(X_t, f)\n","            with open(WINDOW_TMP_PATH + \"y_t.pkl\", 'wb') as f:\n","                pickle.dump(y_t, f)\n","    else:\n","        with open(WINDOW_TMP_PATH + \"X.pkl\", 'rb') as f:\n","            X = pickle.load(f)\n","        with open(WINDOW_TMP_PATH + \"y.pkl\", 'rb') as f:\n","            y = pickle.load(f)\n","        with open(WINDOW_TMP_PATH + \"X_t.pkl\", 'rb') as f:\n","            X_t = pickle.load(f)\n","        with open(WINDOW_TMP_PATH + \"y_t.pkl\", 'rb') as f:\n","            y_t = pickle.load(f)\n","\n","print(f\"shapes: {X.shape} and {y.shape}\")\n","X_t[0], y_t[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TPU see: https://github.com/tensorflow/tensorflow/issues/41635\n","BATCH_SIZE = 8  * strategy.num_replicas_in_sync # Default 8\n","print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n","\n","def create_tf_dataset(X, y, batch_size=BATCH_SIZE):\n","    def generator():\n","        for features, labels in zip(X, y):\n","            yield features, labels\n","\n","    dataset = tf.data.Dataset.from_generator(generator, output_signature=(\n","        tf.TensorSpec(shape=X.shape[1:], dtype=tf.float32),\n","        tf.TensorSpec(shape=y.shape[1:], dtype=tf.float32)\n","    ))\n","\n","    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","    return dataset\n","\n","with strategy.scope():\n","    train_dataset = create_tf_dataset(X, y)\n","    val_dataset = create_tf_dataset(X_t, y_t)"]},{"cell_type":"markdown","metadata":{},"source":["# CNN Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import (\n","    SpatialDropout1D,\n","    Dense,\n","    Conv1D,\n","    Layer,\n","    Add,\n","    Input,\n","    Concatenate,\n","    Flatten,\n",")\n","from tensorflow.keras import Model\n","from tensorflow.keras.regularizers import L1L2\n","\n","\n","MODEL_NAME = \"WAVENET\"\n","EPOCHS = 300\n","PATIENCE_EPOCHS = 15\n","MIN_FILTER = HALF_LIFE//2 + 2\n","MIN_DENSE = HALF_LIFE//2\n","FILTERS = [MIN_FILTER]\n","HIDDEN_DENSE = [MIN_DENSE]\n","BIAS = True\n","DROPRATE = 0.45\n","POOL_SIZE = HALF_LIFE // 2\n","KERNEL_SIZE = HALF_LIFE // 2\n","DILATION_RATE = 1\n","REG_WEIGHTS = 0.001\n","\n","\n","tf.keras.saving.get_custom_objects().clear()\n","\n","@tf.keras.saving.register_keras_serializable()\n","class ConvBlock(Layer):\n","    \"\"\"\n","    CNN Residual Block that uses zero-padding to maintain `steps` value of the ouput equal to the one in the input.\n","    Residual Block is obtained by stacking togeather (2x) the following:\n","        - 1D Dilated Convolution\n","        - ReLu\n","        - Spatial Dropout\n","    And adding the input after trasnforming it with a 1x1 Conv\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        filters=1,\n","        kernel_size=2,\n","        dilation_rate=1,\n","        kernel_initializer=\"glorot_normal\",\n","        bias_initializer=\"glorot_normal\",\n","        kernel_regularizer=None,\n","        bias_regularizer=None,\n","        use_bias=False,\n","        dropout_rate=0.0,\n","        layer_id=None,\n","        **kwargs,\n","    ):\n","        super(ConvBlock, self).__init__(**kwargs)\n","        assert dilation_rate is not None and dilation_rate > 0 and filters > 0 and kernel_size > 0\n","\n","        self.filters = filters\n","        self.kernel_size = kernel_size\n","        self.dilation_rate = dilation_rate\n","        self.kernel_initializer = kernel_initializer\n","        self.bias_initializer = bias_initializer\n","        self.kernel_regularizer = kernel_regularizer\n","        self.bias_regularizer = bias_regularizer\n","        self.use_bias = use_bias\n","        self.dropout_rate = dropout_rate\n","        self.layer_id = str(layer_id)\n","\n","    def get_config(self):\n","        config = super(ConvBlock, self).get_config()\n","        config.update({\n","            'filters': self.filters,\n","            'kernel_size': self.kernel_size,\n","            'dilation_rate': self.dilation_rate,\n","            'kernel_initializer': self.kernel_initializer,\n","            'bias_initializer': self.bias_initializer,\n","            'kernel_regularizer': self.kernel_regularizer,\n","            'bias_regularizer': self.bias_regularizer,\n","            'use_bias': self.use_bias,\n","            'dropout_rate': self.dropout_rate,\n","        })\n","        return config\n","\n","    def build(self, inputs):\n","        self.conv1 = Conv1D(\n","            filters=self.filters,\n","            kernel_size=self.kernel_size,\n","            use_bias=self.use_bias,\n","            bias_initializer=self.bias_initializer,\n","            bias_regularizer=self.bias_regularizer,\n","            kernel_initializer=self.kernel_initializer,\n","            kernel_regularizer=self.kernel_regularizer,\n","            padding=\"causal\",\n","            dilation_rate=self.dilation_rate,\n","            activation=\"relu\",\n","            name=f\"Conv1D_1_{self.layer_id}\"\n","        )\n","        # Spatial dropout is specific to convolutions by dropping an entire timewindow,\n","        # not to rely too heavily on specific features detected by the kernels.\n","        self.dropout1 = SpatialDropout1D(\n","            self.dropout_rate, trainable=True, name=f\"SpatialDropout1D_1_{self.layer_id}\"\n","        )\n","        # Capture a higher order feature set from the previous convolution\n","        self.conv2 = Conv1D(\n","            filters=self.filters,\n","            kernel_size=self.kernel_size,\n","            use_bias=self.use_bias,\n","            bias_initializer=self.bias_initializer,\n","            bias_regularizer=self.bias_regularizer,\n","            kernel_initializer=self.kernel_initializer,\n","            kernel_regularizer=self.kernel_regularizer,\n","            padding=\"causal\",\n","            dilation_rate=self.dilation_rate,\n","            activation=\"relu\",\n","            name=f\"Conv1D_2_{self.layer_id}\"\n","        )\n","        self.dropout2 = SpatialDropout1D(\n","            self.dropout_rate, trainable=True, name=f\"SpatialDropout1D_2_{self.layer_id}\"\n","        )\n","        # The skip connection is an addition of the input to the block with the output of the second dropout layer.\n","        # Solves vanishing gradient, carries info from earlier layers to later layers, allowing gradients to flow across this alternative path.\n","        # Does not learn direct mappings, but differences (residuals) while keeping temporal context.\n","        # Note how it keeps dims intact with kernel 1.\n","        self.skip_out = Conv1D(\n","            filters=self.filters,\n","            kernel_size=1,\n","            activation=\"linear\",\n","            padding=\"same\",\n","            name=f\"Conv1D_skipconnection_{self.layer_id}\",\n","        )\n","        # This is the elementwise add for the residual connection and Conv1d 2's output\n","        self.residual_out = Add(name=f\"residual_Add_{self.layer_id}\")\n","\n","    def call(self, inputs):\n","        x = self.conv1(inputs)\n","        x = self.dropout1(x)\n","        x = self.conv2(x)\n","        x = self.dropout2(x)\n","\n","        # Residual output by adding the inputs back\n","        skip_out_x = self.skip_out(inputs)\n","        x = self.residual_out([x, skip_out_x])\n","        return x, skip_out_x\n","\n","\n","def CNN(\n","    input_shape,\n","    dense_units=None,\n","    output_horizon=PREDICTION_HORIZON,\n","    filters=[MIN_FILTER],\n","    kernel_size=KERNEL_SIZE,\n","    dilation_rate=DILATION_RATE,\n","    kernel_regularizer=None,\n","    bias_regularizer=None,\n","    use_bias=False,\n","    dropout_rate=DROPRATE,\n","):\n","    \"\"\"\n","    Tensorflow CNN Model builder.\n","    see: https://www.tensorflow.org/api_docs/python/tf/keras/Model\n","    see: https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing#the_model_class\n","    see: https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/L2\n","    \"\"\"\n","    main_input = Input(shape=input_shape, name=\"main_input\")\n","    x = main_input\n","    skip_connections = []\n","    for i, filter in enumerate(filters):\n","        x, x_skip = ConvBlock(\n","            filters=filter,\n","            kernel_size=kernel_size,\n","            dilation_rate=dilation_rate ** (i + 1),\n","            kernel_regularizer=kernel_regularizer,\n","            bias_regularizer=bias_regularizer,\n","            use_bias=use_bias,\n","            dropout_rate=dropout_rate,\n","            layer_id=i,\n","        )(x)\n","        skip_connections.append(x_skip)\n","    if skip_connections:\n","        skip_connections.append(x)\n","        aggregated = Concatenate(axis=-1, name=f\"Final_Residuals\")(skip_connections)\n","        aggregated = Conv1D(filters[-1], kernel_size=1, activation=\"linear\", padding='same')(aggregated)\n","    if dense_units:\n","        # Dense networks for deep learning ifrequired.\n","        x = Flatten()(x)\n","        # First layer\n","        x = Dense(dense_units[0], input_shape=input_shape, activation=\"tanh\", name=f\"Dense_0\")(x)\n","        for i, units  in enumerate(dense_units, start=1):\n","            x = Dense(units , activation=\"tanh\", name=f\"Dense__{i}\")(x)\n","        # Last layer - Logits output\n","        x = Dense(output_horizon, activation=None, name=f\"Dense_Classifier\")(x)\n","    else:\n","        x = Conv1D(filters=output_horizon, kernel_size=1, padding=\"causal\", activation=None, name=f\"Conv_Classifier\")(x)\n","\n","    model = Model(\n","        inputs=[main_input],\n","        outputs=x,\n","        name=MODEL_NAME,\n","    )\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","from tensorflow.keras.losses import BinaryFocalCrossentropy\n","from tensorflow.keras.metrics import AUC, BinaryAccuracy\n","from tensorflow.keras.callbacks import EarlyStopping, TensorBoard,ReduceLROnPlateau\n","from tensorflow.keras.optimizers import AdamW\n","from tensorflow.keras.mixed_precision import LossScaleOptimizer\n","\n","with strategy.scope():\n","    MODEL_DIR = f\"models/{MODEL_NAME}\"\n","    IMAGES_DIR = f\"images/{MODEL_NAME}/images\"\n","    LOG_BASEPATH = f\"logs/{MODEL_NAME}/tb\"\n","\n","    weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y.flatten()), y=y.flatten().flatten())\n","    CLASS_WEIGHTS = {0: weights[0], 1: weights[1]}\n","    LEARN_RATE = 0.001 # ReduceLROnPlateau will alter this.\n","    ERROR_ALPHA = 0.6 # 0.5 > gives more weight to positive class errors. The weight for class 0 is 1.0 - alpha.\n","    ERROR_GAMMA = 0.8 # focal factor\" to down-weight easy examples loss contribution. 0 > focus on hard examples.\n","    TARGET_METRIC = \"auc\"\n","    # https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryFocalCrossentropy\n","    LOSS = BinaryFocalCrossentropy(apply_class_balancing=False, from_logits=True, alpha=ERROR_ALPHA, gamma=ERROR_GAMMA, name='fbce', reduction=\"auto\")\n","    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy\n","    METRICS = [BinaryAccuracy(name='ba'), AUC(name=TARGET_METRIC, from_logits=True)]\n","\n","    def build_cnn(\n","        input_shape,\n","        X,\n","        y=None, # if dataset\n","        Xt=None,\n","        yt=None, # if dataset\n","        output_horizon= PREDICTION_HORIZON,\n","        filters= FILTERS,\n","        kernel_size= KERNEL_SIZE,\n","        dilation_rate= DILATION_RATE,\n","        kernel_regularizer=L1L2(l1= REG_WEIGHTS, l2=REG_WEIGHTS//10),\n","        bias_regularizer=L1L2(l1= REG_WEIGHTS, l2=REG_WEIGHTS//10),\n","        dropout_rate=DROPRATE,\n","        dense_units=HIDDEN_DENSE,\n","        lr=LEARN_RATE,\n","        patience=PATIENCE_EPOCHS,\n","        epochs=EPOCHS,\n","        batch_size=BATCH_SIZE,\n","        use_bias=BIAS,\n","        loss=LOSS,\n","        class_weights=CLASS_WEIGHTS,\n","        b_cv = False,\n","    ):\n","        model = CNN(\n","            input_shape=input_shape,\n","            dense_units=dense_units,\n","            output_horizon=output_horizon,\n","            filters=filters,\n","            kernel_size=kernel_size,\n","            dilation_rate=dilation_rate,\n","            kernel_regularizer=kernel_regularizer,\n","            bias_regularizer=bias_regularizer,\n","            use_bias=use_bias,\n","            dropout_rate=dropout_rate,\n","        )\n","        # AdamW for deep network.\n","        optimizer = AdamW(learning_rate=lr, clipvalue=1.)\n","        if not is_tpu_strategy(strategy):\n","            # TPUs already use bfloat16\n","            optimizer = LossScaleOptimizer(optimizer, dynamic=True)\n","        model.compile(loss=loss, optimizer=optimizer, metrics=METRICS)\n","        callbacks = [EarlyStopping(\n","                        patience=patience,\n","                        monitor=f\"{TARGET_METRIC}\",\n","                        restore_best_weights=True,\n","                    ),\n","                    ReduceLROnPlateau(monitor=f\"{TARGET_METRIC}\",\n","                                    factor=0.5,\n","                                    patience=patience // 2,\n","                                    verbose=1 if not b_cv else 0,\n","                                    min_lr=1e-3\n","                    )]\n","        callbacks.append(TensorBoard(log_dir=LOG_BASEPATH,\n","                                    histogram_freq=2,\n","                                    write_graph=True,\n","                                    write_images=True,\n","                                    update_freq='epoch',\n","                                    profile_batch=2))\n","\n","        history = model.fit(\n","            X,\n","            validation_data=Xt,\n","            epochs=epochs,\n","            batch_size=batch_size,\n","            callbacks=callbacks,\n","            class_weight=class_weights,\n","            verbose=1 if not b_cv else 0,\n","        )\n","\n","        return model, history\n","\n","\n","    input_shape = (\n","        WINDOW,\n","        1 if len(X.shape) < 3 else X.shape[2],\n","    )\n","    print(f\"input_shape: {input_shape}, X: {X.shape}, y: {y.shape}\")\n","\n","    model, history = build_cnn(input_shape, X=train_dataset, Xt=val_dataset)\n","    model.save(MODEL_PATH)\n","    model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score, roc_auc_score\n","\n","ypred_raw = model.predict([X_t])\n","pred = (ypred_raw > 0.5).astype(int)\n","metrics = {}\n","\n","metrics = {\n","    \"Accuracy\": accuracy_score(y_t.flatten(), pred.flatten()),\n","    \"Precision\": precision_score(y_t.flatten(), pred.flatten()),\n","    \"Recall\": recall_score(y_t.flatten(), pred.flatten()),\n","    \"F1b Score\": fbeta_score(y_t.flatten(), pred.flatten(), average=\"weighted\", beta=0.1),\n","    \"ROC AUC\": roc_auc_score(y_t.flatten(), ypred_raw.flatten(), average='weighted')  # Using raw probabilities\n","}\n","\n","metrics_unseen_df = pd.DataFrame.from_dict(metrics, orient='index')\n","metrics_unseen_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.math import confusion_matrix\n","import seaborn as sns\n","\n","def plot_confusion_matrix(cm, labels, cm2=None, labels2=None):\n","        plt.figure(figsize=(8 if cm2 is not None else 4, 4))\n","        if cm2 is not None:\n","            plt.subplot(1, 2, 1)\n","        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Accent)\n","\n","        df_cm = pd.DataFrame((cm / np.sum(cm, axis=1)[:, None])*100, index=[i for i in labels], columns=[i for i in labels])\n","        cm_plot1 = sns.heatmap(df_cm, annot=True,  fmt=\".2f\", cmap='Blues', xticklabels=labels, yticklabels=labels).get_figure()\n","        plt.xlabel('Predicted Labels')\n","        plt.ylabel('True Labels')\n","        plt.title('Confusion Matrix 1')\n","        tick_marks = np.arange(len(labels))\n","        plt.xticks(tick_marks, labels, rotation=45)\n","        plt.yticks(tick_marks, labels)\n","\n","        cm_plot2=None\n","        if cm2 is not None:\n","            plt.subplot(1, 2, 2)\n","            df_cm = pd.DataFrame((cm2 / np.sum(cm2, axis=1)[:, None])*100, index=[i for i in labels2], columns=[i for i in labels2])\n","            cm_plot12 = sns.heatmap(df_cm, annot=True,  fmt=\".2f\", cmap='Reds', xticklabels=labels, yticklabels=labels).get_figure()\n","            plt.xlabel('Predicted Labels')\n","            plt.title('Confusion Matrix 2')\n","        plt.tight_layout()\n","\n","        return cm_plot1, cm_plot2\n","\n","\n","cm = confusion_matrix(y_t.flatten(), pred)\n","figure, _ = plot_confusion_matrix(cm, labels=[1,0])"]},{"cell_type":"markdown","metadata":{},"source":["# Grid Search"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n","from tensorboard.plugins.hparams import api as hp\n","from tensorflow.summary import create_file_writer\n","import json\n","\n","HP_KERNEL_SIZE = hp.HParam(\"kernel_size\", hp.Discrete([KERNEL_SIZE * 2, KERNEL_SIZE]))\n","HP_BATCH_SIZE = hp.HParam(\"batch_size\", hp.Discrete([BATCH_SIZE]))\n","HP_EPOCHS = hp.HParam(\"epochs\", hp.Discrete([EPOCHS]))\n","HP_DILATION_RATE = hp.HParam(\"dilation_rate\", hp.Discrete([DILATION_RATE]))\n","HP_DROPOUT_RATE = hp.HParam(\"dropout_rate\", hp.Discrete([DROPRATE, DROPRATE*2]))\n","HP_REG_WEIGHTS = hp.HParam(\"reg_weight\", hp.Discrete([REG_WEIGHTS, REG_WEIGHTS*2]))\n","HP_LEARNING_RATE = hp.HParam(\"learning_rate\", hp.Discrete([LEARN_RATE]))\n","HP_PATIENCE = hp.HParam(\"patience\", hp.Discrete([PATIENCE_EPOCHS]))\n","HP_BIAS = hp.HParam(\"bias\", hp.Discrete([BIAS, False]))\n","HP_ALPHA = hp.HParam(\"alpha\", hp.Discrete([ERROR_ALPHA, ERROR_ALPHA-0.5, ERROR_ALPHA+0.5]))\n","HP_GAMMA = hp.HParam(\"gamma\", hp.Discrete([ERROR_GAMMA, ERROR_GAMMA-0.5, ERROR_GAMMA+0.5]))\n","HP_HIDDEN_DENSE = hp.HParam(\"dense_units\", hp.Discrete([\n","    f\"{WINDOW}\",\n","    f\"{WINDOW*2}_{WINDOW}\",\n","    f\"{WINDOW*2}_{WINDOW}_{WINDOW//2}\",\n","    f\"{WINDOW*4}_{WINDOW*2}\",\n","]))\n","HP_FILTERS = hp.HParam(\"filters\", hp.Discrete([\n","    f\"{MIN_FILTER}\",\n","    f\"{MIN_FILTER}_{MIN_FILTER*2}\",\n","    f\"{MIN_FILTER*2}_{MIN_FILTER*4}_{MIN_FILTER*8}\",\n","    f\"{MIN_FILTER}_{MIN_FILTER*8}\",\n","]))\n","HPARAMS = [\n","    HP_FILTERS,\n","    HP_KERNEL_SIZE,\n","    HP_BATCH_SIZE,\n","    HP_EPOCHS,\n","    HP_DILATION_RATE,\n","    HP_DROPOUT_RATE,\n","    HP_REG_WEIGHTS,\n","    HP_LEARNING_RATE,\n","    HP_PATIENCE,\n","    HP_BIAS,\n","    HP_HIDDEN_DENSE,\n","    HP_ALPHA,\n","        HP_GAMMA\n","    ]\n","\n","def grid_search_build_cnn(input_shape, X, y, Xt=None, yt=None, hparams=HPARAMS, file_name=f\"best_params.json\", checkpoint_file = f\"checkpoint.json\"):\n","    def _decode_arrays(config_str):\n","        return [int(unit) for unit in config_str.split('_')]\n","\n","    def _save_best_params(best_params, best_loss, best_metric, other_metrics = None, file_name=\"best_params.json\"):\n","        os.makedirs(MODEL_DIR, exist_ok=True)\n","        with open(f\"{MODEL_DIR}/{file_name}\", \"w\") as file:\n","            json.dump({\"best_params\": best_params, \"best_loss\": best_loss, \"best_metric\": best_metric, 'other_metrics': other_metrics}, file)\n","\n","    def _load_checkpoint(file_name):\n","        json = None\n","        try:\n","            os.makedirs(MODEL_DIR, exist_ok=True)\n","            with open(f\"{MODEL_DIR}/{file_name}\", \"r\") as file:\n","                json = json.load(file)\n","        except Exception as e:\n","            print(f\"File {MODEL_DIR}/{file_name} not found or error {e}\")\n","        return json\n","\n","    def _save_checkpoint(state, file_name):\n","        os.makedirs(MODEL_DIR, exist_ok=True)\n","        with open(f\"{MODEL_DIR}/{file_name}\", \"w\") as file:\n","            json.dump(state, file)\n","\n","    with create_file_writer(f\"{LOG_BASEPATH}/hparam_tuning\").as_default():\n","        hp.hparams_config(\n","            hparams=hparams,\n","            metrics=[hp.Metric(TARGET_METRIC, display_name=TARGET_METRIC)],\n","        )\n","\n","    start_index = 0\n","    best_loss = np.inf\n","    best_metric = -np.inf\n","    best_params = None\n","    checkpoint = _load_checkpoint(checkpoint_file)\n","    if checkpoint:\n","        start_index = checkpoint['next_index']\n","        best_loss = checkpoint['best_loss']\n","        best_metric = checkpoint['best_metric']\n","        best_params = checkpoint['best_params']\n","\n","    grid = list(ParameterGrid({h.name: h.domain.values for h in hparams}))\n","    for index, hp_values in enumerate(tqdm(grid[start_index:], desc=\"Grid Search..\"), start=start_index):\n","        dense_units = _decode_arrays(hp_values[\"dense_units\"])\n","        filters = _decode_arrays(hp_values[\"filters\"])\n","        b = hp_values[\"bias\"]\n","        k = hp_values[\"kernel_size\"]\n","        d = hp_values[\"dilation_rate\"]\n","        rw = hp_values[\"reg_weight\"]\n","        drop = hp_values[\"dropout_rate\"]\n","\n","        ERROR_ALPHA = hp_values[\"alpha\"]\n","        ERROR_GAMMA = hp_values[\"gamma\"]\n","        print(f\"Shapes{input_shape}: x{X[0].shape}xg{X[1].shape}y{y.shape}, filters {filters}, dense {dense_units}, k: {k}, d: {d}, rw: {rw}, drop: {drop}, b: {b}, alpha: {ERROR_ALPHA},  gamma: {ERROR_GAMMA}\")\n","\n","        model, history = build_cnn(input_shape, X, y,\n","                                    output_horizon=PREDICTION_HORIZON,\n","                                    Xt=Xt, yt=yt,\n","                                    filters=filters,\n","                                    kernel_size=k,\n","                                    dilation_rate=d,\n","                                    kernel_regularizer=L1L2(l1=rw, l2=rw),\n","                                    bias_regularizer=L1L2(l1=rw, l2=rw),\n","                                    dropout_rate=drop,\n","                                    dense_units=dense_units,\n","                                    use_bias=b,\n","                                    b_cv=True)\n","        loss = history.history[f\"val_loss\"][-1]\n","        metric = history.history[f\"val_{TARGET_METRIC}\"][-1]\n","        if (metric > best_metric):\n","            best_history = history\n","            best_loss = loss\n","            best_metric = metric\n","            best_model = model\n","            best_params = hp_values\n","            other_metrics = {\n","                f\"{TARGET_METRIC}\": history.history[f\"{TARGET_METRIC}\"][-1],\n","                f\"v_{TARGET_METRIC}\": history.history[f\"val_{TARGET_METRIC}\"][-1],\n","                'ba': history.history[\"ba\"][-1],\n","                'v_ba': history.history[\"val_ba\"][-1],\n","            }\n","            _save_best_params(best_params, best_loss, best_metric, other_metrics, file_name)\n","        checkpoint_state = {\n","            'next_index': index + 1,\n","            'best_loss': best_loss,\n","            'best_metric': best_metric,\n","            'best_params': best_params\n","        }\n","        _save_checkpoint(checkpoint_state, checkpoint_file)\n","    return best_model, best_history, best_params, best_loss, best_metric\n","\n","PARAM_SEARCH = False\n","if PARAM_SEARCH:\n","    with strategy.scope():\n","        assert not np.any(pd.isna(X)) and not np.any(np.isnan(X_t))\n","        print(f\"{X.shape}\")\n","        input_shape = (\n","            WINDOW,\n","            1 if len(X.shape) < 3 else X.shape[2],\n","        )\n","\n","        model, history, best_params, best_loss, best_metric = grid_search_build_cnn(input_shape, X=X, y=y, Xt=X_t, yt=y_t, hparams=HPARAMS)\n","        print(best_params)\n","        print(best_metric)"]},{"cell_type":"markdown","metadata":{},"source":["# CV"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import TimeSeriesSplit\n","\n","CV_MODEL = True\n","CV_SPLITS = 3\n","\n","def train_cv_model(X, y, input_shape, n_splits=5, perturb=True, window=WINDOW, horizon=PREDICTION_HORIZON):\n","    def _perturb_gaussiannoise(X, noise_level=0.1):\n","        sigma = noise_level * np.std(X)\n","        noise = np.random.normal(0, sigma, X.shape)\n","        return X + noise\n","\n","    if perturb:\n","        X = _perturb_gaussiannoise(X)\n","\n","    results = []\n","    tscv = TimeSeriesSplit(n_splits=n_splits)\n","    global metrics_col\n","    metrics_col = None\n","\n","    for train_index, test_index in tqdm(tscv.split(X), desc=f\"CV Testing for n_splits: {n_splits}\"):\n","        X_train = X.iloc[train_index]\n","        y_train = y.iloc[train_index]\n","        X_test = X.iloc[test_index]\n","        y_test = y.iloc[test_index]\n","\n","        X_train_windows, y_train_windows = prepare_windows(X_train, y_train, window_size=window, horizon=horizon)\n","        X_test_windows, y_test_windows = prepare_windows(X_test, y_test, window_size=window, horizon=horizon)\n","\n","        try:\n","            cv_model, _ = build_cnn(input_shape, X=X_train_windows, y=y_train_windows, Xt=X_test_windows, yt=y_test_windows)\n","\n","            result = cv_model.evaluate([X_test_windows], y_test_windows, verbose=0)\n","            results.append(result)\n","\n","            if metrics_col is None:\n","                metrics_col = cv_model.metrics\n","        except Exception as e:\n","            print(f\"CV error on fold with exception: {e}\")\n","\n","    if metrics_col is None:\n","        raise ValueError(\"No successful model training; metrics_col is None\")\n","\n","    metrics_names = [metric.name for metric in metrics_col]\n","    results_df = pd.DataFrame(results, columns=metrics_names)\n","\n","    return results_df\n","\n","# results_df = train_cv_model(train_ts_df.drop(columns=[META_LABEL]), train_ts_df[META_LABEL], input_shape)\n","# results_df"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":4755137,"sourceId":8061237,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
