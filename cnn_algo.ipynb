{"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":4755137,"sourceId":8061237,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CNN","metadata":{"_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","id":"oaDoHbxVH0CW"}},{"cell_type":"markdown","source":"## Notebook's Environment","metadata":{"_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","id":"z_cBqdYOoY5S"}},{"cell_type":"code","source":"INSTALL_DEPS = True\nif INSTALL_DEPS:\n  %pip install hurst==0.0.5\n  %pip install imbalanced_learn==0.12.3\n  %pip install imblearn==0.0\n  %pip install protobuf==5.27.0\n  %pip install pykalman==0.9.7\n  %pip install tqdm==4.66.4\n\n!python --version","metadata":{"_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","collapsed":false,"id":"eETPYJLiMU-b","jupyter":{"outputs_hidden":false},"outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cloud Environment Setup","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nIN_KAGGLE = IN_COLAB = False\ntry:\n    # https://www.tensorflow.org/install/pip#windows-wsl2\n    import google.colab\n    from google.colab import drive\n\n    drive.mount(\"/content/drive\")\n    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n    MODEL_PATH = \"/content/drive/MyDrive/models\"\n    IN_COLAB = True\n    print(\"Colab!\")\nexcept:\n    IN_COLAB = False\nif \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and not IN_COLAB:\n    print(\"Running in Kaggle...\")\n    for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n    MODEL_PATH = \"./models\"\n    DATA_PATH = \"/kaggle/input/intra-day-agriculture-futures-trades-2023-2024\"\n    IN_KAGGLE = True\n    print(\"Kaggle!\")\nelif not IN_COLAB:\n    IN_KAGGLE = False\n    MODEL_PATH = \"./models\"\n    DATA_PATH = \"./data/\"\n    print(\"running localhost!\")","metadata":{"_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","collapsed":false,"id":"Q4-GoceIIfT_","jupyter":{"outputs_hidden":false},"outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import mixed_precision\n\nprint(f'Tensorflow version: [{tf.__version__}]')\n\ntf.get_logger().setLevel('INFO')\n\n#tf.config.set_soft_device_placement(True)\n#tf.config.experimental.enable_op_determinism()\n#tf.random.set_seed(1)\ntry:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n\n  tf.config.experimental_connect_to_cluster(tpu)\n  tf.tpu.experimental.initialize_tpu_system(tpu)\n  strategy = tf.distribute.TPUStrategy(tpu)\nexcept Exception as e:\n  gpus = tf.config.experimental.list_physical_devices('GPU')\n  if len(gpus) > 0:\n    try:\n        strategy = tf.distribute.MirroredStrategy()\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)\n    finally:\n        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n  else:\n    # CPU is final fallback\n    strategy = tf.distribute.get_strategy()\n    print(\"Running on CPU\")\n\ndef is_tpu_strategy(strategy):\n    return isinstance(strategy, tf.distribute.TPUStrategy)\n\nprint(\"Number of accelerators:\", strategy.num_replicas_in_sync)\nos.getcwd()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instruments","metadata":{}},{"cell_type":"markdown","source":"## Data Load","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom algo_trading_utility_script import *\n\nfilename = f\"{DATA_PATH}{os.sep}futures_{INTERVAL}.csv\"\nprint(filename)\nfuts_df = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n\nprint(futs_df.shape)\n\nHALF_LIFE, HURST = get_ou(futs_df, f'{TARGET_FUT}_Close')\n\nprint(\"Half-Life:\", HALF_LIFE)\nprint(\"Hurst:\", HURST)\n\nfuts_df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\n\nplt.plot(futs_df[f'{TARGET_FUT}_Close'], label=f'{TARGET_FUT} Close', alpha=0.7)\nplt.title(f'{TARGET_FUT} Price')\nplt.xlabel('Date')\nplt.ylabel('Price')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare the Data","metadata":{}},{"cell_type":"code","source":"import pickle\nfrom tqdm import tqdm\n\nfrom sklearn.preprocessing import normalize\n\nTEST_SPLIT = 0.8\nTRAIN_SIZE = int(len(futs_df) * TEST_SPLIT)\nCACHE = True\nFUTURES_TMP_FILE = \"./tmp/futures.pkl\"\nos.makedirs(\"./tmp/\", exist_ok=True)\n\ndef oversample_mean_reversions(train_agri_ts, window, period=INTERVAL, hurst=HURST):\n    samples = []\n    for df in tqdm(train_agri_ts, desc=\"oversample_mean_reversions\"):\n        bb_df = df.copy()\n        results_df = param_search_bbs(bb_df, StockFeatExt.CLOSE, period, initial_window=window * 2, window_min=window // 2, hurst=hurst)\n        results_df = results_df[results_df[\"Metric\"] == \"Sharpe\"]\n        bb_df, _ = bollinger_band_backtest(bb_df, StockFeatExt.CLOSE, results_df[\"Window\"].iloc[0], period, std_factor=results_df[\"Standard_Factor\"].iloc[0])\n\n        samples.append(bb_df[train_agri_ts[0].columns].reset_index(drop=True))\n    return train_agri_ts + samples\n\ndef normalize_and_label_data(ts, meta_label=META_LABEL, cols_to_scale=COLS_TO_SCALE):\n    y0 = 0\n    y1 = 0\n    process_data = []\n    for df in tqdm(ts, desc=\"label_data\"):\n        df = aug_metalabel_mr(df)\n        y0  += (df[meta_label] == 0).sum()\n        y1  += (df[meta_label] > 0).sum()\n\n        if cols_to_scale is not None:\n            aug_df_scaled = normalize(df[cols_to_scale])\n            aug_df_scaled = pd.DataFrame(aug_df_scaled, columns=cols_to_scale)\n            df[cols_to_scale] = aug_df_scaled\n            df = df.loc[:, ~df.columns.duplicated(keep=\"first\")]\n\n        process_data.append(df)\n\n    total = y0 + y1\n    class_weight_0 = total / y0 if y0 != 0 else 0\n    class_weight_1 = total / y1 if y1 != 0 else 0\n    class_weights = {0: class_weight_0, 1: class_weight_1}\n\n    return process_data, class_weights\n\nwith strategy.scope():\n    if not os.path.exists(FUTURES_TMP_FILE):\n        futs_exog_df = process_exog(MARKET_FUTS, futs_df)\n        train_agri_ts, val_agri_ts = process_futures(FUTS, futs_df, futs_exog_df, TRAIN_SIZE, INTERVAL)\n        train_agri_ts = oversample_mean_reversions(train_agri_ts, HALF_LIFE)\n        val_agri_ts = oversample_mean_reversions(val_agri_ts, HALF_LIFE)\n\n        if CACHE:\n            with open(FUTURES_TMP_FILE, 'wb') as f:\n                pickle.dump((train_agri_ts, val_agri_ts), f)\n    else:\n        with open(FUTURES_TMP_FILE, 'rb') as f:\n            train_agri_ts, val_agri_ts = pickle.load(f)\n\ntrain_agri_ts, CLASS_WEIGHTS = normalize_and_label_data(train_agri_ts)\nval_agri_ts, val_weights = normalize_and_label_data(val_agri_ts)\n\nprint(f\"train: {CLASS_WEIGHTS}\")\nprint(f\"test: {val_weights}\")\nnp.shape(train_agri_ts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES = KF_COLS + BB_COLS + MOM_COLS + StockFeatExt.list\n\nsample = val_agri_ts[0]\nprint(sample[META_LABEL].value_counts())\n\nsampled_pattenrs = sample[sample[META_LABEL] > 0]\nsampled_pattenrs[FEATURES + [META_LABEL, \"Ret\"]].tail(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"WINDOW = 8\nWINDOW_TMP_PATH = \"./tmp/\"\n# TPU see: https://github.com/tensorflow/tensorflow/issues/41635\nBATCH_SIZE = 8  * strategy.num_replicas_in_sync # Default 8\nprint(f\"BATCH_SIZE: {BATCH_SIZE}\")\n\ndef prepare_windows(data_df, label_df, window_size=WINDOW):\n    \"\"\"\n    Prepare windows of features and corresponding labels for classification.\n\n    Parameters:\n    - data_df: DataFrame containing the features.\n    - label_df: DataFrame containing the labels.\n    - window_size: The size of the input window.\n\n    Returns:\n    - X: Array of input windows.\n    - y: Array of corresponding labels.\n    \"\"\"\n    X, y = [], []\n    for i in range(len(data_df) - window_size):\n        input_window = data_df.iloc[i : i + window_size].values\n        assert not np.isnan(input_window).any(), \"NaN values found in input window\"\n        X.append(input_window)\n        if label_df is not None:\n            target_label = label_df.iloc[i + window_size]\n            y.append([target_label])\n            assert not np.isnan(target_label).any(), \"NaN values found in target label\"\n    return np.array(X), np.array(y)\n\ndef prepare_windows_with_disjoint_ts(ts_list, window_size=WINDOW):\n    \"\"\"\n    Generator function to yield windows of features and corresponding labels from multiple time series.\n\n    Parameters:\n    - ts_list: List of DataFrames, each containing a time series.\n    - window_size: The size of the input window.\n\n    Yields:\n    - features: The input window of features.\n    - labels: The corresponding label.\n    \"\"\"\n    for data_df in ts_list:\n        X, y = prepare_windows(data_df[FEATURES], data_df[META_LABEL], window_size=window_size)\n        for features, labels in zip(X, y):\n            yield features, labels\n\ndef create_tf_dataset_from_generator(ts_list, window_size=WINDOW, batch_size=BATCH_SIZE):\n    \"\"\"\n    Create a TensorFlow dataset from a generator.\n\n    Parameters:\n    - ts_list: List of DataFrames, each containing a time series.\n    - window_size: The size of the input window.\n    - batch_size: The batch size for the dataset.\n\n    Returns:\n    - dataset: A TensorFlow dataset.\n    \"\"\"\n    dataset = tf.data.Dataset.from_generator(\n        lambda: prepare_windows_with_disjoint_ts(ts_list, window_size=window_size),\n        output_signature=(\n            tf.TensorSpec(shape=(window_size, len(FEATURES)), dtype=tf.float32),\n            tf.TensorSpec(shape=(1,), dtype=tf.float32)  # Assuming labels are floats for binary classification\n        )\n    )\n    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return dataset\n\nwith strategy.scope():\n    train_dataset = create_tf_dataset_from_generator(train_agri_ts)\n    val_dataset = create_tf_dataset_from_generator(val_agri_ts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN Architecture","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, Add, Multiply, Input, Flatten, Dense, AveragePooling1D, SpatialDropout1D, Activation, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\n\nMODEL_NAME = \"WAVENET\"\nMAX_DILATION = 8\nDILATION_RATE = 1\nEPOCHS = 30\nPATIENCE_EPOCHS = 8\nFILTERS = 10\nDROPRATE = 0.4\nKERNEL_SIZE = 4\nREG_WEIGHTS = 0.01\nCONVOLUTIONS = 8\nBATCH_SIZE = 10\nLEARN_RATTE =1e-4\n\ndef wavenet_block(inputs, layer_id, filters, kernel_size=KERNEL_SIZE, \n                  dilation_rate=DILATION_RATE, reg_param=REG_WEIGHTS, dropout_rate=DROPRATE):\n    conv_f = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding='causal',\n                    kernel_regularizer=l2(reg_param), name=f'conv_f_{layer_id}')(inputs)\n    conv_g = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding='causal',\n                    kernel_regularizer=l2(reg_param), name=f'conv_g_{layer_id}')(inputs)\n    tanh_out = Activation('tanh')(conv_f)\n    sigmoid_out = Activation('sigmoid')(conv_g)\n    merged = Multiply()([tanh_out, sigmoid_out])\n    merged = SpatialDropout1D(dropout_rate)(merged)\n\n    skip_out = Conv1D(filters, 1, padding='same', kernel_regularizer=l2(reg_param), name=f'skip_{layer_id}')(merged)\n    residual_out = Conv1D(filters, 1, padding='same', kernel_regularizer=l2(reg_param), name=f'residual_{layer_id}')(inputs)\n    residual_out = Add()([residual_out, skip_out])\n\n    return residual_out, skip_out\n\n\ndef build_wavenet_model(input_shape, filters=FILTERS, kernel_size=KERNEL_SIZE, \n                        reg_param=REG_WEIGHTS, dropout_rate=DROPRATE, convolutions=CONVOLUTIONS, \n                        max_dilation=MAX_DILATION):\n    inputs = Input(shape=input_shape)\n    x = inputs\n    skip_connections = []\n\n    for i in range(convolutions):\n        dilation_rate = min(2 ** i, max_dilation)\n        x, skip = wavenet_block(x, layer_id=i,filters=filters, \n                                kernel_size=kernel_size, dilation_rate=dilation_rate,  \n                                reg_param=reg_param, dropout_rate=dropout_rate)\n        skip_connections.append(skip)\n\n    x = Add()(skip_connections)\n    x = Activation('relu')(x)\n\n    # Downsample the output to coarser frames\n    x = AveragePooling1D(pool_size=2)(x)\n\n    x = Conv1D(filters, 1, activation='relu', kernel_regularizer=l2(reg_param), name='post_conv_1')(x)\n    x = Conv1D(filters, 1, activation='relu', kernel_regularizer=l2(reg_param), name='post_conv_2')(x)\n    x = Flatten()(x)\n    x = Dense(1024, activation='relu', kernel_regularizer=l2(reg_param), name='fc1')(x)\n    x = Dropout(dropout_rate, name='fcdo1')(x)\n    x = Dense(1024, activation='relu', kernel_regularizer=l2(reg_param), name='fc1')(x)\n    x = Dropout(dropout_rate, name='fcdo1')(x)\n    outputs = Dense(1, activation='sigmoid', kernel_regularizer=l2(reg_param), name='output_dense')(x)\n\n    model = Model(inputs, outputs, name=MODEL_NAME)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.metrics import AUC, BinaryAccuracy\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\n\nMODEL_DIR = f\"models/{MODEL_NAME}\"\nIMAGES_DIR = f\"images/{MODEL_NAME}/images\"\nLOG_BASEPATH = f\"logs/{MODEL_NAME}/tb\"\n\ndef build_cnn(input_shape, train_dataset, test_dataset=None, lr=LEARN_RATTE):\n    model = build_wavenet_model(\n        input_shape=input_shape,\n        filters=FILTERS,\n        kernel_size=KERNEL_SIZE,\n        reg_param=REG_WEIGHTS,\n        dropout_rate=DROPRATE,\n        convolutions=CONVOLUTIONS,\n    )\n\n    optimizer = Adam(learning_rate=lr)\n    model.compile(\n        loss=BinaryCrossentropy(),\n        optimizer=optimizer,\n        metrics=[BinaryAccuracy(name='binary_accuracy'), AUC(name='auc')]\n    )\n    callbacks = [\n        EarlyStopping(\n            patience=PATIENCE_EPOCHS,\n            monitor=\"val_loss\",\n            restore_best_weights=True,\n        ),\n        ReduceLROnPlateau(\n            monitor=\"val_loss\",\n            factor=0.5,\n            patience=2,\n            verbose=1,\n            min_lr=1e-5\n        ),\n        TensorBoard(\n            log_dir=LOG_BASEPATH,\n            histogram_freq=1,\n            write_images=True\n        )\n    ]\n\n    history = model.fit(\n        train_dataset,\n        validation_data=test_dataset,\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        callbacks=callbacks,\n        verbose=1\n    )\n\n    return model, history\n\nwith strategy.scope():\n    input_shape = (WINDOW, len(FEATURES))\n    print(f\"input_shape: {input_shape}\")\n\n    model, history = build_cnn(input_shape, train_dataset=train_dataset, test_dataset=val_dataset)\n    model.save(MODEL_PATH)\n    model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score, roc_auc_score\n\ndef print_metrics():\n    ypred_raw = model.predict([X_t])\n    pred = (ypred_raw > 0.5).astype(int)\n    metrics = {}\n\n    metrics = {\n        \"Accuracy\": accuracy_score(y_t.flatten(), pred.flatten()),\n        \"Precision\": precision_score(y_t.flatten(), pred.flatten()),\n        \"Recall\": recall_score(y_t.flatten(), pred.flatten()),\n        \"F1b Score\": fbeta_score(y_t.flatten(), pred.flatten(), average=\"weighted\", beta=0.1),\n        \"ROC AUC\": roc_auc_score(y_t.flatten(), ypred_raw.flatten(), average='weighted')  # Using raw probabilities\n    }\n\n    metrics_unseen_df = pd.DataFrame.from_dict(metrics, orient='index')\n    metrics_unseen_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.math import confusion_matrix\nimport seaborn as sns\n\ndef plot_confusion_matrix(cm, labels, cm2=None, labels2=None):\n        plt.figure(figsize=(8 if cm2 is not None else 4, 4))\n        if cm2 is not None:\n            plt.subplot(1, 2, 1)\n        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Accent)\n\n        df_cm = pd.DataFrame((cm / np.sum(cm, axis=1)[:, None])*100, index=[i for i in labels], columns=[i for i in labels])\n        cm_plot1 = sns.heatmap(df_cm, annot=True,  fmt=\".2f\", cmap='Blues', xticklabels=labels, yticklabels=labels).get_figure()\n        plt.xlabel('Predicted Labels')\n        plt.ylabel('True Labels')\n        plt.title('Confusion Matrix 1')\n        tick_marks = np.arange(len(labels))\n        plt.xticks(tick_marks, labels, rotation=45)\n        plt.yticks(tick_marks, labels)\n        plt.tight_layout()\n\n        return cm_plot1\n\ncm = confusion_matrix(y_t.flatten(), pred)\nfigure = plot_confusion_matrix(cm, labels=[1,0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Grid Search","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\nfrom tensorboard.plugins.hparams import api as hp\nfrom tensorflow.summary import create_file_writer\nimport json\n\nHP_KERNEL_SIZE = hp.HParam(\"kernel_size\", hp.Discrete([KERNEL_SIZE * 2, KERNEL_SIZE]))\nHP_BATCH_SIZE = hp.HParam(\"batch_size\", hp.Discrete([BATCH_SIZE]))\nHP_EPOCHS = hp.HParam(\"epochs\", hp.Discrete([EPOCHS]))\nHP_DILATION_RATE = hp.HParam(\"dilation_rate\", hp.Discrete([DILATION_RATE]))\nHP_DROPOUT_RATE = hp.HParam(\"dropout_rate\", hp.Discrete([DROPRATE, DROPRATE*2]))\nHP_REG_WEIGHTS = hp.HParam(\"reg_weight\", hp.Discrete([REG_WEIGHTS, REG_WEIGHTS*2]))\nHP_LEARNING_RATE = hp.HParam(\"learning_rate\", hp.Discrete([LEARN_RATE]))\nHP_PATIENCE = hp.HParam(\"patience\", hp.Discrete([PATIENCE_EPOCHS]))\nHP_ALPHA = hp.HParam(\"alpha\", hp.Discrete([ERROR_ALPHA, ERROR_ALPHA-0.5, ERROR_ALPHA+0.5]))\nHP_GAMMA = hp.HParam(\"gamma\", hp.Discrete([ERROR_GAMMA, ERROR_GAMMA-0.5, ERROR_GAMMA+0.5]))\nHP_HIDDEN_DENSE = hp.HParam(\"dense_units\", hp.Discrete([\n    f\"{WINDOW}\",\n    f\"{WINDOW*2}_{WINDOW}\",\n    f\"{WINDOW*2}_{WINDOW}_{WINDOW//2}\",\n    f\"{WINDOW*4}_{WINDOW*2}\",\n]))\nHP_FILTERS = hp.HParam(\"filters\", hp.Discrete([FILTERS //2 ,FILTERS, FILTERS * 2]))\nHPARAMS = [\n    HP_FILTERS,\n    HP_KERNEL_SIZE,\n    HP_BATCH_SIZE,\n    HP_EPOCHS,\n    HP_DILATION_RATE,\n    HP_DROPOUT_RATE,\n    HP_REG_WEIGHTS,\n    HP_LEARNING_RATE,\n    HP_PATIENCE,\n    HP_HIDDEN_DENSE,\n    HP_ALPHA,\n        HP_GAMMA\n    ]\n\ndef grid_search_build_cnn(input_shape, X, y, Xt=None, yt=None, hparams=HPARAMS, file_name=f\"best_params.json\", checkpoint_file = f\"checkpoint.json\"):\n    def _decode_arrays(config_str):\n        return [int(unit) for unit in config_str.split('_')]\n\n    def _save_best_params(best_params, best_loss, best_metric, other_metrics = None, file_name=\"best_params.json\"):\n        os.makedirs(MODEL_DIR, exist_ok=True)\n        with open(f\"{MODEL_DIR}/{file_name}\", \"w\") as file:\n            json.dump({\"best_params\": best_params, \"best_loss\": best_loss, \"best_metric\": best_metric, 'other_metrics': other_metrics}, file)\n\n    def _load_checkpoint(file_name):\n        json = None\n        try:\n            os.makedirs(MODEL_DIR, exist_ok=True)\n            with open(f\"{MODEL_DIR}/{file_name}\", \"r\") as file:\n                json = json.load(file)\n        except Exception as e:\n            print(f\"File {MODEL_DIR}/{file_name} not found or error {e}\")\n        return json\n\n    def _save_checkpoint(state, file_name):\n        os.makedirs(MODEL_DIR, exist_ok=True)\n        with open(f\"{MODEL_DIR}/{file_name}\", \"w\") as file:\n            json.dump(state, file)\n\n    with create_file_writer(f\"{LOG_BASEPATH}/hparam_tuning\").as_default():\n        hp.hparams_config(\n            hparams=hparams,\n            metrics=[hp.Metric(TARGET_METRIC, display_name=TARGET_METRIC)],\n        )\n\n    start_index = 0\n    best_loss = np.inf\n    best_metric = -np.inf\n    best_params = None\n    checkpoint = _load_checkpoint(checkpoint_file)\n    if checkpoint:\n        start_index = checkpoint['next_index']\n        best_loss = checkpoint['best_loss']\n        best_metric = checkpoint['best_metric']\n        best_params = checkpoint['best_params']\n\n    grid = list(ParameterGrid({h.name: h.domain.values for h in hparams}))\n    for index, hp_values in enumerate(tqdm(grid[start_index:], desc=\"Grid Search..\"), start=start_index):\n        dense_units = _decode_arrays(hp_values[\"dense_units\"])\n        filters = _decode_arrays(hp_values[\"filters\"])\n        b = hp_values[\"bias\"]\n        k = hp_values[\"kernel_size\"]\n        d = hp_values[\"dilation_rate\"]\n        rw = hp_values[\"reg_weight\"]\n        drop = hp_values[\"dropout_rate\"]\n\n        ERROR_ALPHA = hp_values[\"alpha\"]\n        ERROR_GAMMA = hp_values[\"gamma\"]\n        print(f\"Shapes{input_shape}: x{X[0].shape}xg{X[1].shape}y{y.shape}, filters {filters}, dense {dense_units}, k: {k}, d: {d}, rw: {rw}, drop: {drop}, b: {b}, alpha: {ERROR_ALPHA},  gamma: {ERROR_GAMMA}\")\n\n        model, history = build_cnn(input_shape, X, y,\n                                    Xt=Xt, yt=yt,\n                                    filters=filters,\n                                    kernel_size=k,\n                                    b_cv=True)\n        loss = history.history[f\"val_loss\"][-1]\n        metric = history.history[f\"val_{TARGET_METRIC}\"][-1]\n        if (metric > best_metric):\n            best_history = history\n            best_loss = loss\n            best_metric = metric\n            best_model = model\n            best_params = hp_values\n            other_metrics = {\n                f\"{TARGET_METRIC}\": history.history[f\"{TARGET_METRIC}\"][-1],\n                f\"v_{TARGET_METRIC}\": history.history[f\"val_{TARGET_METRIC}\"][-1],\n                'ba': history.history[\"ba\"][-1],\n                'v_ba': history.history[\"val_ba\"][-1],\n            }\n            _save_best_params(best_params, best_loss, best_metric, other_metrics, file_name)\n        checkpoint_state = {\n            'next_index': index + 1,\n            'best_loss': best_loss,\n            'best_metric': best_metric,\n            'best_params': best_params\n        }\n        _save_checkpoint(checkpoint_state, checkpoint_file)\n    return best_model, best_history, best_params, best_loss, best_metric\n\nPARAM_SEARCH = False\nif PARAM_SEARCH:\n    with strategy.scope():\n        assert not np.any(pd.isna(X)) and not np.any(np.isnan(X_t))\n        print(f\"{X.shape}\")\n        input_shape = (\n            WINDOW,\n            1 if len(X.shape) < 3 else X.shape[2],\n        )\n\n        model, history, best_params, best_loss, best_metric = grid_search_build_cnn(input_shape, X=X, y=y, Xt=X_t, yt=y_t, hparams=HPARAMS)\n        print(best_params)\n        print(best_metric)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CV","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import TimeSeriesSplit\n\nCV_MODEL = True\nCV_SPLITS = 3\n\ndef train_cv_model(X, y, input_shape, n_splits=5, perturb=True, window=WINDOW):\n    def _perturb_gaussiannoise(X, noise_level=0.1):\n        sigma = noise_level * np.std(X)\n        noise = np.random.normal(0, sigma, X.shape)\n        return X + noise\n\n    if perturb:\n        X = _perturb_gaussiannoise(X)\n\n    results = []\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n    global metrics_col\n    metrics_col = None\n\n    for train_index, test_index in tqdm(tscv.split(X), desc=f\"CV Testing for n_splits: {n_splits}\"):\n        X_train = X.iloc[train_index]\n        y_train = y.iloc[train_index]\n        X_test = X.iloc[test_index]\n        y_test = y.iloc[test_index]\n\n        X_train_windows, y_train_windows = prepare_windows(X_train, y_train, window_size=window)\n        X_test_windows, y_test_windows = prepare_windows(X_test, y_test, window_size=window)\n\n        try:\n            cv_model, _ = build_cnn(input_shape, X=X_train_windows, y=y_train_windows, Xt=X_test_windows, yt=y_test_windows)\n\n            result = cv_model.evaluate([X_test_windows], y_test_windows, verbose=0)\n            results.append(result)\n\n            if metrics_col is None:\n                metrics_col = cv_model.metrics\n        except Exception as e:\n            print(f\"CV error on fold with exception: {e}\")\n\n    if metrics_col is None:\n        raise ValueError(\"No successful model training; metrics_col is None\")\n\n    metrics_names = [metric.name for metric in metrics_col]\n    results_df = pd.DataFrame(results, columns=metrics_names)\n\n    return results_df\n\n# results_df = train_cv_model(train_ts_df.drop(columns=[META_LABEL]), train_ts_df[META_LABEL], input_shape)\n# results_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}