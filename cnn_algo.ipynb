{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1036ad0",
   "metadata": {
    "_cell_guid": "143f9c02-8d86-484a-b4c2-eb1317289f2a",
    "_uuid": "068dcfb3-c368-468d-8410-aea88bc0b181",
    "id": "oaDoHbxVH0CW",
    "papermill": {
     "duration": 0.007734,
     "end_time": "2024-06-01T21:47:52.207387",
     "exception": false,
     "start_time": "2024-06-01T21:47:52.199653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bbc8ec",
   "metadata": {
    "_cell_guid": "b5ba98a0-9590-4ccd-b238-cfae63d19770",
    "_uuid": "6a6076dd-8ce5-47e2-8913-74dcaa2eacf0",
    "id": "z_cBqdYOoY5S",
    "papermill": {
     "duration": 0.006911,
     "end_time": "2024-06-01T21:47:52.221485",
     "exception": false,
     "start_time": "2024-06-01T21:47:52.214574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Notebook's Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d45818",
   "metadata": {
    "_cell_guid": "44c8b09f-6f40-410d-aa3c-89b119fb2456",
    "_uuid": "56c0c199-418e-4fa2-a71a-30d54c3a8b2c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-01T21:47:52.236731Z",
     "iopub.status.busy": "2024-06-01T21:47:52.236435Z",
     "iopub.status.idle": "2024-06-01T21:50:26.008610Z",
     "shell.execute_reply": "2024-06-01T21:50:26.007353Z"
    },
    "id": "eETPYJLiMU-b",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "49f77cf0-e6a3-44d8-9dae-05a929fa4804",
    "papermill": {
     "duration": 153.78285,
     "end_time": "2024-06-01T21:50:26.011336",
     "exception": false,
     "start_time": "2024-06-01T21:47:52.228486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INSTALL_DEPS = True\n",
    "if INSTALL_DEPS:\n",
    "    %pip install hurst==0.0.5\n",
    "    %pip install imbalanced_learn==0.12.3\n",
    "    %pip install imblearn==0.0\n",
    "    %pip install protobuf==5.27.0\n",
    "    %pip install pykalman==0.9.7\n",
    "    %pip install tqdm==4.66.4\n",
    "    %pip install shap==0.45.1\n",
    "    %pip install tensorflow==2.15.1\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c609f8",
   "metadata": {
    "papermill": {
     "duration": 0.029221,
     "end_time": "2024-06-01T21:50:26.069649",
     "exception": false,
     "start_time": "2024-06-01T21:50:26.040428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cloud Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4026e619",
   "metadata": {
    "_cell_guid": "cf2e55fb-0872-49df-ae06-aa49505f9474",
    "_uuid": "ccc8fcee-37e2-48b5-8501-6285d13e13cd",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-01T21:50:26.126258Z",
     "iopub.status.busy": "2024-06-01T21:50:26.125870Z",
     "iopub.status.idle": "2024-06-01T21:50:26.146846Z",
     "shell.execute_reply": "2024-06-01T21:50:26.145858Z"
    },
    "id": "Q4-GoceIIfT_",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "7dcb11f2-d20e-4714-e4fe-f9895dc22aac",
    "papermill": {
     "duration": 0.051278,
     "end_time": "2024-06-01T21:50:26.148883",
     "exception": false,
     "start_time": "2024-06-01T21:50:26.097605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "IN_KAGGLE = IN_COLAB = False\n",
    "try:\n",
    "    # https://www.tensorflow.org/install/pip#windows-wsl2\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n",
    "    MODEL_PATH = \"/content/drive/MyDrive/models\"\n",
    "    IN_COLAB = True\n",
    "    print(\"Colab!\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and not IN_COLAB:\n",
    "    print(\"Running in Kaggle...\")\n",
    "    for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "    MODEL_PATH = \"./models\"\n",
    "    DATA_PATH = \"/kaggle/input/intra-day-agriculture-futures-trades-2023-2024\"\n",
    "    IN_KAGGLE = True\n",
    "    print(\"Kaggle!\")\n",
    "elif not IN_COLAB:\n",
    "    IN_KAGGLE = False\n",
    "    MODEL_PATH = \"./models\"\n",
    "    DATA_PATH = \"./data/\"\n",
    "    print(\"running localhost!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45df714",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:50:26.200085Z",
     "iopub.status.busy": "2024-06-01T21:50:26.199265Z",
     "iopub.status.idle": "2024-06-01T21:50:33.550553Z",
     "shell.execute_reply": "2024-06-01T21:50:33.549595Z"
    },
    "papermill": {
     "duration": 7.379249,
     "end_time": "2024-06-01T21:50:33.552669",
     "exception": false,
     "start_time": "2024-06-01T21:50:26.173420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "print(f'Tensorflow version: [{tf.__version__}]')\n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "#tf.config.set_soft_device_placement(True)\n",
    "#tf.config.experimental.enable_op_determinism()\n",
    "#tf.random.set_seed(1)\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.TPUStrategy(tpu)\n",
    "except Exception as e:\n",
    "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "  if len(gpus) > 0:\n",
    "    try:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n",
    "  else:\n",
    "    # CPU is final fallback\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "def is_tpu_strategy(strategy):\n",
    "    return isinstance(strategy, tf.distribute.TPUStrategy)\n",
    "\n",
    "print(\"Number of accelerators:\", strategy.num_replicas_in_sync)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a920618",
   "metadata": {
    "papermill": {
     "duration": 0.024152,
     "end_time": "2024-06-01T21:50:33.601406",
     "exception": false,
     "start_time": "2024-06-01T21:50:33.577254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b6fb2d",
   "metadata": {
    "papermill": {
     "duration": 0.023657,
     "end_time": "2024-06-01T21:50:33.649143",
     "exception": false,
     "start_time": "2024-06-01T21:50:33.625486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3c109e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:50:33.698526Z",
     "iopub.status.busy": "2024-06-01T21:50:33.698028Z",
     "iopub.status.idle": "2024-06-01T21:50:36.189295Z",
     "shell.execute_reply": "2024-06-01T21:50:36.188362Z"
    },
    "papermill": {
     "duration": 2.519825,
     "end_time": "2024-06-01T21:50:36.192863",
     "exception": false,
     "start_time": "2024-06-01T21:50:33.673038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from algo_trading_utility_script import *\n",
    "\n",
    "filename = f\"{DATA_PATH}{os.sep}futures_{INTERVAL}.csv\"\n",
    "print(filename)\n",
    "futs_df = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "print(futs_df.shape)\n",
    "\n",
    "HALF_LIFE, HURST = get_ou(futs_df, f'{TARGET_FUT}_Close')\n",
    "\n",
    "print(\"Half-Life:\", HALF_LIFE)\n",
    "print(\"Hurst:\", HURST)\n",
    "\n",
    "futs_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79e96a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:50:36.289999Z",
     "iopub.status.busy": "2024-06-01T21:50:36.289683Z",
     "iopub.status.idle": "2024-06-01T21:50:36.789092Z",
     "shell.execute_reply": "2024-06-01T21:50:36.788212Z"
    },
    "papermill": {
     "duration": 0.543907,
     "end_time": "2024-06-01T21:50:36.791432",
     "exception": false,
     "start_time": "2024-06-01T21:50:36.247525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.plot(futs_df[f'{TARGET_FUT}_Close'], label=f'{TARGET_FUT} Close', alpha=0.7)\n",
    "plt.title(f'{TARGET_FUT} Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740fd691",
   "metadata": {
    "papermill": {
     "duration": 0.025487,
     "end_time": "2024-06-01T21:50:36.842694",
     "exception": false,
     "start_time": "2024-06-01T21:50:36.817207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e040b51b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T21:50:36.895106Z",
     "iopub.status.busy": "2024-06-01T21:50:36.894829Z",
     "iopub.status.idle": "2024-06-01T22:04:21.859254Z",
     "shell.execute_reply": "2024-06-01T22:04:21.858294Z"
    },
    "papermill": {
     "duration": 824.993229,
     "end_time": "2024-06-01T22:04:21.861471",
     "exception": false,
     "start_time": "2024-06-01T21:50:36.868242",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, normalize, FunctionTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "BIAS = 0.\n",
    "CLASS_WEIGHTS = {0: 1., 1: 1.}\n",
    "SCALERS = None\n",
    "TEST_SPLIT = 0.8\n",
    "TRAIN_SIZE = int(len(futs_df) * TEST_SPLIT)\n",
    "CACHE = True\n",
    "FUTURES_TMP_FILE = \"./tmp/futures.pkl\"\n",
    "os.makedirs(\"./tmp/\", exist_ok=True)\n",
    "\n",
    "# FEATURES_SELECTED from feature selection GBC notebook.\n",
    "COLS_TO_SCALE = ['10Y_Barcount', '10Y_Spread', '10Y_Volume', '2YY_Spread', '2YY_Volume',\n",
    "                'Filtered_X', 'KG_X', 'KG_Z1', 'RTY_Spread', 'SD', 'Spread',\n",
    "                'VXM_Open', 'VXM_Spread', 'Volume'] # StockFeat.list + MARKET_COLS + BB_COLS + SR_COLS + KF_COLS # FEATURES_SELECTED\n",
    "FEATURES = FEATURES_SELECTED # StockFeat.list + MARKET_COLS + KF_COLS + BB_COLS + MOM_COLS + SR_COLS # FEATURES_SELECTED\n",
    "\n",
    "print(f\"Scaling these features: {COLS_TO_SCALE}\")\n",
    "print(f\"Training on these features: {FEATURES}\")\n",
    "\n",
    "def oversample_mean_reversions(train_agri_ts, window, period=INTERVAL, hurst=HURST):\n",
    "    samples = []\n",
    "    for df in tqdm(train_agri_ts, desc=\"oversample_mean_reversions\"):\n",
    "        bb_df = df.copy()\n",
    "        results_df = param_search_bbs(bb_df, StockFeatExt.CLOSE, period, initial_window=window * 2, window_min=window // 2, hurst=hurst)\n",
    "        results_df = results_df[results_df[\"Metric\"] == \"Sharpe\"]\n",
    "        bb_df, _ = bollinger_band_backtest(bb_df, StockFeatExt.CLOSE, results_df[\"Window\"].iloc[0], period, std_factor=results_df[\"Standard_Factor\"].iloc[0])\n",
    "\n",
    "        samples.append(bb_df[train_agri_ts[0].columns].reset_index(drop=True))\n",
    "    return train_agri_ts + samples\n",
    "\n",
    "def normalize_and_label_data(ts, meta_label=META_LABEL, cols_to_scale=COLS_TO_SCALE, scalers=None):\n",
    "    def _get_first_difference(data_df):\n",
    "        return data_df.diff(1).fillna(0)\n",
    "\n",
    "    def _get_log_returns(data_df):\n",
    "        return np.log(data_df / data_df.shift(1)).fillna(0)\n",
    "\n",
    "    y0 = 0\n",
    "    y1 = 0\n",
    "    dfs = []\n",
    "    new_scalers = []\n",
    "    for df, scaler in tqdm(zip(ts, scalers or [None] * len(ts)), desc=\"label_data\"):\n",
    "        df = aug_metalabel_mr(df)\n",
    "        if (df[meta_label] > 0).sum() == 0:\n",
    "            print(\"A DS with no Positive Label was found!\")\n",
    "            continue\n",
    "        y0 += (df[meta_label] == 0).sum()\n",
    "        y1 += (df[meta_label] > 0).sum()\n",
    "        if cols_to_scale is not None:\n",
    "            if scaler is None:\n",
    "                scaler= StandardScaler() # FunctionTransformer(_get_first_difference) #\n",
    "                scaler.fit(df[cols_to_scale])\n",
    "                new_scalers.append(scaler)\n",
    "            df[cols_to_scale] = scaler.transform(df[cols_to_scale])\n",
    "            df = df.iloc[1:] # First data is always nan after a transform\n",
    "        df = df.loc[:, ~df.columns.duplicated(keep=\"first\")]\n",
    "        dfs.append(df.dropna())\n",
    "\n",
    "    # Unless we SMOTE, this dataset is imbalanced.\n",
    "    total = y0 + y1\n",
    "    class_weight_0 = total / y0 if y0 != 0 else 0\n",
    "    class_weight_1 = total / y1 if y1 != 0 else 0\n",
    "    class_weights = {0: class_weight_0, 1: class_weight_1}\n",
    "\n",
    "    # the bias will shift activation to be more sensible to the imbalance.\n",
    "    bias = np.log(y1 / y0)\n",
    "\n",
    "    return dfs, class_weights, bias, new_scalers if len(new_scalers)> 0 else scalers\n",
    "\n",
    "with strategy.scope():\n",
    "    if not os.path.exists(FUTURES_TMP_FILE):\n",
    "        futs_exog_df = process_exog(MARKET_FUTS, futs_df)\n",
    "        train_agri_ts, val_agri_ts = process_futures(FUTS, futs_df, futs_exog_df, TRAIN_SIZE, INTERVAL)\n",
    "        # Same as SMOTE, but reusing the same TS with different MR algos.\n",
    "        train_agri_ts = oversample_mean_reversions(train_agri_ts, HALF_LIFE)\n",
    "        val_agri_ts = oversample_mean_reversions(val_agri_ts, HALF_LIFE)\n",
    "        if CACHE:\n",
    "            with open(FUTURES_TMP_FILE, 'wb') as f:\n",
    "                pickle.dump((train_agri_ts, val_agri_ts), f)\n",
    "    else:\n",
    "        with open(FUTURES_TMP_FILE, 'rb') as f:\n",
    "            train_agri_ts, val_agri_ts = pickle.load(f)\n",
    "    train_agri_ts, CLASS_WEIGHTS, BIAS, SCALERS = normalize_and_label_data(train_agri_ts, cols_to_scale=COLS_TO_SCALE)\n",
    "    val_agri_ts, val_weights, _, _ = normalize_and_label_data(val_agri_ts, cols_to_scale=COLS_TO_SCALE, scalers=SCALERS)\n",
    "\n",
    "print(f\"train weights: {CLASS_WEIGHTS}\")\n",
    "print(f\"test weights: {val_weights}\")\n",
    "np.shape(train_agri_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062b206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:04:22.925775Z",
     "iopub.status.busy": "2024-06-01T22:04:22.925401Z",
     "iopub.status.idle": "2024-06-01T22:04:22.961381Z",
     "shell.execute_reply": "2024-06-01T22:04:22.960246Z"
    },
    "papermill": {
     "duration": 0.579166,
     "end_time": "2024-06-01T22:04:22.963608",
     "exception": false,
     "start_time": "2024-06-01T22:04:22.384442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = val_agri_ts[0]\n",
    "print(sample[META_LABEL].value_counts())\n",
    "\n",
    "sampled_pattenrs = sample[sample[META_LABEL] > 0]\n",
    "sampled_pattenrs[FEATURES + [META_LABEL, \"Ret\"]].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970cc401",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:04:24.041750Z",
     "iopub.status.busy": "2024-06-01T22:04:24.041173Z",
     "iopub.status.idle": "2024-06-01T22:04:24.116381Z",
     "shell.execute_reply": "2024-06-01T22:04:24.115529Z"
    },
    "papermill": {
     "duration": 0.589586,
     "end_time": "2024-06-01T22:04:24.118368",
     "exception": false,
     "start_time": "2024-06-01T22:04:23.528782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "WINDOW = 511  # window is the k+k-1xd-1 or the sum i=0..n of 1+sum(receptive feild)x2^i\n",
    "WINDOW_TMP_PATH = \"./tmp/\"\n",
    "# TPU see: https://github.com/tensorflow/tensorflow/issues/41635\n",
    "BATCH_SIZE = 8  * strategy.num_replicas_in_sync # Default 8\n",
    "print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n",
    "\n",
    "def prepare_windows(data_df, label_df, window_size=WINDOW):\n",
    "    \"\"\"\n",
    "    Prepare windows of features and corresponding labels for classification.\n",
    "    IMPORTANT: There is no padding, incomplete timewindows are discarded!\n",
    "\n",
    "    Parameters:\n",
    "    - data_df: DataFrame containing the features.\n",
    "    - label_df: DataFrame containing the labels.\n",
    "    - window_size: The size of the input window.\n",
    "\n",
    "    Returns:\n",
    "    - X: Array of input windows.\n",
    "    - y: Array of corresponding labels.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data_df) - window_size):\n",
    "        input_window = data_df.iloc[i : i + window_size].values\n",
    "        assert not np.isnan(input_window).any(), \"NaN values found in input window\"\n",
    "        X.append(input_window)\n",
    "        if label_df is not None:\n",
    "            target_label = label_df.iloc[i + window_size]\n",
    "            y.append([target_label])\n",
    "            assert not np.isnan(target_label).any(), \"NaN values found in target label\"\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def prepare_windows_with_disjoint_ts(ts_list, window_size=WINDOW):\n",
    "    \"\"\"\n",
    "    Generator function to yield windows of features and corresponding labels from multiple time series.\n",
    "\n",
    "    Parameters:\n",
    "    - ts_list: List of DataFrames, each containing a time series.\n",
    "    - window_size: The size of the input window.\n",
    "\n",
    "    Yields:\n",
    "    - features: The input window of features.\n",
    "    - labels: The corresponding label.\n",
    "    \"\"\"\n",
    "    for data_df in ts_list:\n",
    "        X, y = prepare_windows(data_df[FEATURES], data_df[META_LABEL], window_size=window_size)\n",
    "        for features, labels in zip(X, y):\n",
    "            yield features, labels\n",
    "\n",
    "def create_windowed_dataset_from_generator(ts_list, window_size=WINDOW, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Create a TensorFlow dataset from a generator.\n",
    "\n",
    "    Parameters:\n",
    "    - ts_list: List of DataFrames, each containing a time series.\n",
    "    - window_size: The size of the input window.\n",
    "    - batch_size: The batch size for the dataset.\n",
    "\n",
    "    Returns:\n",
    "    - dataset: A TensorFlow dataset.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: prepare_windows_with_disjoint_ts(ts_list, window_size=window_size),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(window_size, len(FEATURES)), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(1,), dtype=tf.float32)  # Assuming labels are floats for binary classification\n",
    "        )\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def create_dataset_from_generator(ts_list, batch_size):\n",
    "    def generator(ts_list):\n",
    "        full_df = pd.concat(ts_list)\n",
    "        for i, row in full_df.iterrows():\n",
    "            yield row[FEATURES].values, row[META_LABEL]  # Reshape to match (1,)\n",
    "\n",
    "    output_signature = (\n",
    "        tf.TensorSpec(shape=(len(FEATURES),), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator(ts_list),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "with strategy.scope():\n",
    "    train_dataset = create_windowed_dataset_from_generator(train_agri_ts, batch_size=BATCH_SIZE)\n",
    "    val_dataset = create_windowed_dataset_from_generator(val_agri_ts, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d4b9ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:04:25.144082Z",
     "iopub.status.busy": "2024-06-01T22:04:25.143258Z",
     "iopub.status.idle": "2024-06-01T22:04:25.814586Z",
     "shell.execute_reply": "2024-06-01T22:04:25.813584Z"
    },
    "papermill": {
     "duration": 1.189539,
     "end_time": "2024-06-01T22:04:25.816702",
     "exception": false,
     "start_time": "2024-06-01T22:04:24.627163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# INPUT_SHAPE = (len(FEATURES), ) # The expected shape, where the None shape is BATCH_SIZE\n",
    "\n",
    "sampled_dataset = val_dataset.shuffle(buffer_size=250).take(1)\n",
    "for features, labels in train_dataset.take(1):\n",
    "    INPUT_SHAPE = features.numpy().shape[1:]  # Assuming the shape is (batch_size, len(FEATURES))\n",
    "    print(\"Features:\", features.numpy())\n",
    "    print(\"Labels:\", labels.numpy())\n",
    "\n",
    "print(\"INPUT_SHAPE:\", INPUT_SHAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dcea7e",
   "metadata": {
    "papermill": {
     "duration": 0.511757,
     "end_time": "2024-06-01T22:04:26.898618",
     "exception": false,
     "start_time": "2024-06-01T22:04:26.386861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CNN \n",
    "\n",
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962db0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:04:27.946404Z",
     "iopub.status.busy": "2024-06-01T22:04:27.945702Z",
     "iopub.status.idle": "2024-06-01T22:04:27.973677Z",
     "shell.execute_reply": "2024-06-01T22:04:27.972731Z"
    },
    "papermill": {
     "duration": 0.55979,
     "end_time": "2024-06-01T22:04:27.975565",
     "exception": false,
     "start_time": "2024-06-01T22:04:27.415775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Add, Multiply, Input, Flatten, Dense, GlobalAveragePooling1D, MaxPooling1D, SpatialDropout1D, Activation, Dropout, ReLU, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.initializers import Constant, HeNormal\n",
    "\n",
    "MODEL_NAME = None\n",
    "MAX_DILATION = 8\n",
    "FILTERS = 32\n",
    "DROPRATE = 0.4\n",
    "KERNEL_SIZE = 3\n",
    "REG_WEIGHTS = 1e-5\n",
    "CONVOLUTIONS = 12\n",
    "DENSE_SIZE = 128\n",
    "DENSE_DEPTH = 8\n",
    "\n",
    "def resnet_block(in_x, layer_id, filters, kernel_size, reg_param, stride=1):\n",
    "    # Convolutionals\n",
    "    x = Conv1D(filters, kernel_size,\n",
    "               strides=stride,\n",
    "               padding='same',\n",
    "               kernel_regularizer=l2(reg_param),\n",
    "               name=f'conv1_{layer_id}')(in_x)\n",
    "    x = BatchNormalization(name=f'bn1_{layer_id}')(x)\n",
    "    x = ReLU(name=f'relu1_{layer_id}')(x)\n",
    "    x = Conv1D(filters, kernel_size,\n",
    "               strides=1,\n",
    "               padding='same',\n",
    "               kernel_regularizer=l2(reg_param),\n",
    "               name=f'conv2_{layer_id}')(x)\n",
    "    x = BatchNormalization(name=f'bn2_{layer_id}')(x)\n",
    "\n",
    "    # Identity shortcut\n",
    "    if in_x.shape[-1] != filters or stride != 1:\n",
    "        in_x = Conv1D(filters, 1, strides=stride,\n",
    "                      padding='same',\n",
    "                      kernel_regularizer=l2(reg_param),\n",
    "                      name=f'conv_shortcut_{layer_id}')(in_x)\n",
    "        in_x = BatchNormalization(name=f'bn_shortcut_{layer_id}')(in_x)\n",
    "\n",
    "    # Residual\n",
    "    x = Add(name=f'add_{layer_id}')([x, in_x])\n",
    "    x = ReLU(name=f'relu2_{layer_id}')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def build_resnet_model(input_shape, reg_param=1e-4):\n",
    "    MODEL_NAME = \"RESNET\"\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(64, 7, strides=2, padding='same', kernel_regularizer=l2(reg_param), name='initial_conv')(inputs)\n",
    "    x = BatchNormalization(name='initial_bn')(x)\n",
    "    x = ReLU(name='initial_relu')(x)\n",
    "    x = MaxPooling1D(pool_size=3, strides=2, padding='same', name='initial_maxpool')(x)\n",
    "\n",
    "    # Incremental Residual Blocks - same as the paper's 34-layer architecture\n",
    "    filters = 64\n",
    "    for i in range(3):\n",
    "        x = resnet_block(x, layer_id=f'conv2_{i}', filters=filters, kernel_size=3, reg_param=reg_param)\n",
    "    filters = 128\n",
    "    for i in range(4):\n",
    "        stride = 1 if i == 0 else 2\n",
    "        x = resnet_block(x, layer_id=f'conv3_{i}', filters=filters, stride=stride, kernel_size=3, reg_param=reg_param)\n",
    "    filters = 256\n",
    "    for i in range(6):\n",
    "        stride = 1 if i == 0 else 2\n",
    "        x = resnet_block(x, layer_id=f'conv4_{i}', filters=filters, stride=stride, kernel_size=3, reg_param=reg_param)\n",
    "    filters = 512\n",
    "    for i in range(3):\n",
    "        stride = 1 if i == 0 else 2\n",
    "        x = resnet_block(x, layer_id=f'conv5_{i}', filters=filters, stride=stride, kernel_size=3, reg_param=reg_param)\n",
    "\n",
    "    x = GlobalAveragePooling1D(name='global_avg_pool')(x)\n",
    "    outputs = Dense(1, activation='sigmoid', name='output_dense')(x)\n",
    "\n",
    "    model = Model(inputs, outputs, name=MODEL_NAME)\n",
    "    return model\n",
    "\n",
    "\n",
    "def dense_residual_block(in_x, units, reg_param, dropout_rate, layer_id):\n",
    "    x = Dense(units, kernel_regularizer=l2(reg_param), name=f'dense_{layer_id}_1')(in_x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(units, kernel_regularizer=l2(reg_param), name=f'dense_{layer_id}_2')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if in_x.shape[-1] != units:\n",
    "        # Original RESNet had a Conv1D\n",
    "        in_x = Dense(units, kernel_initializer=HeNormal(), kernel_regularizer=l1_l2(reg_param))(in_x)\n",
    "    x = Add()([in_x, x])\n",
    "    x = LeakyReLU()(x)\n",
    "    return x\n",
    "\n",
    "def build_deep_resnet_model(input_shape,\n",
    "                            reg_param=REG_WEIGHTS,\n",
    "                            dropout_rate=DROPRATE,\n",
    "                            output_bias=BIAS,\n",
    "                            dense_units = DENSE_SIZE,\n",
    "                            dense_layers = DENSE_DEPTH):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for layer_id in range(dense_layers):\n",
    "        x = dense_residual_block(x, dense_units, reg_param, dropout_rate, layer_id)\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid', name='output_dense', bias_initializer=Constant(output_bias))(x)\n",
    "    model = Model(inputs, outputs, name=MODEL_NAME)\n",
    "    return model\n",
    "\n",
    "def build_baseline_model(input_shape,\n",
    "                        reg_param=REG_WEIGHTS,\n",
    "                        dropout_rate=DROPRATE,\n",
    "                        output_bias=BIAS,\n",
    "                        dense_size = DENSE_SIZE):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = Dense(dense_size, kernel_regularizer=l1_l2(reg_param), kernel_initializer=HeNormal())(inputs)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dense(dense_size, kernel_regularizer=l1_l2(reg_param), kernel_initializer=HeNormal())(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid', name='output_dense', bias_initializer=Constant(output_bias))(x)\n",
    "\n",
    "    return Model(inputs, outputs, name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f1d4c",
   "metadata": {
    "papermill": {
     "duration": 0.510579,
     "end_time": "2024-06-01T22:04:29.044051",
     "exception": false,
     "start_time": "2024-06-01T22:04:28.533472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306c079",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-01T22:04:30.062588Z",
     "iopub.status.busy": "2024-06-01T22:04:30.061632Z",
     "iopub.status.idle": "2024-06-02T00:30:25.045595Z",
     "shell.execute_reply": "2024-06-02T00:30:25.044637Z"
    },
    "papermill": {
     "duration": 8763.009363,
     "end_time": "2024-06-02T00:30:32.562642",
     "exception": false,
     "start_time": "2024-06-01T22:04:29.553279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, BinaryFocalCrossentropy\n",
    "from tensorflow.keras.metrics import  AUC, Precision, Recall, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "MODEL_DIR = f\"models/{MODEL_NAME}.keras\"\n",
    "MODEL_HISTORY = f\"{MODEL_PATH}/history.json\"\n",
    "IMAGES_DIR = f\"images/{MODEL_NAME}/images\"\n",
    "LOG_BASEPATH = f\"logs/{MODEL_NAME}/tb\"\n",
    "TARGET_METRIC = \"tp\"\n",
    "\n",
    "EPOCHS = 30\n",
    "PATIENCE_EPOCHS = 5\n",
    "LEARN_RATE =1e-3\n",
    "LEARN_RATE_MIN = 1e-5\n",
    "ALPHA = CLASS_WEIGHTS[1] / (CLASS_WEIGHTS[0] + CLASS_WEIGHTS[1])\n",
    "GAMMA = 2.\n",
    "\n",
    "PURGE = True\n",
    "if PURGE:\n",
    "    # Remove tensorboard logs and other training artefacts for a fresh loop.\n",
    "    shutil.rmtree(LOG_BASEPATH, ignore_errors=True)\n",
    "    shutil.rmtree(MODEL_DIR, ignore_errors=True)\n",
    "    shutil.rmtree(IMAGES_DIR, ignore_errors=True)\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_BASEPATH, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "print(f\"alpha: {ALPHA}, gamma {GAMMA}, bias: {BIAS}\")\n",
    "\n",
    "def build_cnn(input_shape, train_dataset, test_dataset=None,\n",
    "                lr=LEARN_RATE,\n",
    "                lr_min=LEARN_RATE_MIN,\n",
    "                target_metric=TARGET_METRIC,\n",
    "                patience=PATIENCE_EPOCHS,\n",
    "                epochs=EPOCHS,\n",
    "                class_weight=CLASS_WEIGHTS,\n",
    "                initial_bias = BIAS,\n",
    "                conv_layers = CONVOLUTIONS,\n",
    "                max_dilation = MAX_DILATION,\n",
    "                filters = FILTERS,\n",
    "                kernel_size = KERNEL_SIZE,\n",
    "                reg_param = REG_WEIGHTS,\n",
    "                dropout_rate = DROPRATE,\n",
    "                dense_units = DENSE_SIZE,\n",
    "                dense_layers = DENSE_DEPTH // 4):\n",
    "    model = build_resnet_model(\n",
    "        input_shape=input_shape,\n",
    "        reg_param=reg_param\n",
    "    )\n",
    "    optimizer = Adam(learning_rate=lr, clipnorm=1.)\n",
    "    loss = BinaryFocalCrossentropy (from_logits=False,\n",
    "                                    alpha=ALPHA,\n",
    "                                    gamma=GAMMA,\n",
    "                                    reduction='sum_over_batch_size',\n",
    "                                    name='bfce')\n",
    "    model.compile(\n",
    "        loss=loss,\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\n",
    "            TruePositives(name=TARGET_METRIC), # Max TP\n",
    "            TrueNegatives(name='tn'),\n",
    "            FalsePositives(name='fp'),\n",
    "            FalseNegatives(name='fn'),\n",
    "            Precision(name='p'),\n",
    "            Recall(name='r'),\n",
    "            AUC(name='auc'),\n",
    "            AUC(name='prc', curve='PR')\n",
    "        ],\n",
    "    )\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            patience=patience,\n",
    "            monitor=f\"val_{target_metric}\",\n",
    "            restore_best_weights=True,\n",
    "            mode=\"max\" # TARGET_METRIC max or min\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor=f\"val_{target_metric}\",\n",
    "            factor=0.5,\n",
    "            patience=1,\n",
    "            verbose=1,\n",
    "            min_lr=lr_min,\n",
    "            mode=\"max\" # TARGET_METRIC max or min\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=LOG_BASEPATH,\n",
    "            histogram_freq=1,\n",
    "            write_images=True\n",
    "        )\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=test_dataset,\n",
    "        epochs=epochs,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        class_weight=class_weight\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "history_dict = None\n",
    "with strategy.scope():\n",
    "    if not PURGE and os.path.exists(MODEL_PATH):\n",
    "        print(f\"Loading model from: {MODEL_PATH}\")\n",
    "        model = tf.keras.models.load_model(MODEL_PATH)\n",
    "        if os.path.exists(MODEL_HISTORY):\n",
    "            with open(MODEL_HISTORY, 'r') as f:\n",
    "                history_dict = json.load(f)\n",
    "    else:\n",
    "        print(f\"input_shape: {INPUT_SHAPE}\")\n",
    "        model, history = build_cnn(INPUT_SHAPE, train_dataset=train_dataset, test_dataset=val_dataset)\n",
    "        history_dict = history.history\n",
    "        model.save(MODEL_PATH)\n",
    "        # float32 is not directly serializable to JSON\n",
    "        history_dict = {k: [float(i) for i in v] for k, v in history_dict.items()}\n",
    "        with open(MODEL_HISTORY, 'w') as f:\n",
    "            json.dump(history_dict, f)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6df88f",
   "metadata": {
    "papermill": {
     "duration": 7.435075,
     "end_time": "2024-06-02T00:30:47.607073",
     "exception": false,
     "start_time": "2024-06-02T00:30:40.171998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Visualize History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce7dcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T00:31:02.264715Z",
     "iopub.status.busy": "2024-06-02T00:31:02.263996Z",
     "iopub.status.idle": "2024-06-02T00:31:04.396276Z",
     "shell.execute_reply": "2024-06-02T00:31:04.395313Z"
    },
    "papermill": {
     "duration": 9.523718,
     "end_time": "2024-06-02T00:31:04.399000",
     "exception": false,
     "start_time": "2024-06-02T00:30:54.875282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model_stats(history_dict):\n",
    "    plt.figure(figsize=(18, 10))\n",
    "\n",
    "    # Plotting Loss\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(history_dict['loss'], label='Train Loss')\n",
    "    plt.plot(history_dict['val_loss'], label='Val Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Plotting AUC\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(history_dict['auc'], label='Train AUC')\n",
    "    plt.plot(history_dict['val_auc'], label='Val AUC')\n",
    "    plt.title('Model AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # Plotting Precision\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(history_dict['p'], label='Train Precision')\n",
    "    plt.plot(history_dict['val_p'], label='Val Precision')\n",
    "    plt.title('Model Precision')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # Plotting Recall\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(history_dict['r'], label='Train Recall')\n",
    "    plt.plot(history_dict['val_r'], label='Val Recall')\n",
    "    plt.title('Model Recall')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    # Plotting True Positives\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(history_dict['tp'], label='Train True Positives')\n",
    "    plt.plot(history_dict['val_tp'], label='Val True Positives')\n",
    "    plt.title('Model True Positives')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('True Positives')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    # Plotting PRC (Precision-Recall Curve)\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(history_dict['prc'], label='Train PRC')\n",
    "    plt.plot(history_dict['val_prc'], label='Val PRC')\n",
    "    plt.title('Model PRC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('PRC')\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{IMAGES_DIR}/{MODEL_NAME}_stats.png')\n",
    "    plt.show()\n",
    "\n",
    "if history_dict is not None:\n",
    "    plot_model_stats(history_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ab5925",
   "metadata": {
    "papermill": {
     "duration": 7.467683,
     "end_time": "2024-06-02T00:31:19.338034",
     "exception": false,
     "start_time": "2024-06-02T00:31:11.870351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Explain and Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ca10c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T00:31:34.082220Z",
     "iopub.status.busy": "2024-06-02T00:31:34.081492Z",
     "iopub.status.idle": "2024-06-02T00:31:38.126155Z",
     "shell.execute_reply": "2024-06-02T00:31:38.125267Z"
    },
    "papermill": {
     "duration": 11.290591,
     "end_time": "2024-06-02T00:31:38.128051",
     "exception": false,
     "start_time": "2024-06-02T00:31:26.837460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score, roc_auc_score\n",
    "\n",
    "def print_metrics_and_distribution(model, data, labels):\n",
    "    ypred_proba = model.predict(data)\n",
    "    pred = (ypred_proba > 0.5).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(labels, pred.flatten()),\n",
    "        \"Precision\": precision_score(labels, pred.flatten()),\n",
    "        \"Recall\": recall_score(labels, pred.flatten()),\n",
    "        \"F1b Score\": fbeta_score(labels, pred.flatten(), average=\"weighted\", beta=0.1),\n",
    "        \"ROC AUC\": roc_auc_score(labels, ypred_proba.flatten(), average='weighted')\n",
    "    }\n",
    "\n",
    "    metrics_df = pd.DataFrame.from_dict(metrics, orient='index')\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(ypred_proba, color='blue', fill=True, alpha=0.7)\n",
    "\n",
    "    mu, std = norm.fit(ypred_proba)\n",
    "    xmin, xmax = plt.xlim()\n",
    "    x = np.linspace(xmin, xmax, 100)\n",
    "    p = norm.pdf(x, mu, std)\n",
    "    plt.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "    plt.title('PDF')\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{IMAGES_DIR}/{MODEL_NAME}_pdf.png')\n",
    "    plt.show()\n",
    "\n",
    "    metrics_df.to_json(f\"{MODEL_PATH}/stats.json\")\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "if MODEL_NAME == \"WAVENET\":\n",
    "    test_data, test_labels = prepare_windows(val_agri_ts[0][FEATURES], val_agri_ts[0][META_LABEL], window_size=WINDOW)\n",
    "else:\n",
    "    test_data, test_labels = val_agri_ts[0][FEATURES], val_agri_ts[0][META_LABEL]\n",
    "metrics_df = print_metrics_and_distribution(model, test_data, test_labels)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee7e8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T00:31:52.987402Z",
     "iopub.status.busy": "2024-06-02T00:31:52.986692Z",
     "iopub.status.idle": "2024-06-02T00:31:54.272020Z",
     "shell.execute_reply": "2024-06-02T00:31:54.271143Z"
    },
    "papermill": {
     "duration": 8.697888,
     "end_time": "2024-06-02T00:31:54.274132",
     "exception": false,
     "start_time": "2024-06-02T00:31:45.576244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.math import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(model, data, labels, label_names=['RW', 'MR']):\n",
    "    ypred_proba = model.predict(data)\n",
    "    pred = (ypred_proba > 0.5).astype(int)\n",
    "\n",
    "    print(labels.shape)\n",
    "    print(pred.shape)\n",
    "    if len(labels.shape) > 0:\n",
    "        labels = labels.flatten()\n",
    "    if len(pred.shape) > 0:\n",
    "        pred = pred.flatten()\n",
    "    cm = confusion_matrix(labels, pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    df_cm = pd.DataFrame((cm / np.sum(cm, axis=1)[:, None])*100, index=[i for i in label_names], columns=[i for i in label_names])\n",
    "    cm_plot = sns.heatmap(df_cm, annot=True, fmt=\".2f\", cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{IMAGES_DIR}/{MODEL_NAME}_cm.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(model, test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd0554",
   "metadata": {
    "papermill": {
     "duration": 7.367246,
     "end_time": "2024-06-02T00:32:08.911228",
     "exception": false,
     "start_time": "2024-06-02T00:32:01.543982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323cb9fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T00:32:23.796675Z",
     "iopub.status.busy": "2024-06-02T00:32:23.796300Z",
     "iopub.status.idle": "2024-06-02T00:32:24.752932Z",
     "shell.execute_reply": "2024-06-02T00:32:24.752097Z"
    },
    "papermill": {
     "duration": 8.366573,
     "end_time": "2024-06-02T00:32:24.759404",
     "exception": false,
     "start_time": "2024-06-02T00:32:16.392831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "RANDOMIZE_SIZE = 32\n",
    "SAMPLE_SIZE = 25\n",
    "\n",
    "if MODEL_NAME != \"WAVENET\":\n",
    "    # Shap doesn't work with window encoded data.\n",
    "    background_features, background_labels = train_agri_ts[0][FEATURES].values,train_agri_ts[0][META_LABEL].values\n",
    "    test_data, test_labels = val_agri_ts[0][FEATURES].values, val_agri_ts[0][META_LABEL].values\n",
    "    test_features, test_labels = test_data[:SAMPLE_SIZE], test_labels[:SAMPLE_SIZE]\n",
    "\n",
    "    shap.explainers._deep.deep_tf.op_handlers[\"LeakyRelu\"] = shap.explainers._deep.deep_tf.op_handlers[\"Relu\"]\n",
    "    shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.op_handlers[\"Add\"]\n",
    "    shap.explainers._deep.deep_tf.op_handlers[\"BatchToSpaceND\"] = shap.explainers._deep.deep_tf.op_handlers[\"Mean\"]\n",
    "    shap.explainers._deep.deep_tf.op_handlers[\"SpaceToBatchND\"] = shap.explainers._deep.deep_tf.op_handlers[\"Mean\"]\n",
    "    # this is a hack: https://github.com/shap/shap/issues/1463\n",
    "\n",
    "    e = shap.DeepExplainer(model, background_features)\n",
    "\n",
    "    shap_values = e.shap_values(test_features)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "    shap_values = np.squeeze(shap_values)\n",
    "\n",
    "    print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "    print(f\"Test features shape: {test_features.shape}\")\n",
    "    assert shap_values.shape == test_features.shape\n",
    "    shap.summary_plot(shap_values, test_features, feature_names=FEATURES)\n",
    "    plt.savefig(f'{IMAGES_DIR}/{MODEL_NAME}_shap_sum.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984547b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T00:32:39.601019Z",
     "iopub.status.busy": "2024-06-02T00:32:39.600326Z",
     "iopub.status.idle": "2024-06-02T00:32:39.608162Z",
     "shell.execute_reply": "2024-06-02T00:32:39.607215Z"
    },
    "papermill": {
     "duration": 7.42798,
     "end_time": "2024-06-02T00:32:39.610074",
     "exception": false,
     "start_time": "2024-06-02T00:32:32.182094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MODEL_NAME != \"WAVENET\":\n",
    "    sample_index = 2\n",
    "    e = shap.KernelExplainer(model, background)\n",
    "\n",
    "    select = range(SAMPLE_SIZE)\n",
    "    shap_features = test_features[select]\n",
    "    train_features = background_features[select]\n",
    "    shap_values = e.shap_values(shap_features, nsamples=SAMPLE_SIZE)\n",
    "    print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0: SAMPLE_SIZE]\n",
    "    shap_values = np.squeeze(shap_values)\n",
    "    y_pred = (shap_values.sum(1) + e.expected_value) > 0\n",
    "    misclassified = y_pred != test_labels[select]\n",
    "    print(f\"({shap_values.sum(1)} + {e.expected_value}) > 0\")\n",
    "    print(f\"Misclassified: {np.shape(misclassified)} out of {np.shape(y_pred)[0]}\")\n",
    "\n",
    "    print(f\"Explainer expected value: {e.expected_value}\")\n",
    "    shap.decision_plot(e.expected_value, shap_values, train_features, feature_names=FEATURES, link='logit', highlight=misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfdb577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T00:32:54.374960Z",
     "iopub.status.busy": "2024-06-02T00:32:54.373988Z",
     "iopub.status.idle": "2024-06-02T00:32:54.380207Z",
     "shell.execute_reply": "2024-06-02T00:32:54.379435Z"
    },
    "papermill": {
     "duration": 7.401307,
     "end_time": "2024-06-02T00:32:54.382132",
     "exception": false,
     "start_time": "2024-06-02T00:32:46.980825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MODEL_NAME != \"WAVENET\":\n",
    "    shap.decision_plot(\n",
    "        e.expected_value,\n",
    "        shap_values[misclassified],\n",
    "        train_features[misclassified],\n",
    "        link=\"logit\",\n",
    "        highlight=0,\n",
    "        feature_names=FEATURES\n",
    "    )\n",
    "    plt.savefig(f'{IMAGES_DIR}/{MODEL_NAME}_shap_force.png')\n",
    "\n",
    "    shap.force_plot(\n",
    "        e.expected_value,\n",
    "        shap_values[misclassified],\n",
    "        train_features[misclassified],\n",
    "        link=\"logit\",\n",
    "        feature_names=FEATURES\n",
    "    )\n",
    "    plt.savefig(f'{IMAGES_DIR}/{MODEL_NAME}_shap_force_misclassed.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153c475c",
   "metadata": {
    "papermill": {
     "duration": 7.256572,
     "end_time": "2024-06-02T00:33:09.018840",
     "exception": false,
     "start_time": "2024-06-02T00:33:01.762268",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Grid Search and CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7380a1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T00:33:23.904114Z",
     "iopub.status.busy": "2024-06-02T00:33:23.903748Z",
     "iopub.status.idle": "2024-06-02T00:33:23.948357Z",
     "shell.execute_reply": "2024-06-02T00:33:23.947645Z"
    },
    "papermill": {
     "duration": 7.457575,
     "end_time": "2024-06-02T00:33:23.950392",
     "exception": false,
     "start_time": "2024-06-02T00:33:16.492817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.summary import create_file_writer\n",
    "\n",
    "import json\n",
    "\n",
    "HP_KERNEL_SIZE = hp.HParam(\"kernel_size\", hp.Discrete([int(KERNEL_SIZE * 2), int(KERNEL_SIZE), int(KERNEL_SIZE // 2)]))\n",
    "HP_BATCH_SIZE = hp.HParam(\"batch_size\", hp.Discrete([int(BATCH_SIZE)]))\n",
    "HP_EPOCHS = hp.HParam(\"epochs\", hp.Discrete([int(EPOCHS)]))\n",
    "HP_DILATION_RATE = hp.HParam(\"dilation_rate\", hp.Discrete([int(MAX_DILATION), int(MAX_DILATION * 2)]))\n",
    "HP_DROPOUT_RATE = hp.HParam(\"dropout_rate\", hp.Discrete([float(DROPRATE), float(DROPRATE * 2)]))\n",
    "HP_REG_WEIGHTS = hp.HParam(\"reg_weight\", hp.Discrete([float(REG_WEIGHTS), float(REG_WEIGHTS / 2)]))\n",
    "HP_LEARNING_RATE = hp.HParam(\"learning_rate\", hp.Discrete([float(LEARN_RATE)]))\n",
    "HP_PATIENCE = hp.HParam(\"patience\", hp.Discrete([int(PATIENCE_EPOCHS)]))\n",
    "HP_DENSE_DEPTH = hp.HParam(\"dense_depth\", hp.Discrete([int(DENSE_DEPTH), int(DENSE_DEPTH * 2)]))\n",
    "HP_DENSE_UNITS = hp.HParam(\"dense_units\", hp.Discrete([int(DENSE_DEPTH // 2), int(DENSE_DEPTH), int(DENSE_DEPTH * 2)]))\n",
    "HP_FILTERS = hp.HParam(\"filters\", hp.Discrete([int(FILTERS // 2), int(FILTERS), int(FILTERS * 2)]))\n",
    "HP_CONVOLUTIONS= hp.HParam(\"convolutions\", hp.Discrete([int(CONVOLUTIONS // 2), int(CONVOLUTIONS), int(CONVOLUTIONS * 2)]))\n",
    "HPARAMS = [\n",
    "    HP_FILTERS,\n",
    "    HP_KERNEL_SIZE,\n",
    "    HP_BATCH_SIZE,\n",
    "    HP_EPOCHS,\n",
    "    HP_DILATION_RATE,\n",
    "    HP_DROPOUT_RATE,\n",
    "    HP_REG_WEIGHTS,\n",
    "    HP_LEARNING_RATE,\n",
    "    HP_PATIENCE,\n",
    "    HP_DENSE_UNITS,\n",
    "    HP_DENSE_DEPTH,\n",
    "    HP_CONVOLUTIONS]\n",
    "\n",
    "def grid_search_build_cnn(input_shape, train_dataset, test_dataset, hparams=HPARAMS, file_name=f\"best_params.json\", checkpoint_file = f\"checkpoint.json\"):\n",
    "    def _decode_arrays(config_str):\n",
    "        return [int(unit) for unit in config_str.split('_')]\n",
    "\n",
    "    def _save_best_params(best_params, best_loss, best_metric, other_metrics = None, file_name=\"best_params.json\"):\n",
    "        os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "        with open(f\"{MODEL_DIR}/{file_name}\", \"w\") as file:\n",
    "            json.dump({\"best_params\": best_params, \"best_loss\": best_loss, \"best_metric\": best_metric, 'other_metrics': other_metrics}, file)\n",
    "\n",
    "    def _load_checkpoint(file_name):\n",
    "        json = None\n",
    "        try:\n",
    "            os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "            with open(f\"{MODEL_DIR}/{file_name}\", \"r\") as file:\n",
    "                json = json.load(file)\n",
    "        except Exception as e:\n",
    "            print(f\"File {MODEL_DIR}/{file_name} not found or error {e}\")\n",
    "        return json\n",
    "\n",
    "    def _save_checkpoint(state, file_name):\n",
    "        os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "        with open(f\"{MODEL_DIR}/{file_name}\", \"w\") as file:\n",
    "            json.dump(state, file)\n",
    "\n",
    "    with create_file_writer(f\"{LOG_BASEPATH}/hparam_tuning\").as_default():\n",
    "        hp.hparams_config(\n",
    "            hparams=hparams,\n",
    "            metrics=[hp.Metric(TARGET_METRIC, display_name=TARGET_METRIC)],\n",
    "        )\n",
    "\n",
    "    start_index = 0\n",
    "    best_loss = np.inf\n",
    "    best_metric = -np.inf\n",
    "    best_params = None\n",
    "    checkpoint = _load_checkpoint(checkpoint_file)\n",
    "    if checkpoint:\n",
    "        start_index = checkpoint['next_index']\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        best_metric = checkpoint['best_metric']\n",
    "        best_params = checkpoint['best_params']\n",
    "\n",
    "    grid = list(ParameterGrid({h.name: h.domain.values for h in hparams}))\n",
    "    for index, hp_values in enumerate(tqdm(grid[start_index:], desc=\"Grid Search..\"), start=start_index):\n",
    "        lr = hp_values[\"learning_rate\"]\n",
    "        conv_layers=hp_values[\"convolutions\"]\n",
    "        max_dilation=hp_values[\"dilation_rate\"]\n",
    "        filters=hp_values[\"filters\"]\n",
    "        kernel_size=hp_values[\"kernel_size\"]\n",
    "        reg_param=hp_values[\"reg_weight\"]\n",
    "        dropout_rate=hp_values[\"dropout_rate\"]\n",
    "        dense_units=hp_values[\"dense_units\"]\n",
    "        dense_layers=hp_values[\"dense_depth\"]\n",
    "\n",
    "        model, history = build_cnn(input_shape,\n",
    "                                   train_dataset, test_dataset=test_dataset,\n",
    "                                    lr=lr,\n",
    "                                    lr_min=LEARN_RATE_MIN,\n",
    "                                    target_metric=TARGET_METRIC,\n",
    "                                    conv_layers=conv_layers,\n",
    "                                    max_dilation=max_dilation,\n",
    "                                    filters=filters,\n",
    "                                    kernel_size=kernel_size,\n",
    "                                    reg_param=reg_param,\n",
    "                                    dropout_rate=dropout_rate,\n",
    "                                    dense_units=dense_units,\n",
    "                                    dense_layers=dense_layers)\n",
    "\n",
    "        history_dict = history.history\n",
    "        loss = history_dict[f\"val_loss\"][-1]\n",
    "        metric = history_dict[f\"val_{TARGET_METRIC}\"][-1]\n",
    "        if (metric > best_metric):\n",
    "            best_history = history\n",
    "            best_loss = loss\n",
    "            best_metric = metric\n",
    "            best_model = model\n",
    "            best_params = hp_values\n",
    "            other_metrics = {\n",
    "                f\"{TARGET_METRIC}\": history_dict[f\"{TARGET_METRIC}\"][-1],\n",
    "                f\"v_{TARGET_METRIC}\": history_dict[f\"val_{TARGET_METRIC}\"][-1],\n",
    "                'ba': history_dict[\"ba\"][-1],\n",
    "                'v_ba': history_dict[\"val_ba\"][-1],\n",
    "            }\n",
    "            _save_best_params(best_params, best_loss, best_metric, other_metrics, file_name)\n",
    "        checkpoint_state = {\n",
    "            'next_index': index + 1,\n",
    "            'best_loss': best_loss,\n",
    "            'best_metric': best_metric,\n",
    "            'best_params': best_params\n",
    "        }\n",
    "        _save_checkpoint(checkpoint_state, checkpoint_file)\n",
    "    return best_model, best_history, best_params, best_loss, best_metric\n",
    "\n",
    "PARAM_SEARCH = False\n",
    "if PARAM_SEARCH:\n",
    "    with strategy.scope():\n",
    "        model, history, best_params, best_loss, best_metric = grid_search_build_cnn(INPUT_SHAPE, train_dataset, val_dataset)\n",
    "        print(best_params)\n",
    "        print(best_metric)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5117138,
     "sourceId": 8561110,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 180912526,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9944.673694,
   "end_time": "2024-06-02T00:33:34.203396",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-01T21:47:49.529702",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
