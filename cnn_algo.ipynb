{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","id":"oaDoHbxVH0CW"},"source":["# CNN"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","id":"z_cBqdYOoY5S"},"source":["## Notebook's Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","collapsed":false,"id":"eETPYJLiMU-b","jupyter":{"outputs_hidden":false},"outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"outputs":[],"source":["INSTALL_DEPS = False\n","if INSTALL_DEPS:\n","    %pip install hurst==0.0.5\n","    %pip install imbalanced_learn==0.12.3\n","    %pip install imblearn==0.0\n","    %pip install protobuf==5.27.0\n","    %pip install pykalman==0.9.7\n","    %pip install tqdm==4.66.4\n","    %pip install shap==0.45.1\n","    %pip install tensorflow==2.15.1\n","!python --version"]},{"cell_type":"markdown","metadata":{},"source":["## Cloud Environment Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","collapsed":false,"id":"Q4-GoceIIfT_","jupyter":{"outputs_hidden":false},"outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","IN_KAGGLE = IN_COLAB = False\n","try:\n","    # https://www.tensorflow.org/install/pip#windows-wsl2\n","    import google.colab\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/drive\")\n","    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n","    MODEL_PATH = \"/content/drive/MyDrive/models\"\n","    IN_COLAB = True\n","    print(\"Colab!\")\n","except:\n","    IN_COLAB = False\n","if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and not IN_COLAB:\n","    print(\"Running in Kaggle...\")\n","    for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n","        for filename in filenames:\n","            print(os.path.join(dirname, filename))\n","    MODEL_PATH = \"./models\"\n","    DATA_PATH = \"/kaggle/input/intra-day-agriculture-futures-trades-2023-2024\"\n","    IN_KAGGLE = True\n","    print(\"Kaggle!\")\n","elif not IN_COLAB:\n","    IN_KAGGLE = False\n","    MODEL_PATH = \"./models\"\n","    DATA_PATH = \"./data/\"\n","    print(\"running localhost!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import mixed_precision\n","\n","print(f'Tensorflow version: [{tf.__version__}]')\n","\n","tf.get_logger().setLevel('INFO')\n","\n","#tf.config.set_soft_device_placement(True)\n","#tf.config.experimental.enable_op_determinism()\n","#tf.random.set_seed(1)\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","\n","  tf.config.experimental_connect_to_cluster(tpu)\n","  tf.tpu.experimental.initialize_tpu_system(tpu)\n","  strategy = tf.distribute.TPUStrategy(tpu)\n","except Exception as e:\n","  gpus = tf.config.experimental.list_physical_devices('GPU')\n","  if len(gpus) > 0:\n","    try:\n","        strategy = tf.distribute.MirroredStrategy()\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","    except RuntimeError as e:\n","        print(e)\n","    finally:\n","        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n","  else:\n","    # CPU is final fallback\n","    strategy = tf.distribute.get_strategy()\n","    print(\"Running on CPU\")\n","\n","def is_tpu_strategy(strategy):\n","    return isinstance(strategy, tf.distribute.TPUStrategy)\n","\n","print(\"Number of accelerators:\", strategy.num_replicas_in_sync)\n","os.getcwd()"]},{"cell_type":"markdown","metadata":{},"source":["# Instruments"]},{"cell_type":"markdown","metadata":{},"source":["## Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from algo_trading_utility_script import *\n","\n","filename = f\"{DATA_PATH}{os.sep}futures_{INTERVAL}.csv\"\n","print(filename)\n","futs_df = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n","\n","print(futs_df.shape)\n","\n","HALF_LIFE, HURST = get_ou(futs_df, f'{TARGET_FUT}_Close')\n","\n","print(\"Half-Life:\", HALF_LIFE)\n","print(\"Hurst:\", HURST)\n","\n","futs_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 4))\n","\n","plt.plot(futs_df[f'{TARGET_FUT}_Close'], label=f'{TARGET_FUT} Close', alpha=0.7)\n","plt.title(f'{TARGET_FUT} Price')\n","plt.xlabel('Date')\n","plt.ylabel('Price')\n","plt.legend()\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pickle\n","from sklearn.preprocessing import StandardScaler, normalize, FunctionTransformer\n","from tqdm import tqdm\n","\n","BIAS = 0.\n","CLASS_WEIGHTS = {0: 1., 1: 1.}\n","SCALERS = None\n","TEST_SPLIT = 0.8\n","TRAIN_SIZE = int(len(futs_df) * TEST_SPLIT)\n","CACHE = True\n","FUTURES_TMP_FILE = \"./tmp/futures.pkl\"\n","os.makedirs(\"./tmp/\", exist_ok=True)\n","\n","# FEATURES_SELECTED from feature selection GBC notebook.\n","COLS_TO_SCALE = ['10Y_Barcount', '10Y_Spread', '10Y_Volume', '2YY_Spread', '2YY_Volume',\n","                'Filtered_X', 'KG_X', 'KG_Z1', 'RTY_Spread', 'SD', 'Spread',\n","                'VXM_Open', 'VXM_Spread', 'Volume'] # StockFeat.list + MARKET_COLS + BB_COLS + SR_COLS + KF_COLS # FEATURES_SELECTED\n","FEATURES = FEATURES_SELECTED # StockFeat.list + MARKET_COLS + KF_COLS + BB_COLS + MOM_COLS + SR_COLS # FEATURES_SELECTED\n","\n","print(f\"Scaling these features: {COLS_TO_SCALE}\")\n","print(f\"Training on these features: {FEATURES}\")\n","\n","def oversample_mean_reversions(train_agri_ts, window, period=INTERVAL, hurst=HURST):\n","    samples = []\n","    for df in tqdm(train_agri_ts, desc=\"oversample_mean_reversions\"):\n","        bb_df = df.copy()\n","        results_df = param_search_bbs(bb_df, StockFeatExt.CLOSE, period, initial_window=window * 2, window_min=window // 2, hurst=hurst)\n","        results_df = results_df[results_df[\"Metric\"] == \"Sharpe\"]\n","        bb_df, _ = bollinger_band_backtest(bb_df, StockFeatExt.CLOSE, results_df[\"Window\"].iloc[0], period, std_factor=results_df[\"Standard_Factor\"].iloc[0])\n","\n","        samples.append(bb_df[train_agri_ts[0].columns].reset_index(drop=True))\n","    return train_agri_ts + samples\n","\n","def normalize_and_label_data(ts, meta_label=META_LABEL, cols_to_scale=COLS_TO_SCALE, scalers=None):\n","    def _get_first_difference(data_df):\n","        return data_df.diff(1).fillna(0)\n","\n","    def _get_log_returns(data_df):\n","        return np.log(data_df / data_df.shift(1)).fillna(0)\n","\n","    y0 = 0\n","    y1 = 0\n","    dfs = []\n","    new_scalers = []\n","    for df, scaler in tqdm(zip(ts, scalers or [None] * len(ts)), desc=\"label_data\"):\n","        df = aug_metalabel_mr(df)\n","        if (df[meta_label] > 0).sum() == 0:\n","            print(\"A DS with no Positive Label was found!\")\n","            continue\n","        y0 += (df[meta_label] == 0).sum()\n","        y1 += (df[meta_label] > 0).sum()\n","        if cols_to_scale is not None:\n","            if scaler is None:\n","                scaler= StandardScaler() # FunctionTransformer(_get_first_difference) #\n","                scaler.fit(df[cols_to_scale])\n","                new_scalers.append(scaler)\n","            df[cols_to_scale] = scaler.transform(df[cols_to_scale])\n","            df = df.iloc[1:] # First data is always nan after a transform\n","        df = df.loc[:, ~df.columns.duplicated(keep=\"first\")]\n","        dfs.append(df.dropna())\n","\n","    # Unless we SMOTE, this dataset is imbalanced.\n","    total = y0 + y1\n","    class_weight_0 = total / y0 if y0 != 0 else 0\n","    class_weight_1 = total / y1 if y1 != 0 else 0\n","    class_weights = {0: class_weight_0, 1: class_weight_1}\n","\n","    # the bias will shift activation to be more sensible to the imbalance.\n","    bias = np.log(y1 / y0)\n","\n","    return dfs, class_weights, bias, new_scalers if len(new_scalers)> 0 else scalers\n","\n","with strategy.scope():\n","    if not os.path.exists(FUTURES_TMP_FILE):\n","        futs_exog_df = process_exog(MARKET_FUTS, futs_df)\n","        train_agri_ts, val_agri_ts = process_futures(FUTS, futs_df, futs_exog_df, TRAIN_SIZE, INTERVAL)\n","        # Same as SMOTE, but reusing the same TS with different MR algos.\n","        train_agri_ts = oversample_mean_reversions(train_agri_ts, HALF_LIFE)\n","        val_agri_ts = oversample_mean_reversions(val_agri_ts, HALF_LIFE)\n","        if CACHE:\n","            with open(FUTURES_TMP_FILE, 'wb') as f:\n","                pickle.dump((train_agri_ts, val_agri_ts), f)\n","    else:\n","        with open(FUTURES_TMP_FILE, 'rb') as f:\n","            train_agri_ts, val_agri_ts = pickle.load(f)\n","    train_agri_ts, CLASS_WEIGHTS, BIAS, SCALERS = normalize_and_label_data(train_agri_ts, cols_to_scale=COLS_TO_SCALE)\n","    val_agri_ts, val_weights, _, _ = normalize_and_label_data(val_agri_ts, cols_to_scale=COLS_TO_SCALE, scalers=SCALERS)\n","\n","print(f\"train weights: {CLASS_WEIGHTS}\")\n","print(f\"test weights: {val_weights}\")\n","np.shape(train_agri_ts)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample = val_agri_ts[0]\n","print(sample[META_LABEL].value_counts())\n","\n","sampled_pattenrs = sample[sample[META_LABEL] > 0]\n","sampled_pattenrs[FEATURES + [META_LABEL, \"Ret\"]].tail(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["WINDOW = 511  # window is the k+k-1xd-1 or the sum i=0..n of 1+sum(receptive feild)x2^i\n","WINDOW_TMP_PATH = \"./tmp/\"\n","# TPU see: https://github.com/tensorflow/tensorflow/issues/41635\n","BATCH_SIZE = 8  * strategy.num_replicas_in_sync # Default 8\n","print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n","\n","def prepare_windows(data_df, label_df, window_size=WINDOW):\n","    \"\"\"\n","    Prepare windows of features and corresponding labels for classification.\n","    IMPORTANT: There is no padding, incomplete timewindows are discarded!\n","\n","    Parameters:\n","    - data_df: DataFrame containing the features.\n","    - label_df: DataFrame containing the labels.\n","    - window_size: The size of the input window.\n","\n","    Returns:\n","    - X: Array of input windows.\n","    - y: Array of corresponding labels.\n","    \"\"\"\n","    X, y = [], []\n","    for i in range(len(data_df) - window_size):\n","        input_window = data_df.iloc[i : i + window_size].values\n","        assert not np.isnan(input_window).any(), \"NaN values found in input window\"\n","        X.append(input_window)\n","        if label_df is not None:\n","            target_label = label_df.iloc[i + window_size]\n","            y.append([target_label])\n","            assert not np.isnan(target_label).any(), \"NaN values found in target label\"\n","    return np.array(X), np.array(y)\n","\n","def prepare_windows_with_disjoint_ts(ts_list, window_size=WINDOW):\n","    \"\"\"\n","    Generator function to yield windows of features and corresponding labels from multiple time series.\n","\n","    Parameters:\n","    - ts_list: List of DataFrames, each containing a time series.\n","    - window_size: The size of the input window.\n","\n","    Yields:\n","    - features: The input window of features.\n","    - labels: The corresponding label.\n","    \"\"\"\n","    for data_df in ts_list:\n","        X, y = prepare_windows(data_df[FEATURES], data_df[META_LABEL], window_size=window_size)\n","        for features, labels in zip(X, y):\n","            yield features, labels\n","\n","def create_windowed_dataset_from_generator(ts_list, window_size=WINDOW, batch_size=BATCH_SIZE):\n","    \"\"\"\n","    Create a TensorFlow dataset from a generator.\n","\n","    Parameters:\n","    - ts_list: List of DataFrames, each containing a time series.\n","    - window_size: The size of the input window.\n","    - batch_size: The batch size for the dataset.\n","\n","    Returns:\n","    - dataset: A TensorFlow dataset.\n","    \"\"\"\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: prepare_windows_with_disjoint_ts(ts_list, window_size=window_size),\n","        output_signature=(\n","            tf.TensorSpec(shape=(window_size, len(FEATURES)), dtype=tf.float32),\n","            tf.TensorSpec(shape=(1,), dtype=tf.float32)  # Assuming labels are floats for binary classification\n","        )\n","    )\n","    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","    return dataset\n","\n","def create_dataset_from_generator(ts_list, batch_size):\n","    def generator(ts_list):\n","        full_df = pd.concat(ts_list)\n","        for i, row in full_df.iterrows():\n","            yield row[FEATURES].values, row[META_LABEL]  # Reshape to match (1,)\n","\n","    output_signature = (\n","        tf.TensorSpec(shape=(len(FEATURES),), dtype=tf.float32),\n","        tf.TensorSpec(shape=(), dtype=tf.float32)\n","    )\n","\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: generator(ts_list),\n","        output_signature=output_signature\n","    )\n","    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","    return dataset\n","\n","with strategy.scope():\n","    train_dataset = create_windowed_dataset_from_generator(train_agri_ts, batch_size=BATCH_SIZE)\n","    val_dataset = create_windowed_dataset_from_generator(val_agri_ts, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# INPUT_SHAPE = (len(FEATURES), ) # The expected shape, where the None shape is BATCH_SIZE\n","\n","sampled_dataset = val_dataset.shuffle(buffer_size=250).take(1)\n","for features, labels in train_dataset.take(1):\n","    INPUT_SHAPE = features.numpy().shape[1:]  # Assuming the shape is (batch_size, len(FEATURES))\n","    print(\"Features:\", features.numpy())\n","    print(\"Labels:\", labels.numpy())\n","\n","print(\"INPUT_SHAPE:\", INPUT_SHAPE)"]},{"cell_type":"markdown","metadata":{},"source":["# CNN \n","\n","## Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Conv1D, Add, Multiply, Input, Flatten, Dense, GlobalAveragePooling1D, MaxPooling1D, SpatialDropout1D, Activation, Dropout, LeakyReLU, BatchNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2, l1, l1_l2\n","from tensorflow.keras.initializers import Constant, HeNormal\n","\n","MODEL_NAME = \"WAVENET\" # BASE, RESNET, WAVENET\n","MAX_DILATION = 8\n","FILTERS = 32\n","DROPRATE = 0.4\n","KERNEL_SIZE = 3\n","REG_WEIGHTS = 1e-5\n","CONVOLUTIONS = 12\n","DENSE_SIZE = 128\n","DENSE_DEPTH = 8\n","\n","def wavenet_block(inputs, layer_id, filters, kernel_size, dilation_rate, reg_param, dropout_rate):\n","    conv_f = Conv1D(filters, kernel_size,\n","                    dilation_rate=dilation_rate,\n","                    padding='causal',\n","                    kernel_regularizer=l2(reg_param),\n","                    name=f'conv_f_{layer_id}')(inputs)\n","    conv_f = BatchNormalization()(conv_f)\n","    conv_g = Conv1D(filters, kernel_size,\n","                    dilation_rate=dilation_rate,\n","                    padding='causal',\n","                    kernel_regularizer=l2(reg_param),\n","                    name=f'conv_g_{layer_id}')(inputs)\n","    conv_g = BatchNormalization()(conv_g)\n","\n","    # Gate\n","    tanh_out = LeakyReLU()(conv_f)\n","    sigmoid_out = Activation('sigmoid')(conv_g)\n","    merged = Multiply()([tanh_out, sigmoid_out])\n","    merged = SpatialDropout1D(dropout_rate)(merged)\n","\n","    # Residuals\n","    skip_out = Conv1D(filters, 1, padding='same',\n","                      kernel_regularizer=l2(reg_param),\n","                      name=f'skip_{layer_id}')(merged)\n","    skip_out = BatchNormalization()(skip_out)\n","    residual_out = Conv1D(filters, 1, padding='same',\n","                          kernel_regularizer=l2(reg_param),\n","                          name=f'residual_{layer_id}')(inputs)\n","    residual_out = BatchNormalization()(residual_out)\n","    residual_out = Add()([residual_out, skip_out])\n","\n","    return residual_out, skip_out\n","\n","\n","\n","def build_wavenet_model(input_shape,\n","                        conv_layers = CONVOLUTIONS,\n","                        max_dilation = MAX_DILATION,\n","                        filters = FILTERS,\n","                        kernel_size = KERNEL_SIZE,\n","                        reg_param = REG_WEIGHTS,\n","                        dropout_rate = DROPRATE,\n","                        dense_units = DENSE_SIZE,\n","                        dense_layers = DENSE_DEPTH // 4,\n","                        output_bias=None):\n","    inputs = Input(shape=input_shape)\n","    x = inputs\n","    skip_connections = []\n","\n","    for block_id in range(int(conv_layers)):\n","        dilation_rate = min(2 ** block_id, max_dilation)\n","        x, skip = wavenet_block(x, f'block1_{block_id}', filters, kernel_size, dilation_rate, reg_param, dropout_rate)\n","        skip_connections.append(skip)\n","    x = Add()(skip_connections)\n","    x = LeakyReLU()(x)\n","    x = MaxPooling1D()(x)\n","    skip_connections = [] # Reset for the next.\n","    for block_id in range(int(conv_layers//2)):\n","        dilation_rate = min(2 ** block_id, max_dilation)\n","        x, skip = wavenet_block(x, f'block2_{block_id}', filters*2, kernel_size, dilation_rate, reg_param, dropout_rate)\n","        skip_connections.append(skip)\n","    x = Add()(skip_connections)\n","    x = LeakyReLU()(x)\n","    x = GlobalAveragePooling1D()(x)\n","\n","    # Dense layers\n","    for layer_id in range(dense_layers):\n","        x = Dense(dense_units, name=f'dense_{layer_id}', kernel_regularizer=l2(reg_param))(x)\n","        x = LeakyReLU()(x)\n","        x = BatchNormalization()(x)\n","        x = Dropout(dropout_rate)(x)\n","\n","    outputs = Dense(1, activation='sigmoid', name='output_dense', bias_initializer=Constant(output_bias))(x)\n","    model = Model(inputs, outputs, name=MODEL_NAME)\n","    return model\n","\n","\n","def dense_residual_block(in_x, units, reg_param, dropout_rate, layer_id):\n","    x = Dense(units, kernel_regularizer=l2(reg_param), name=f'dense_{layer_id}_1')(in_x)\n","    x = LeakyReLU()(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(dropout_rate)(x)\n","    x = Dense(units, kernel_regularizer=l2(reg_param), name=f'dense_{layer_id}_2')(x)\n","    x = BatchNormalization()(x)\n","    if in_x.shape[-1] != units:\n","        # Original RESNet had a Conv1D\n","        in_x = Dense(units, kernel_initializer=HeNormal(), kernel_regularizer=l1_l2(reg_param))(in_x)\n","    x = Add()([in_x, x])\n","    x = LeakyReLU()(x)\n","    return x\n","\n","def build_deep_resnet_model(input_shape,\n","                            reg_param=REG_WEIGHTS,\n","                            dropout_rate=DROPRATE,\n","                            output_bias=BIAS,\n","                            dense_units = DENSE_SIZE,\n","                            dense_layers = DENSE_DEPTH):\n","    inputs = Input(shape=input_shape)\n","    x = inputs\n","    for layer_id in range(dense_layers):\n","        x = dense_residual_block(x, dense_units, reg_param, dropout_rate, layer_id)\n","\n","    outputs = Dense(1, activation='sigmoid', name='output_dense', bias_initializer=Constant(output_bias))(x)\n","    model = Model(inputs, outputs, name=MODEL_NAME)\n","    return model\n","\n","def build_baseline_model(input_shape,\n","                        reg_param=REG_WEIGHTS,\n","                        dropout_rate=DROPRATE,\n","                        output_bias=BIAS,\n","                        dense_size = DENSE_SIZE):\n","    inputs = Input(shape=input_shape)\n","\n","    x = Dense(dense_size, kernel_regularizer=l1_l2(reg_param), kernel_initializer=HeNormal())(inputs)\n","    x = LeakyReLU()(x)\n","    x = Dense(dense_size, kernel_regularizer=l1_l2(reg_param), kernel_initializer=HeNormal())(x)\n","    x = LeakyReLU()(x)\n","    x = Dropout(dropout_rate)(x)\n","\n","    outputs = Dense(1, activation='sigmoid', name='output_dense', bias_initializer=Constant(output_bias))(x)\n","\n","    return Model(inputs, outputs, name=MODEL_NAME)"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","from tensorflow.keras.losses import BinaryCrossentropy, BinaryFocalCrossentropy\n","from tensorflow.keras.metrics import  AUC, Precision, Recall, TruePositives, TrueNegatives, FalsePositives, FalseNegatives\n","from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n","from tensorflow.keras.optimizers import Adam\n","\n","import shutil\n","import json\n","\n","MODEL_DIR = f\"models/{MODEL_NAME}.keras\"\n","MODEL_HISTORY = f\"{MODEL_PATH}/history.json\"\n","IMAGES_DIR = f\"images/{MODEL_NAME}/images\"\n","LOG_BASEPATH = f\"logs/{MODEL_NAME}/tb\"\n","TARGET_METRIC = \"tp\"\n","\n","EPOCHS = 30\n","PATIENCE_EPOCHS = 5\n","LEARN_RATE =1e-3\n","LEARN_RATE_MIN = 1e-5\n","ALPHA = CLASS_WEIGHTS[1] / (CLASS_WEIGHTS[0] + CLASS_WEIGHTS[1])\n","GAMMA = 2.\n","\n","PURGE = True\n","if PURGE:\n","    # Remove tensorboard logs and other training artefacts for a fresh loop.\n","    shutil.rmtree(LOG_BASEPATH, ignore_errors=True)\n","    shutil.rmtree(MODEL_DIR, ignore_errors=True)\n","    shutil.rmtree(IMAGES_DIR, ignore_errors=True)\n","os.makedirs(IMAGES_DIR, exist_ok=True)\n","os.makedirs(LOG_BASEPATH, exist_ok=True)\n","os.makedirs(MODEL_DIR, exist_ok=True)\n","print(f\"alpha: {ALPHA}, gamma {GAMMA}, bias: {BIAS}\")\n","\n","def build_cnn(input_shape, train_dataset, test_dataset=None,\n","                lr=LEARN_RATE,\n","                lr_min=LEARN_RATE_MIN,\n","                target_metric=TARGET_METRIC,\n","                patience=PATIENCE_EPOCHS,\n","                epochs=EPOCHS,\n","                class_weight=CLASS_WEIGHTS,\n","                initial_bias = BIAS,\n","                conv_layers = CONVOLUTIONS,\n","                max_dilation = MAX_DILATION,\n","                filters = FILTERS,\n","                kernel_size = KERNEL_SIZE,\n","                reg_param = REG_WEIGHTS,\n","                dropout_rate = DROPRATE,\n","                dense_units = DENSE_SIZE,\n","                dense_layers = DENSE_DEPTH // 4):\n","    model = build_wavenet_model(\n","        input_shape=input_shape,\n","        reg_param=reg_param,\n","        dropout_rate=dropout_rate,\n","        output_bias=initial_bias,\n","        conv_layers = conv_layers,\n","        max_dilation = max_dilation,\n","        filters = filters,\n","        kernel_size = kernel_size,\n","        dense_units = dense_units,\n","        dense_layers = dense_layers,\n","    )\n","    optimizer = Adam(learning_rate=lr, clipnorm=1.)\n","    loss = BinaryFocalCrossentropy (from_logits=False,\n","                                    alpha=ALPHA,\n","                                    gamma=GAMMA,\n","                                    reduction='sum_over_batch_size',\n","                                    name='bfce')\n","    model.compile(\n","        loss=loss,\n","        optimizer=optimizer,\n","        metrics=[\n","            TruePositives(name=TARGET_METRIC), # Max TP\n","            TrueNegatives(name='tn'),\n","            FalsePositives(name='fp'),\n","            FalseNegatives(name='fn'),\n","            Precision(name='p'),\n","            Recall(name='r'),\n","            AUC(name='auc'),\n","            AUC(name='prc', curve='PR')\n","        ],\n","    )\n","    callbacks = [\n","        EarlyStopping(\n","            patience=patience,\n","            monitor=f\"val_{target_metric}\",\n","            restore_best_weights=True,\n","            mode=\"max\" # TARGET_METRIC max or min\n","        ),\n","        ReduceLROnPlateau(\n","            monitor=f\"val_{target_metric}\",\n","            factor=0.5,\n","            patience=1,\n","            verbose=1,\n","            min_lr=lr_min,\n","            mode=\"max\" # TARGET_METRIC max or min\n","        ),\n","        TensorBoard(\n","            log_dir=LOG_BASEPATH,\n","            histogram_freq=1,\n","            write_images=True\n","        )\n","    ]\n","    history = model.fit(\n","        train_dataset,\n","        validation_data=test_dataset,\n","        epochs=epochs,\n","        batch_size=BATCH_SIZE,\n","        callbacks=callbacks,\n","        verbose=1,\n","        class_weight=class_weight\n","    )\n","    return model, history\n","\n","history_dict = None\n","with strategy.scope():\n","    if not PURGE and os.path.exists(MODEL_PATH):\n","        print(f\"Loading model from: {MODEL_PATH}\")\n","        model = tf.keras.models.load_model(MODEL_PATH)\n","        if os.path.exists(MODEL_HISTORY):\n","            with open(MODEL_HISTORY, 'r') as f:\n","                history_dict = json.load(f)\n","    else:\n","        print(f\"input_shape: {INPUT_SHAPE}\")\n","        model, history = build_cnn(INPUT_SHAPE, train_dataset=train_dataset, test_dataset=val_dataset)\n","        history_dict = history.history\n","        model.save(MODEL_PATH)\n","        # float32 is not directly serializable to JSON\n","        history_dict = {k: [float(i) for i in v] for k, v in history_dict.items()}\n","        with open(MODEL_HISTORY, 'w') as f:\n","            json.dump(history_dict, f)\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["# Visualize History"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_model_stats(history_dict):\n","    plt.figure(figsize=(18, 10))\n","\n","    # Plotting Loss\n","    plt.subplot(2, 3, 1)\n","    plt.plot(history_dict['loss'], label='Train Loss')\n","    plt.plot(history_dict['val_loss'], label='Val Loss')\n","    plt.title('Model Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend(loc='upper right')\n","\n","    # Plotting AUC\n","    plt.subplot(2, 3, 2)\n","    plt.plot(history_dict['auc'], label='Train AUC')\n","    plt.plot(history_dict['val_auc'], label='Val AUC')\n","    plt.title('Model AUC')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('AUC')\n","    plt.legend(loc='lower right')\n","\n","    # Plotting Precision\n","    plt.subplot(2, 3, 3)\n","    plt.plot(history_dict['p'], label='Train Precision')\n","    plt.plot(history_dict['val_p'], label='Val Precision')\n","    plt.title('Model Precision')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Precision')\n","    plt.legend(loc='lower right')\n","\n","    # Plotting Recall\n","    plt.subplot(2, 3, 4)\n","    plt.plot(history_dict['r'], label='Train Recall')\n","    plt.plot(history_dict['val_r'], label='Val Recall')\n","    plt.title('Model Recall')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Recall')\n","    plt.legend(loc='lower right')\n","\n","    # Plotting True Positives\n","    plt.subplot(2, 3, 5)\n","    plt.plot(history_dict['tp'], label='Train True Positives')\n","    plt.plot(history_dict['val_tp'], label='Val True Positives')\n","    plt.title('Model True Positives')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('True Positives')\n","    plt.legend(loc='upper right')\n","\n","    # Plotting PRC (Precision-Recall Curve)\n","    plt.subplot(2, 3, 6)\n","    plt.plot(history_dict['prc'], label='Train PRC')\n","    plt.plot(history_dict['val_prc'], label='Val PRC')\n","    plt.title('Model PRC')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('PRC')\n","    plt.legend(loc='lower right')\n","\n","    plt.tight_layout()\n","    plt.savefig(f'{IMAGES_DIR}/{MODEL_NAME}_stats.png')\n","    plt.show()\n","\n","if history_dict is not None:\n","    plot_model_stats(history_dict)"]},{"cell_type":"markdown","metadata":{},"source":["# Explain and Interpret"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import seaborn as sns\n","from scipy.stats import norm\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score, roc_auc_score\n","\n","def print_metrics_and_distribution(model, data, labels):\n","    ypred_proba = model.predict(data)\n","    pred = (ypred_proba > 0.5).astype(int)\n","\n","    metrics = {\n","        \"Accuracy\": accuracy_score(labels, pred.flatten()),\n","        \"Precision\": precision_score(labels, pred.flatten()),\n","        \"Recall\": recall_score(labels, pred.flatten()),\n","        \"F1b Score\": fbeta_score(labels, pred.flatten(), average=\"weighted\", beta=0.1),\n","        \"ROC AUC\": roc_auc_score(labels, ypred_proba.flatten(), average='weighted')\n","    }\n","\n","    metrics_df = pd.DataFrame.from_dict(metrics, orient='index')\n","\n","    plt.figure(figsize=(10, 6))\n","    sns.kdeplot(ypred_proba, color='blue', fill=True, alpha=0.7)\n","\n","    mu, std = norm.fit(ypred_proba)\n","    xmin, xmax = plt.xlim()\n","    x = np.linspace(xmin, xmax, 100)\n","    p = norm.pdf(x, mu, std)\n","    plt.plot(x, p, 'k', linewidth=2)\n","\n","    plt.title('PDF')\n","    plt.xlabel('Predicted Probability')\n","    plt.ylabel('Density')\n","\n","    plt.tight_layout()\n","    plt.savefig(f'{IMAGES_DIR}/{MODEL_NAME}_pdf.png')\n","    plt.show()\n","\n","    metrics_df.to_json(f\"{MODEL_PATH}/stats.json\")\n","\n","    return metrics_df\n","\n","if MODEL_NAME == \"WAVENET\":\n","    test_data, test_labels = prepare_windows(val_agri_ts[0][FEATURES], val_agri_ts[0][META_LABEL], window_size=WINDOW)\n","else:\n","    test_data, test_labels = val_agri_ts[0][FEATURES], val_agri_ts[0][META_LABEL]\n","metrics_df = print_metrics_and_distribution(model, test_data, test_labels)\n","metrics_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import seaborn as sns\n","\n","from tensorflow.math import confusion_matrix\n","\n","def plot_confusion_matrix(model, data, labels, label_names=['RW', 'MR']):\n","    ypred_proba = model.predict(data)\n","    pred = (ypred_proba > 0.5).astype(int)\n","\n","    print(labels.shape)\n","    print(pred.shape)\n","    if len(labels.shape) > 0:\n","        labels = labels.flatten()\n","    if len(pred.shape) > 0:\n","        pred = pred.flatten()\n","    cm = confusion_matrix(labels, pred)\n","\n","    plt.figure(figsize=(8, 6))\n","    df_cm = pd.DataFrame((cm / np.sum(cm, axis=1)[:, None])*100, index=[i for i in label_names], columns=[i for i in label_names])\n","    cm_plot = sns.heatmap(df_cm, annot=True, fmt=\".2f\", cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n","    plt.xlabel('Predicted Labels')\n","    plt.ylabel('True Labels')\n","    plt.title('Confusion Matrix')\n","\n","    plt.tight_layout()\n","    plt.savefig(f'{IMAGES_DIR}/{MODEL_NAME}_cm.png')\n","    plt.show()\n","\n","plot_confusion_matrix(model, test_data, test_labels)"]},{"cell_type":"markdown","metadata":{},"source":["## SHAP"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import shap\n","shap.initjs()\n","\n","RANDOMIZE_SIZE = 32\n","SAMPLE_SIZE = 25\n","\n","if MODEL_NAME != \"WAVENET\":\n","    # Shap doesn't work with window encoded data.\n","    background_features, background_labels = train_agri_ts[0][FEATURES].values,train_agri_ts[0][META_LABEL].values\n","    test_data, test_labels = val_agri_ts[0][FEATURES].values, val_agri_ts[0][META_LABEL].values\n","    test_features, test_labels = test_data[:SAMPLE_SIZE], test_labels[:SAMPLE_SIZE]\n","\n","    shap.explainers._deep.deep_tf.op_handlers[\"LeakyRelu\"] = shap.explainers._deep.deep_tf.op_handlers[\"Relu\"]\n","    shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.op_handlers[\"Add\"]\n","    shap.explainers._deep.deep_tf.op_handlers[\"BatchToSpaceND\"] = shap.explainers._deep.deep_tf.op_handlers[\"Mean\"]\n","    shap.explainers._deep.deep_tf.op_handlers[\"SpaceToBatchND\"] = shap.explainers._deep.deep_tf.op_handlers[\"Mean\"]\n","    # this is a hack: https://github.com/shap/shap/issues/1463\n","\n","    e = shap.DeepExplainer(model, background_features)\n","\n","    shap_values = e.shap_values(test_features)\n","    if isinstance(shap_values, list):\n","        shap_values = shap_values[0]\n","    shap_values = np.squeeze(shap_values)\n","\n","    print(f\"SHAP values shape: {shap_values.shape}\")\n","    print(f\"Test features shape: {test_features.shape}\")\n","    assert shap_values.shape == test_features.shape\n","    shap.summary_plot(shap_values, test_features, feature_names=FEATURES)\n","    plt.savefig(f'{IMAGES_DIR}/{MODEL_NAME}_shap_sum.png')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if MODEL_NAME != \"WAVENET\":\n","    sample_index = 2\n","    e = shap.KernelExplainer(model, background)\n","\n","    select = range(SAMPLE_SIZE)\n","    shap_features = test_features[select]\n","    train_features = background_features[select]\n","    shap_values = e.shap_values(shap_features, nsamples=SAMPLE_SIZE)\n","    print(f\"SHAP values shape: {shap_values.shape}\")\n","\n","    if isinstance(shap_values, list):\n","        shap_values = shap_values[0: SAMPLE_SIZE]\n","    shap_values = np.squeeze(shap_values)\n","    y_pred = (shap_values.sum(1) + e.expected_value) > 0\n","    misclassified = y_pred != test_labels[select]\n","    print(f\"({shap_values.sum(1)} + {e.expected_value}) > 0\")\n","    print(f\"Misclassified: {np.shape(misclassified)} out of {np.shape(y_pred)[0]}\")\n","\n","    print(f\"Explainer expected value: {e.expected_value}\")\n","    shap.decision_plot(e.expected_value, shap_values, train_features, feature_names=FEATURES, link='logit', highlight=misclassified)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if MODEL_NAME != \"WAVENET\":\n","    shap.decision_plot(\n","        e.expected_value,\n","        shap_values[misclassified],\n","        train_features[misclassified],\n","        link=\"logit\",\n","        highlight=0,\n","        feature_names=FEATURES\n","    )\n","    plt.savefig(f'{IMAGES_DIR}/{MODEL_NAME}_shap_force.png')\n","\n","    shap.force_plot(\n","        e.expected_value,\n","        shap_values[misclassified],\n","        train_features[misclassified],\n","        link=\"logit\",\n","        feature_names=FEATURES\n","    )\n","    plt.savefig(f'{IMAGES_DIR}/{MODEL_NAME}_shap_force_misclassed.png')"]},{"cell_type":"markdown","metadata":{},"source":["# Grid Search and CV"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n","from tensorboard.plugins.hparams import api as hp\n","from tensorflow.summary import create_file_writer\n","\n","import json\n","\n","HP_KERNEL_SIZE = hp.HParam(\"kernel_size\", hp.Discrete([int(KERNEL_SIZE * 2), int(KERNEL_SIZE), int(KERNEL_SIZE // 2)]))\n","HP_BATCH_SIZE = hp.HParam(\"batch_size\", hp.Discrete([int(BATCH_SIZE)]))\n","HP_EPOCHS = hp.HParam(\"epochs\", hp.Discrete([int(EPOCHS)]))\n","HP_DILATION_RATE = hp.HParam(\"dilation_rate\", hp.Discrete([int(MAX_DILATION), int(MAX_DILATION * 2)]))\n","HP_DROPOUT_RATE = hp.HParam(\"dropout_rate\", hp.Discrete([float(DROPRATE), float(DROPRATE * 2)]))\n","HP_REG_WEIGHTS = hp.HParam(\"reg_weight\", hp.Discrete([float(REG_WEIGHTS), float(REG_WEIGHTS / 2)]))\n","HP_LEARNING_RATE = hp.HParam(\"learning_rate\", hp.Discrete([float(LEARN_RATE)]))\n","HP_PATIENCE = hp.HParam(\"patience\", hp.Discrete([int(PATIENCE_EPOCHS)]))\n","HP_DENSE_DEPTH = hp.HParam(\"dense_depth\", hp.Discrete([int(DENSE_DEPTH), int(DENSE_DEPTH * 2)]))\n","HP_DENSE_UNITS = hp.HParam(\"dense_units\", hp.Discrete([int(DENSE_DEPTH // 2), int(DENSE_DEPTH), int(DENSE_DEPTH * 2)]))\n","HP_FILTERS = hp.HParam(\"filters\", hp.Discrete([int(FILTERS // 2), int(FILTERS), int(FILTERS * 2)]))\n","HP_CONVOLUTIONS= hp.HParam(\"convolutions\", hp.Discrete([int(CONVOLUTIONS // 2), int(CONVOLUTIONS), int(CONVOLUTIONS * 2)]))\n","HPARAMS = [\n","    HP_FILTERS,\n","    HP_KERNEL_SIZE,\n","    HP_BATCH_SIZE,\n","    HP_EPOCHS,\n","    HP_DILATION_RATE,\n","    HP_DROPOUT_RATE,\n","    HP_REG_WEIGHTS,\n","    HP_LEARNING_RATE,\n","    HP_PATIENCE,\n","    HP_DENSE_UNITS,\n","    HP_DENSE_DEPTH,\n","    HP_CONVOLUTIONS]\n","\n","def grid_search_build_cnn(input_shape, train_dataset, test_dataset, hparams=HPARAMS, file_name=f\"best_params.json\", checkpoint_file = f\"checkpoint.json\"):\n","    def _decode_arrays(config_str):\n","        return [int(unit) for unit in config_str.split('_')]\n","\n","    def _save_best_params(best_params, best_loss, best_metric, other_metrics = None, file_name=\"best_params.json\"):\n","        os.makedirs(MODEL_DIR, exist_ok=True)\n","        with open(f\"{MODEL_DIR}/{file_name}\", \"w\") as file:\n","            json.dump({\"best_params\": best_params, \"best_loss\": best_loss, \"best_metric\": best_metric, 'other_metrics': other_metrics}, file)\n","\n","    def _load_checkpoint(file_name):\n","        json = None\n","        try:\n","            os.makedirs(MODEL_DIR, exist_ok=True)\n","            with open(f\"{MODEL_DIR}/{file_name}\", \"r\") as file:\n","                json = json.load(file)\n","        except Exception as e:\n","            print(f\"File {MODEL_DIR}/{file_name} not found or error {e}\")\n","        return json\n","\n","    def _save_checkpoint(state, file_name):\n","        os.makedirs(MODEL_DIR, exist_ok=True)\n","        with open(f\"{MODEL_DIR}/{file_name}\", \"w\") as file:\n","            json.dump(state, file)\n","\n","    with create_file_writer(f\"{LOG_BASEPATH}/hparam_tuning\").as_default():\n","        hp.hparams_config(\n","            hparams=hparams,\n","            metrics=[hp.Metric(TARGET_METRIC, display_name=TARGET_METRIC)],\n","        )\n","\n","    start_index = 0\n","    best_loss = np.inf\n","    best_metric = -np.inf\n","    best_params = None\n","    checkpoint = _load_checkpoint(checkpoint_file)\n","    if checkpoint:\n","        start_index = checkpoint['next_index']\n","        best_loss = checkpoint['best_loss']\n","        best_metric = checkpoint['best_metric']\n","        best_params = checkpoint['best_params']\n","\n","    grid = list(ParameterGrid({h.name: h.domain.values for h in hparams}))\n","    for index, hp_values in enumerate(tqdm(grid[start_index:], desc=\"Grid Search..\"), start=start_index):\n","        lr = hp_values[\"learning_rate\"]\n","        conv_layers=hp_values[\"convolutions\"]\n","        max_dilation=hp_values[\"dilation_rate\"]\n","        filters=hp_values[\"filters\"]\n","        kernel_size=hp_values[\"kernel_size\"]\n","        reg_param=hp_values[\"reg_weight\"]\n","        dropout_rate=hp_values[\"dropout_rate\"]\n","        dense_units=hp_values[\"dense_units\"]\n","        dense_layers=hp_values[\"dense_depth\"]\n","\n","        model, history = build_cnn(input_shape,\n","                                   train_dataset, test_dataset=test_dataset,\n","                                    lr=lr,\n","                                    lr_min=LEARN_RATE_MIN,\n","                                    target_metric=TARGET_METRIC,\n","                                    conv_layers=conv_layers,\n","                                    max_dilation=max_dilation,\n","                                    filters=filters,\n","                                    kernel_size=kernel_size,\n","                                    reg_param=reg_param,\n","                                    dropout_rate=dropout_rate,\n","                                    dense_units=dense_units,\n","                                    dense_layers=dense_layers)\n","\n","        history_dict = history.history\n","        loss = history_dict[f\"val_loss\"][-1]\n","        metric = history_dict[f\"val_{TARGET_METRIC}\"][-1]\n","        if (metric > best_metric):\n","            best_history = history\n","            best_loss = loss\n","            best_metric = metric\n","            best_model = model\n","            best_params = hp_values\n","            other_metrics = {\n","                f\"{TARGET_METRIC}\": history_dict[f\"{TARGET_METRIC}\"][-1],\n","                f\"v_{TARGET_METRIC}\": history_dict[f\"val_{TARGET_METRIC}\"][-1],\n","                'ba': history_dict[\"ba\"][-1],\n","                'v_ba': history_dict[\"val_ba\"][-1],\n","            }\n","            _save_best_params(best_params, best_loss, best_metric, other_metrics, file_name)\n","        checkpoint_state = {\n","            'next_index': index + 1,\n","            'best_loss': best_loss,\n","            'best_metric': best_metric,\n","            'best_params': best_params\n","        }\n","        _save_checkpoint(checkpoint_state, checkpoint_file)\n","    return best_model, best_history, best_params, best_loss, best_metric\n","\n","PARAM_SEARCH = True\n","if PARAM_SEARCH:\n","    with strategy.scope():\n","        model, history, best_params, best_loss, best_metric = grid_search_build_cnn(INPUT_SHAPE, train_dataset, val_dataset)\n","        print(best_params)\n","        print(best_metric)"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":5117138,"sourceId":8561110,"sourceType":"datasetVersion"},{"sourceId":180912526,"sourceType":"kernelVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
