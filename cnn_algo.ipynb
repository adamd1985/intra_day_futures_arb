{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","id":"oaDoHbxVH0CW"},"source":["# CNN"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","id":"z_cBqdYOoY5S"},"source":["## Notebook's Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","collapsed":false,"id":"eETPYJLiMU-b","jupyter":{"outputs_hidden":false},"outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"outputs":[],"source":["INSTALL_DEPS = False\n","if INSTALL_DEPS:\n","  %pip install hurst==0.0.5\n","  %pip install imbalanced_learn==0.12.3\n","  %pip install imblearn==0.0\n","  %pip install protobuf==5.27.0\n","  %pip install pykalman==0.9.7\n","  %pip install tqdm==4.66.4\n","\n","!python --version"]},{"cell_type":"markdown","metadata":{},"source":["## Cloud Environment Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","collapsed":false,"id":"Q4-GoceIIfT_","jupyter":{"outputs_hidden":false},"outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","IN_KAGGLE = IN_COLAB = False\n","try:\n","    # https://www.tensorflow.org/install/pip#windows-wsl2\n","    import google.colab\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/drive\")\n","    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n","    MODEL_PATH = \"/content/drive/MyDrive/models\"\n","    IN_COLAB = True\n","    print(\"Colab!\")\n","except:\n","    IN_COLAB = False\n","if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and not IN_COLAB:\n","    print(\"Running in Kaggle...\")\n","    for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n","        for filename in filenames:\n","            print(os.path.join(dirname, filename))\n","    MODEL_PATH = \"./models\"\n","    DATA_PATH = \"/kaggle/input/intra-day-agriculture-futures-trades-2023-2024\"\n","    IN_KAGGLE = True\n","    print(\"Kaggle!\")\n","elif not IN_COLAB:\n","    IN_KAGGLE = False\n","    MODEL_PATH = \"./models\"\n","    DATA_PATH = \"./data/\"\n","    print(\"running localhost!\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras import mixed_precision\n","\n","print(f'Tensorflow version: [{tf.__version__}]')\n","\n","tf.get_logger().setLevel('INFO')\n","\n","#tf.config.set_soft_device_placement(True)\n","#tf.config.experimental.enable_op_determinism()\n","#tf.random.set_seed(1)\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","\n","  tf.config.experimental_connect_to_cluster(tpu)\n","  tf.tpu.experimental.initialize_tpu_system(tpu)\n","  strategy = tf.distribute.TPUStrategy(tpu)\n","except Exception as e:\n","  # Not an exception, just no TPUs available, GPU is fallback\n","  # https://www.tensorflow.org/guide/mixed_precision\n","  print(e)\n","  policy = mixed_precision.Policy('mixed_float16')\n","  mixed_precision.set_global_policy(policy)\n","  gpus = tf.config.experimental.list_physical_devices('GPU')\n","  if len(gpus) > 0:\n","    try:\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n","        strategy = tf.distribute.MirroredStrategy()\n","\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n","    except RuntimeError as e:\n","        print(e)\n","    finally:\n","        print(\"Running on\", len(tf.config.list_physical_devices('GPU')), \"GPU(s)\")\n","  else:\n","    # CPU is final fallback\n","    strategy = tf.distribute.get_strategy()\n","    print(\"Running on CPU\")\n","\n","def is_tpu_strategy(strategy):\n","    return isinstance(strategy, tf.distribute.TPUStrategy)\n","\n","print(\"Number of accelerators:\", strategy.num_replicas_in_sync)\n","os.getcwd()"]},{"cell_type":"markdown","metadata":{},"source":["# Instruments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from utility import *\n","\n","FEATURES_SELECTED = ['10Y_Barcount', '10Y_Spread', '10Y_Volume', '2YY_Spread', '2YY_Volume',\n","                    'CONTRA', 'Filtered_X', 'KG_X', 'KG_Z1', 'RTY_Spread', 'SD', 'Spread',\n","                    'TSMOM', 'VXM_Open', 'VXM_Spread', 'Volume']\n","TARGET_FUT, INTERVAL"]},{"cell_type":"markdown","metadata":{},"source":["## Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from utility import *\n","\n","filename = f\"{DATA_PATH}{os.sep}futures_{INTERVAL}.csv\"\n","print(filename)\n","futs_df = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n","\n","print(futs_df.shape)\n","futs_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["HALF_LIFE, HURST = get_ou(futs_df, f'{TARGET_FUT}_Close')\n","\n","print(\"Half-Life:\", HALF_LIFE)\n","print(\"Hurst:\", HURST)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 4))\n","\n","plt.plot(futs_df[f'{TARGET_FUT}_Close'], label=f'{TARGET_FUT} Close', alpha=0.7)\n","plt.title(f'{TARGET_FUT} Price')\n","plt.xlabel('Date')\n","plt.ylabel('Price')\n","plt.legend()\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare the Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","\n","TEST_SPLIT = 0.6\n","TRAIN_SIZE = int(len(futs_df) * TEST_SPLIT)\n","CACHE = True\n","FUTURES_TMP_FILE = \"./tmp/futures.pkl\"\n","os.makedirs(\"./tmp/\", exist_ok=True)\n","\n","def oversample_mean_reversions(train_agri_ts, window, scalers=None, period=INTERVAL, hurst=HURST):\n","    samples = []\n","    for df, scaler in tqdm(zip(train_agri_ts, scalers or [None]*len(train_agri_ts)), desc=\"oversample_mean_reversions\"):\n","        bb_df = df.copy()\n","        if scaler is not None:\n","            # Revert scaling and perturb with scaler errors\n","            descaled = scaler.inverse_transform(bb_df[COLS_TO_SCALE])\n","            descaled_df = pd.DataFrame(descaled, columns=COLS_TO_SCALE)\n","            bb_df[COLS_TO_SCALE] = descaled_df\n","\n","        results_df = param_search_bbs(bb_df, StockFeatExt.CLOSE, period, initial_window=window * 2, window_min=window // 2, hurst=hurst)\n","        results_df = results_df[results_df[\"Metric\"] == \"Sharpe\"]\n","        bb_df, _ = bollinger_band_backtest(bb_df, StockFeatExt.CLOSE, results_df[\"Window\"].iloc[0], period, std_factor=results_df[\"Standard_Factor\"].iloc[0])\n","        if scaler is not None:\n","            scaled = scaler.transform(bb_df[COLS_TO_SCALE])\n","            scaled_df = pd.DataFrame(scaled, columns=COLS_TO_SCALE)\n","            bb_df[COLS_TO_SCALE] = scaled_df\n","\n","        samples.append(bb_df[train_agri_ts[0].columns].reset_index(drop=True))\n","    return train_agri_ts + samples\n","\n","with strategy.scope():\n","    if not os.path.exists(FUTURES_TMP_FILE):\n","        futs_exog_df = process_exog(MARKET_FUTS, futs_df)\n","        train_agri_ts, val_agri_ts, scalers = process_futures(FUTS, futs_df, futs_exog_df, TRAIN_SIZE, INTERVAL)\n","        train_agri_ts = oversample_mean_reversions(train_agri_ts, HALF_LIFE, scalers=scalers)\n","        val_agri_ts = oversample_mean_reversions(val_agri_ts, HALF_LIFE, scalers=scalers)\n","        if CACHE:\n","            with open(FUTURES_TMP_FILE, 'wb') as f:\n","                pickle.dump((train_agri_ts, val_agri_ts, scalers), f)\n","    else:\n","        with open(FUTURES_TMP_FILE, 'rb') as f:\n","            train_agri_ts, val_agri_ts, scalers = pickle.load(f)\n","\n","np.shape(train_agri_ts)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","\n","PREDICTION_HORIZON = 1\n","WINDOW  = HALF_LIFE\n","WINDOW_TMP_PATH = \"./tmp/\"\n","FEATURES = StockFeatExt.list\n","\n","# TPU see: https://github.com/tensorflow/tensorflow/issues/41635\n","BATCH_SIZE = 8  * strategy.num_replicas_in_sync # Default 8\n","print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n","\n","def prepare_windows(data_df, label_df, window_size=WINDOW, horizon=PREDICTION_HORIZON):\n","    X, y = [], []\n","    for i in range(len(data_df) - window_size - horizon + 1):\n","        input_window = data_df.iloc[i : i + window_size].values\n","        X.append(input_window)\n","        if label_df is not None:\n","            target_window = label_df.iloc[i + window_size : i + window_size + horizon].values\n","            y.append(target_window)\n","    return np.array(X), np.array(y)\n","\n","def prepare_windows_with_disjoint_ts(ts_list, window_size=WINDOW, horizon=PREDICTION_HORIZON):\n","    for data_df in ts_list:\n","        data_df = aug_metalabel_mr(data_df)\n","        X, y = prepare_windows(data_df[FEATURES], data_df[META_LABEL], window_size=window_size, horizon=horizon)\n","        for features, labels in zip(X, y):\n","            yield features, labels\n","\n","# Create TensorFlow dataset from generators\n","def create_tf_dataset_from_generator(ts_list, window_size=WINDOW, horizon=PREDICTION_HORIZON, batch_size=BATCH_SIZE):\n","    dataset = tf.data.Dataset.from_generator(\n","        lambda: prepare_windows_with_disjoint_ts(ts_list, window_size=window_size, horizon=horizon),\n","        output_signature=(\n","            tf.TensorSpec(shape=(window_size, len(FEATURES)), dtype=tf.float32),\n","            tf.TensorSpec(shape=(horizon, ), dtype=tf.float32)\n","        )\n","    )\n","    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","    return dataset\n","\n","with strategy.scope():\n","    train_dataset = create_tf_dataset_from_generator(train_agri_ts)\n","    val_dataset = create_tf_dataset_from_generator(val_agri_ts)"]},{"cell_type":"markdown","metadata":{},"source":["# CNN Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Conv1D, Add, Multiply, Input, Flatten, Dense, AveragePooling1D, SpatialDropout1D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.regularizers import l2\n","\n","MODEL_NAME = \"WAVENET\"\n","\n","def wavenet_block(inputs, filters, kernel_size, dilation_rate, layer_id, reg_param, dropout_rate):\n","    conv_f = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding='causal',\n","                    kernel_regularizer=l2(reg_param), name=f'conv_f_{layer_id}')(inputs)\n","    conv_g = Conv1D(filters, kernel_size, dilation_rate=dilation_rate, padding='causal',\n","                    kernel_regularizer=l2(reg_param), name=f'conv_g_{layer_id}')(inputs)\n","    tanh_out = tf.keras.activations.tanh(conv_f)\n","    sigmoid_out = tf.keras.activations.sigmoid(conv_g)\n","    merged = Multiply()([tanh_out, sigmoid_out])\n","\n","    merged = SpatialDropout1D(dropout_rate)(merged)\n","\n","    skip_out = Conv1D(filters, 1, padding='same', kernel_regularizer=l2(reg_param), name=f'skip_{layer_id}')(merged)\n","    residual_out = Conv1D(filters, 1, padding='same', kernel_regularizer=l2(reg_param), name=f'residual_{layer_id}')(inputs)\n","    residual_out = Add()([residual_out, skip_out])\n","\n","    return residual_out, skip_out\n","\n","def build_wavenet_model(input_shape, filters, kernel_size, dilation_rate, output_horizon, reg_param, dropout_rate, convolutions):\n","    inputs = Input(shape=input_shape)\n","    x = inputs\n","    skip_connections = []\n","\n","    for i in range(1, convolutions - 1):\n","        x, skip = wavenet_block(x,\n","                                filters*2, kernel_size=kernel_size, dilation_rate= 2 ** i,\n","                                layer_id=i,\n","                                reg_param=reg_param, dropout_rate=dropout_rate)\n","        skip_connections.append(skip)\n","\n","    x = Add()(skip_connections)\n","    x = Conv1D(filters, 1, activation='relu', kernel_regularizer=l2(reg_param), name='post_conv_1')(x)\n","    x = AveragePooling1D(pool_size=10, strides=10, name='mean_pooling')(x)\n","    x = Conv1D(filters, 3, padding='same', activation='relu', kernel_regularizer=l2(reg_param), name='non_causal_conv_1')(x)\n","    x = Conv1D(filters, 3, padding='same', activation='relu', kernel_regularizer=l2(reg_param), name='non_causal_conv_2')(x)\n","    x = SpatialDropout1D(dropout_rate)(x)\n","    x = Flatten()(x)\n","    outputs = Dense(output_horizon, activation=None, kernel_regularizer=l2(reg_param), name='output_dense')(x)\n","\n","    model = Model(inputs, outputs, name=MODEL_NAME)\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","\n","from tensorflow.keras.losses import BinaryFocalCrossentropy\n","from tensorflow.keras.metrics import AUC, BinaryAccuracy\n","from tensorflow.keras.callbacks import EarlyStopping, TensorBoard,ReduceLROnPlateau\n","from tensorflow.keras.optimizers import AdamW, Adam, Adamax\n","from tensorflow.keras.mixed_precision import LossScaleOptimizer\n","\n","EPOCHS = 300\n","PATIENCE_EPOCHS = 5\n","MIN_DENSE = 8\n","FILTERS = 32\n","DROPRATE = 0.5\n","KERNEL_SIZE = 2\n","DILATION_RATE = 1\n","REG_WEIGHTS = 0.2\n","CONVOLUTIONS = 4\n","\n","with strategy.scope():\n","    MODEL_DIR = f\"models/{MODEL_NAME}\"\n","    IMAGES_DIR = f\"images/{MODEL_NAME}/images\"\n","    LOG_BASEPATH = f\"logs/{MODEL_NAME}/tb\"\n","    CLASS_WEIGHTS = None # {0: 0.04, 1: 55.}\n","    LEARN_RATE = 0.025 # ReduceLROnPlateau will alter this.\n","    ERROR_ALPHA = 0.5 # 0.5 > gives more weight to positive class errors. The weight for class 0 is 1.0 - alpha.\n","    ERROR_GAMMA =0.4 # focal factor\" to down-weight easy examples loss contribution. 0 > focus on hard examples.\n","    TARGET_METRIC = \"auc\"\n","    # https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryFocalCrossentropy\n","    LOSS = BinaryFocalCrossentropy(apply_class_balancing=False, from_logits=True, alpha=ERROR_ALPHA, gamma=ERROR_GAMMA, name='fbce', reduction=\"auto\")\n","    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy\n","    METRICS = [BinaryAccuracy(name='ba'), AUC(name=TARGET_METRIC, from_logits=True)]\n","\n","    def build_cnn(\n","        input_shape,\n","        train_dataset,\n","        test_dataset=None,\n","        convolutions = CONVOLUTIONS,\n","        output_horizon= PREDICTION_HORIZON,\n","        filters= FILTERS,\n","        kernel_size= KERNEL_SIZE,\n","        dilation_rate= DILATION_RATE,\n","        lr=LEARN_RATE,\n","        patience=PATIENCE_EPOCHS,\n","        epochs=EPOCHS,\n","        batch_size=BATCH_SIZE,\n","        loss=LOSS,\n","        class_weights=CLASS_WEIGHTS,\n","        b_cv = False,\n","    ):\n","        model = build_wavenet_model(input_shape,\n","                                    filters, kernel_size,\n","                                    output_horizon=output_horizon,\n","                                    reg_param=REG_WEIGHTS,\n","                                    dropout_rate=DROPRATE,\n","                                    convolutions=convolutions,\n","                                    dilation_rate=dilation_rate)\n","        # AdamW for deep network.\n","        optimizer = Adamax(learning_rate=lr, clipvalue=1., clipnorm=1.)\n","        if not is_tpu_strategy(strategy):\n","            # TPUs already use bfloat16\n","            optimizer = LossScaleOptimizer(optimizer, dynamic=True)\n","        model.compile(loss=loss, optimizer=optimizer, metrics=METRICS)\n","        callbacks = [EarlyStopping(\n","                        patience=patience,\n","                        monitor=f\"{TARGET_METRIC}\",\n","                        restore_best_weights=True,\n","                    ),\n","                    ReduceLROnPlateau(monitor=f\"{TARGET_METRIC}\",\n","                                    factor=0.5,\n","                                    patience=2,\n","                                    verbose=1 if not b_cv else 0,\n","                                    min_lr=1e-3\n","                    )]\n","        callbacks.append(TensorBoard(log_dir=LOG_BASEPATH,\n","                                    histogram_freq=2,\n","                                    write_graph=True,\n","                                    write_images=True,\n","                                    update_freq='epoch',\n","                                    profile_batch=2))\n","\n","        history = model.fit(\n","            train_dataset,\n","            validation_data=test_dataset,\n","            epochs=epochs,\n","            batch_size=batch_size,\n","            callbacks=callbacks,\n","            class_weight=class_weights,\n","            verbose=1 if not b_cv else 0,\n","        )\n","\n","        return model, history\n","\n","    input_shape = (WINDOW, len(FEATURES))\n","    print(f\"input_shape: {input_shape}\")\n","\n","    model, history = build_cnn(input_shape, train_dataset=train_dataset, test_dataset=val_dataset)\n","    model.save(MODEL_PATH)\n","    model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score, roc_auc_score\n","\n","def print_metrics():\n","    ypred_raw = model.predict([X_t])\n","    pred = (ypred_raw > 0.5).astype(int)\n","    metrics = {}\n","\n","    metrics = {\n","        \"Accuracy\": accuracy_score(y_t.flatten(), pred.flatten()),\n","        \"Precision\": precision_score(y_t.flatten(), pred.flatten()),\n","        \"Recall\": recall_score(y_t.flatten(), pred.flatten()),\n","        \"F1b Score\": fbeta_score(y_t.flatten(), pred.flatten(), average=\"weighted\", beta=0.1),\n","        \"ROC AUC\": roc_auc_score(y_t.flatten(), ypred_raw.flatten(), average='weighted')  # Using raw probabilities\n","    }\n","\n","    metrics_unseen_df = pd.DataFrame.from_dict(metrics, orient='index')\n","    metrics_unseen_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.math import confusion_matrix\n","import seaborn as sns\n","\n","def plot_confusion_matrix(cm, labels, cm2=None, labels2=None):\n","        plt.figure(figsize=(8 if cm2 is not None else 4, 4))\n","        if cm2 is not None:\n","            plt.subplot(1, 2, 1)\n","        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Accent)\n","\n","        df_cm = pd.DataFrame((cm / np.sum(cm, axis=1)[:, None])*100, index=[i for i in labels], columns=[i for i in labels])\n","        cm_plot1 = sns.heatmap(df_cm, annot=True,  fmt=\".2f\", cmap='Blues', xticklabels=labels, yticklabels=labels).get_figure()\n","        plt.xlabel('Predicted Labels')\n","        plt.ylabel('True Labels')\n","        plt.title('Confusion Matrix 1')\n","        tick_marks = np.arange(len(labels))\n","        plt.xticks(tick_marks, labels, rotation=45)\n","        plt.yticks(tick_marks, labels)\n","\n","        cm_plot2=None\n","        if cm2 is not None:\n","            plt.subplot(1, 2, 2)\n","            df_cm = pd.DataFrame((cm2 / np.sum(cm2, axis=1)[:, None])*100, index=[i for i in labels2], columns=[i for i in labels2])\n","            cm_plot12 = sns.heatmap(df_cm, annot=True,  fmt=\".2f\", cmap='Reds', xticklabels=labels, yticklabels=labels).get_figure()\n","            plt.xlabel('Predicted Labels')\n","            plt.title('Confusion Matrix 2')\n","        plt.tight_layout()\n","\n","        return cm_plot1, cm_plot2\n","\n","\n","cm = confusion_matrix(y_t.flatten(), pred)\n","figure, _ = plot_confusion_matrix(cm, labels=[1,0])"]},{"cell_type":"markdown","metadata":{},"source":["# Grid Search"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n","from tensorboard.plugins.hparams import api as hp\n","from tensorflow.summary import create_file_writer\n","import json\n","\n","HP_KERNEL_SIZE = hp.HParam(\"kernel_size\", hp.Discrete([KERNEL_SIZE * 2, KERNEL_SIZE]))\n","HP_BATCH_SIZE = hp.HParam(\"batch_size\", hp.Discrete([BATCH_SIZE]))\n","HP_EPOCHS = hp.HParam(\"epochs\", hp.Discrete([EPOCHS]))\n","HP_DILATION_RATE = hp.HParam(\"dilation_rate\", hp.Discrete([DILATION_RATE]))\n","HP_DROPOUT_RATE = hp.HParam(\"dropout_rate\", hp.Discrete([DROPRATE, DROPRATE*2]))\n","HP_REG_WEIGHTS = hp.HParam(\"reg_weight\", hp.Discrete([REG_WEIGHTS, REG_WEIGHTS*2]))\n","HP_LEARNING_RATE = hp.HParam(\"learning_rate\", hp.Discrete([LEARN_RATE]))\n","HP_PATIENCE = hp.HParam(\"patience\", hp.Discrete([PATIENCE_EPOCHS]))\n","HP_ALPHA = hp.HParam(\"alpha\", hp.Discrete([ERROR_ALPHA, ERROR_ALPHA-0.5, ERROR_ALPHA+0.5]))\n","HP_GAMMA = hp.HParam(\"gamma\", hp.Discrete([ERROR_GAMMA, ERROR_GAMMA-0.5, ERROR_GAMMA+0.5]))\n","HP_HIDDEN_DENSE = hp.HParam(\"dense_units\", hp.Discrete([\n","    f\"{WINDOW}\",\n","    f\"{WINDOW*2}_{WINDOW}\",\n","    f\"{WINDOW*2}_{WINDOW}_{WINDOW//2}\",\n","    f\"{WINDOW*4}_{WINDOW*2}\",\n","]))\n","HP_FILTERS = hp.HParam(\"filters\", hp.Discrete([FILTERS //2 ,FILTERS, FILTERS * 2]))\n","HPARAMS = [\n","    HP_FILTERS,\n","    HP_KERNEL_SIZE,\n","    HP_BATCH_SIZE,\n","    HP_EPOCHS,\n","    HP_DILATION_RATE,\n","    HP_DROPOUT_RATE,\n","    HP_REG_WEIGHTS,\n","    HP_LEARNING_RATE,\n","    HP_PATIENCE,\n","    HP_HIDDEN_DENSE,\n","    HP_ALPHA,\n","        HP_GAMMA\n","    ]\n","\n","def grid_search_build_cnn(input_shape, X, y, Xt=None, yt=None, hparams=HPARAMS, file_name=f\"best_params.json\", checkpoint_file = f\"checkpoint.json\"):\n","    def _decode_arrays(config_str):\n","        return [int(unit) for unit in config_str.split('_')]\n","\n","    def _save_best_params(best_params, best_loss, best_metric, other_metrics = None, file_name=\"best_params.json\"):\n","        os.makedirs(MODEL_DIR, exist_ok=True)\n","        with open(f\"{MODEL_DIR}/{file_name}\", \"w\") as file:\n","            json.dump({\"best_params\": best_params, \"best_loss\": best_loss, \"best_metric\": best_metric, 'other_metrics': other_metrics}, file)\n","\n","    def _load_checkpoint(file_name):\n","        json = None\n","        try:\n","            os.makedirs(MODEL_DIR, exist_ok=True)\n","            with open(f\"{MODEL_DIR}/{file_name}\", \"r\") as file:\n","                json = json.load(file)\n","        except Exception as e:\n","            print(f\"File {MODEL_DIR}/{file_name} not found or error {e}\")\n","        return json\n","\n","    def _save_checkpoint(state, file_name):\n","        os.makedirs(MODEL_DIR, exist_ok=True)\n","        with open(f\"{MODEL_DIR}/{file_name}\", \"w\") as file:\n","            json.dump(state, file)\n","\n","    with create_file_writer(f\"{LOG_BASEPATH}/hparam_tuning\").as_default():\n","        hp.hparams_config(\n","            hparams=hparams,\n","            metrics=[hp.Metric(TARGET_METRIC, display_name=TARGET_METRIC)],\n","        )\n","\n","    start_index = 0\n","    best_loss = np.inf\n","    best_metric = -np.inf\n","    best_params = None\n","    checkpoint = _load_checkpoint(checkpoint_file)\n","    if checkpoint:\n","        start_index = checkpoint['next_index']\n","        best_loss = checkpoint['best_loss']\n","        best_metric = checkpoint['best_metric']\n","        best_params = checkpoint['best_params']\n","\n","    grid = list(ParameterGrid({h.name: h.domain.values for h in hparams}))\n","    for index, hp_values in enumerate(tqdm(grid[start_index:], desc=\"Grid Search..\"), start=start_index):\n","        dense_units = _decode_arrays(hp_values[\"dense_units\"])\n","        filters = _decode_arrays(hp_values[\"filters\"])\n","        b = hp_values[\"bias\"]\n","        k = hp_values[\"kernel_size\"]\n","        d = hp_values[\"dilation_rate\"]\n","        rw = hp_values[\"reg_weight\"]\n","        drop = hp_values[\"dropout_rate\"]\n","\n","        ERROR_ALPHA = hp_values[\"alpha\"]\n","        ERROR_GAMMA = hp_values[\"gamma\"]\n","        print(f\"Shapes{input_shape}: x{X[0].shape}xg{X[1].shape}y{y.shape}, filters {filters}, dense {dense_units}, k: {k}, d: {d}, rw: {rw}, drop: {drop}, b: {b}, alpha: {ERROR_ALPHA},  gamma: {ERROR_GAMMA}\")\n","\n","        model, history = build_cnn(input_shape, X, y,\n","                                    output_horizon=PREDICTION_HORIZON,\n","                                    Xt=Xt, yt=yt,\n","                                    filters=filters,\n","                                    kernel_size=k,\n","                                    b_cv=True)\n","        loss = history.history[f\"val_loss\"][-1]\n","        metric = history.history[f\"val_{TARGET_METRIC}\"][-1]\n","        if (metric > best_metric):\n","            best_history = history\n","            best_loss = loss\n","            best_metric = metric\n","            best_model = model\n","            best_params = hp_values\n","            other_metrics = {\n","                f\"{TARGET_METRIC}\": history.history[f\"{TARGET_METRIC}\"][-1],\n","                f\"v_{TARGET_METRIC}\": history.history[f\"val_{TARGET_METRIC}\"][-1],\n","                'ba': history.history[\"ba\"][-1],\n","                'v_ba': history.history[\"val_ba\"][-1],\n","            }\n","            _save_best_params(best_params, best_loss, best_metric, other_metrics, file_name)\n","        checkpoint_state = {\n","            'next_index': index + 1,\n","            'best_loss': best_loss,\n","            'best_metric': best_metric,\n","            'best_params': best_params\n","        }\n","        _save_checkpoint(checkpoint_state, checkpoint_file)\n","    return best_model, best_history, best_params, best_loss, best_metric\n","\n","PARAM_SEARCH = False\n","if PARAM_SEARCH:\n","    with strategy.scope():\n","        assert not np.any(pd.isna(X)) and not np.any(np.isnan(X_t))\n","        print(f\"{X.shape}\")\n","        input_shape = (\n","            WINDOW,\n","            1 if len(X.shape) < 3 else X.shape[2],\n","        )\n","\n","        model, history, best_params, best_loss, best_metric = grid_search_build_cnn(input_shape, X=X, y=y, Xt=X_t, yt=y_t, hparams=HPARAMS)\n","        print(best_params)\n","        print(best_metric)"]},{"cell_type":"markdown","metadata":{},"source":["# CV"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import TimeSeriesSplit\n","\n","CV_MODEL = True\n","CV_SPLITS = 3\n","\n","def train_cv_model(X, y, input_shape, n_splits=5, perturb=True, window=WINDOW, horizon=PREDICTION_HORIZON):\n","    def _perturb_gaussiannoise(X, noise_level=0.1):\n","        sigma = noise_level * np.std(X)\n","        noise = np.random.normal(0, sigma, X.shape)\n","        return X + noise\n","\n","    if perturb:\n","        X = _perturb_gaussiannoise(X)\n","\n","    results = []\n","    tscv = TimeSeriesSplit(n_splits=n_splits)\n","    global metrics_col\n","    metrics_col = None\n","\n","    for train_index, test_index in tqdm(tscv.split(X), desc=f\"CV Testing for n_splits: {n_splits}\"):\n","        X_train = X.iloc[train_index]\n","        y_train = y.iloc[train_index]\n","        X_test = X.iloc[test_index]\n","        y_test = y.iloc[test_index]\n","\n","        X_train_windows, y_train_windows = prepare_windows(X_train, y_train, window_size=window, horizon=horizon)\n","        X_test_windows, y_test_windows = prepare_windows(X_test, y_test, window_size=window, horizon=horizon)\n","\n","        try:\n","            cv_model, _ = build_cnn(input_shape, X=X_train_windows, y=y_train_windows, Xt=X_test_windows, yt=y_test_windows)\n","\n","            result = cv_model.evaluate([X_test_windows], y_test_windows, verbose=0)\n","            results.append(result)\n","\n","            if metrics_col is None:\n","                metrics_col = cv_model.metrics\n","        except Exception as e:\n","            print(f\"CV error on fold with exception: {e}\")\n","\n","    if metrics_col is None:\n","        raise ValueError(\"No successful model training; metrics_col is None\")\n","\n","    metrics_names = [metric.name for metric in metrics_col]\n","    results_df = pd.DataFrame(results, columns=metrics_names)\n","\n","    return results_df\n","\n","# results_df = train_cv_model(train_ts_df.drop(columns=[META_LABEL]), train_ts_df[META_LABEL], input_shape)\n","# results_df"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":4755137,"sourceId":8061237,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
