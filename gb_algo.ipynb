{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","id":"oaDoHbxVH0CW"},"source":["# Gradient Boosting Trees"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","id":"z_cBqdYOoY5S"},"source":["## Notebook's Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","collapsed":false,"id":"eETPYJLiMU-b","jupyter":{"outputs_hidden":false},"outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"outputs":[],"source":["INSTALL_DEPS = False\n","if INSTALL_DEPS:\n","  %pip install matplotlib==3.8.3\n","  %pip installnumpy==1.26.4\n","  %pip installpandas==2.2.1\n","  %pip installpandas_market_calendars==4.4.0\n","  %pip installpytz==2024.1\n","  %pip installscipy==1.12.0\n","  %pip installta==0.11.0\n","  %pip installyfinance==0.2.37\n","\n","!python --version"]},{"cell_type":"markdown","metadata":{},"source":["## Cloud Environment Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","collapsed":false,"id":"Q4-GoceIIfT_","jupyter":{"outputs_hidden":false},"outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","IN_KAGGLE = IN_COLAB = False\n","try:\n","    # https://www.tensorflow.org/install/pip#windows-wsl2\n","    import google.colab\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/drive\")\n","    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n","    MODEL_PATH = \"/content/drive/MyDrive/models\"\n","    IN_COLAB = True\n","    print(\"Colab!\")\n","except:\n","    IN_COLAB = False\n","if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and not IN_COLAB:\n","    print(\"Running in Kaggle...\")\n","    for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n","        for filename in filenames:\n","            print(os.path.join(dirname, filename))\n","    MODEL_PATH = \"./models\"\n","    DATA_PATH = \"/kaggle/input/\"\n","    IN_KAGGLE = True\n","    print(\"Kaggle!\")\n","elif not IN_COLAB:\n","    IN_KAGGLE = False\n","    MODEL_PATH = \"./models\"\n","    DATA_PATH = \"./data/\"\n","    print(\"running localhost!\")"]},{"cell_type":"markdown","metadata":{},"source":["# Instruments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from constants import *\n","\n","TARGET_FUT, INTERVAL"]},{"cell_type":"markdown","metadata":{},"source":["## Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","filename = f\"{DATA_PATH}{os.sep}futures_{INTERVAL}.csv\"\n","print(filename)\n","futs_df = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n","\n","print(futs_df.shape)\n","futs_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 4))\n","\n","plt.plot(futs_df[f'{TARGET_FUT}_Close'], label=f'{TARGET_FUT} Close', alpha=0.7)\n","plt.title(f'{TARGET_FUT} Price')\n","plt.xlabel('Date')\n","plt.ylabel('Price')\n","plt.legend()\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare the Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from signals import dynamic_support_resistance, kalman_backtest, kf_bollinger_band_backtest, tsmom_backtest\n","from quant_equations import get_ou, modulate_std\n","from tqdm import tqdm\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","KF_COLS = ['SD','Z1', 'Z2', 'Filtered_X', 'KG_X', 'KG_Z1', 'KG_Z2'] # ['Z1', 'Z2', 'Filtered_X', 'Uncertainty', 'Residuals', 'KG_X', 'KG_Z1', 'KG_Z2']\n","BB_COLS = ['MA', 'U','L'] # ['SB','SS','SBS','SSB', 'Unreal_Ret', 'MA','SD', 'U','L', '%B', 'X']\n","SR_COLS = [\"Support\", \"Resistance\"] # [\"PP\", \"S1\", \"R1\", \"S2\", \"R2\", \"Support\", \"Resistance\"]\n","MOM_COLS = [\"TSMOM\", \"CONTRA\"]\n","MARKET_COLS = [f\"{fut}_{col}\" for col in StockFeat.list for fut in MARKET_FUTS]\n","# We scale RAW column, the rest are percentages or log values.\n","COLS_TO_SCALE = StockFeat.list + BB_COLS + SR_COLS\n","\n","def augment_ts(df, target_close, target_high, target_low, target_volume, interval, cols_to_scale=COLS_TO_SCALE, scaler=None):\n","    hl, h = get_ou(df, target_close)\n","    window = abs(hl)\n","    mod_std = modulate_std(h)\n","\n","    mom_df, _ = tsmom_backtest(df, target_close, interval, int(window*2), contra_lookback=window//2, std_threshold=mod_std)\n","    bb_df, _ = kf_bollinger_band_backtest(df[target_close], df[target_volume], interval, std_factor=mod_std)\n","    sr_df, _, _ = dynamic_support_resistance(df, target_close, target_high, target_low, initial_window_size=window)\n","    kf_df, _ = kalman_backtest(bb_df[\"%B\"].bfill().ffill(), df[target_volume], df[target_close], period=interval)\n","\n","    aug_ts_df = pd.concat([df[StockFeat.list], sr_df, kf_df, bb_df, mom_df], axis=1).bfill().ffill()\n","    aug_ts_df = aug_ts_df.loc[:, ~aug_ts_df.columns.duplicated(keep=\"first\")]\n","    if cols_to_scale is not None:\n","        # Scale the raw values, and concat with the signals.\n","        aug_df_scaled = None\n","        if scaler is None:\n","            scaler = StandardScaler()\n","            aug_df_scaled = scaler.fit_transform(aug_ts_df[cols_to_scale])\n","        else:\n","            aug_df_scaled = scaler.transform(aug_ts_df[cols_to_scale])\n","\n","        aug_df_scaled = pd.DataFrame(aug_df_scaled, columns=cols_to_scale)\n","        aug_ts_df = pd.concat([aug_df_scaled, aug_ts_df.drop(columns=cols_to_scale).reset_index(drop=True)], axis=1)\n","        aug_ts_df = aug_ts_df.loc[:, ~aug_ts_df.columns.duplicated(keep=\"first\")]\n","\n","    return aug_ts_df, scaler\n","\n","def process_exog(futures, futs_df, universe_cols=UNIVERSE_COLS):\n","    futs_exog_ts = []\n","    for f in tqdm(futures, desc=\"process_exog\"):\n","        fut_df = futs_df.filter(regex=f\"{f}_.*\")\n","\n","        universe_cols.update(fut_df.columns.tolist())\n","\n","        train_df = fut_df\n","        futs_exog_ts.append(train_df)\n","\n","    futs_exog_df = pd.concat(futs_exog_ts, axis=1)\n","\n","    return futs_exog_df\n","\n","def process_futures(futures, futs_df, futs_exog_df, train_size, interval, cols_to_scale=COLS_TO_SCALE):\n","    training_ts = []\n","    val_ts = []\n","    scalers = []\n","    for f in tqdm(futures, desc=\"process_futures\"):\n","        fut_df = futs_df.filter(regex=f\"{f}_.*\")\n","        fut_df.columns = fut_df.columns.str.replace(f\"{f}_\", \"\", regex=False)\n","        fut_df = pd.concat([fut_df, futs_exog_df], axis=1)\n","\n","        train_df, scaler = augment_ts(fut_df.iloc[:train_size], StockFeat.CLOSE, StockFeat.HIGH, StockFeat.LOW, StockFeat.VOLUME, interval, cols_to_scale=cols_to_scale)\n","        test_df, _ = augment_ts(fut_df.iloc[train_size:], StockFeat.CLOSE, StockFeat.HIGH, StockFeat.LOW, StockFeat.VOLUME, interval, cols_to_scale=cols_to_scale, scaler=scaler)\n","        training_ts.append(train_df.reset_index(drop=True))\n","        val_ts.append(test_df.reset_index(drop=True))\n","        scalers.append(scaler) # we use these later in the validation.\n","\n","    return training_ts, val_ts, scalers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["TEST_SPLIT = 0.7\n","TRAIN_SIZE = int(len(futs_df) * TEST_SPLIT)\n","\n","futs_exog_df = process_exog(MARKET_FUTS, futs_df)\n","train_agri_ts, val_agri_ts, scalers = process_futures(AGRI_FUTS, futs_df, futs_exog_df, TRAIN_SIZE, INTERVAL)\n","# Stacking the lists of dataframes into single dataframes\n","train_ts_df = pd.concat([df.reset_index(drop=True) for df in train_agri_ts], axis=0, ignore_index=True).dropna()\n","test_ts_df = pd.concat([df.reset_index(drop=True) for df in val_agri_ts], axis=0, ignore_index=True).dropna()\n","\n","train_ts_df.tail(5)"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["META_LABEL = \"mr_label\"\n","\n","def aug_metalabel_mr(df, metalabel = META_LABEL):\n","    df = df.copy()\n","    df[metalabel] = 0\n","    position = 0\n","    start_index = None\n","    df[META_LABEL] = 0\n","    for i, row in tqdm(df.iterrows(), desc=\"Posthoc Metalabeling\"):\n","        if row['Closed'] != 0:\n","            # Position closed, work backwards\n","            metalabel = (row['Ret'] > 0.).astype(int)\n","            if start_index is not None and metalabel:\n","                df.loc[start_index:row.name, META_LABEL] = metalabel\n","            position = 0\n","            start_index = None\n","        if row['Position'] != 0 and position == 0:\n","            # New position opened\n","            position = row['Position']\n","            start_index = row.name\n","\n","    return df\n","\n","train_ts_df = aug_metalabel_mr(train_ts_df)\n","test_ts_df = aug_metalabel_mr(test_ts_df)\n","\n","train_ts_df[train_ts_df[META_LABEL] > 0][[\"Ret\", META_LABEL]]"]},{"cell_type":"markdown","metadata":{},"source":["# Data Clean"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CORR_THRESHOLD = 0.90\n","DROP_COL =False\n","\n","ALL_COLS = np.concatenate([StockFeat.list, np.array(MARKET_COLS), np.array(KF_COLS), np.array(BB_COLS), np.array(SR_COLS), np.array(MOM_COLS)])\n","\n","y_train = train_ts_df[META_LABEL]\n","X_train = train_ts_df[ALL_COLS]\n","\n","y_test = test_ts_df[META_LABEL]\n","X_test = test_ts_df[ALL_COLS]\n","\n","X_train_preclean = X_train.copy()\n","X_test_preclean = X_test.copy()\n","\n","corr_matrix = X_train.corr().abs()\n","upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n","to_drop = [column for column in upper.columns if any(upper[column] > CORR_THRESHOLD)]\n","to_drop = sorted(to_drop, key=lambda x: (upper.columns.get_loc(x), -upper[x].max()))\n","print(f\"These are highly corr: {to_drop}\")\n","\n","if to_drop is not None and not DROP_COL:\n","    X_train = X_train.drop(columns=to_drop)\n","    X_test = X_test.drop(columns=to_drop)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from statsmodels.tools.tools import add_constant\n","\n","VIF_THRESHOLD = 5\n","\n","def calculate_vif(X):\n","    x_const = add_constant(X)\n","    vif_data = pd.DataFrame()\n","    vif_data[\"feature\"] = x_const.columns\n","    vif_data[\"VIF\"] = [\n","        variance_inflation_factor(x_const.values, i)\n","        for i in range(x_const.shape[1])\n","    ]\n","    return vif_data\n","\n","vif_data = calculate_vif(X_train)\n","vif_data = vif_data.sort_values(by=\"VIF\", ascending=False)\n","\n","vif_data = vif_data.replace([np.inf, -np.inf], np.nan).dropna()\n","acceptable_vif = vif_data[vif_data[\"VIF\"] < VIF_THRESHOLD].sort_values(by=\"feature\")\n","selected_features = acceptable_vif[\"feature\"].tolist()\n","if 'const' in selected_features:\n","    selected_features.remove('const')\n","\n","if not DROP_COL:\n","    X_train = X_train[selected_features]\n","    X_test = X_test[selected_features]\n","\n","print(f\"Multi-Colinear: {vif_data[vif_data['VIF'] >= VIF_THRESHOLD]['feature'].values}\")\n","\n","CLEAN_FEATURES = X_train.columns"]},{"cell_type":"markdown","metadata":{},"source":["# GBC"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","def param_search():\n","    # Best parameters found: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n","    model = GradientBoostingClassifier(random_state=42)\n","    param_grid = {\n","        'n_estimators': [50, 150, 300],\n","        'learning_rate': [0.001, 0.01, 0.1],\n","        'max_depth': [3, 7, 9],\n","    }\n","\n","    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='precision', verbose=1, n_jobs=-1)\n","    grid_search.fit(X_train, y_train)\n","    print(f\"Best parameters found: {grid_search.best_params_}\")\n","    print(f\"Best precision score: {grid_search.best_score_}\")\n","\n","    best_model = grid_search.best_estimator_\n","    return best_model\n","\n","PARAM_SEARCH = True\n","if PARAM_SEARCH:\n","    gbc = param_search()\n","else:\n","    gbc = GradientBoostingClassifier(random_state=42, learning_rate=0.01, max_depth=3, n_estimators=300)\n","    gbc.fit(X_train, y_train)\n","gbc"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report, roc_curve, auc, precision_recall_curve, fbeta_score, precision_score, recall_score, RocCurveDisplay, PrecisionRecallDisplay\n","\n","def print_classification_metrics(X_test, y_test, best_model):\n","    # Predictions and probabilities\n","    y_pred = best_model.predict(X_test)\n","    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n","\n","    print(classification_report(y_test, y_pred))\n","\n","    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7))\n","\n","    # ROC Curves\n","    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n","    roc_auc = auc(fpr, tpr)\n","    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n","    RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=\"GBC\").plot(ax=ax1)\n","    ax1.set_title('Receiver Operating Characteristic (ROC)')\n","    PrecisionRecallDisplay(precision=precision, recall=recall).plot(ax=ax2)\n","    ax2.set_title('Precision-Recall Curve')\n","\n","    plt.show()\n","\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = fbeta_score(y_test, y_pred, average='weighted', beta=0.)\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'F1 Beta Score: {f1:.4f}')\n","\n","print_classification_metrics(X_test, y_test, gbc)"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Selection"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature_importance = gbc.feature_importances_\n","features = X_train.columns if isinstance(X_train, pd.DataFrame) else [f'Feature {i}' for i in range(X_train.shape[1])]\n","\n","# Create a DataFrame for feature importance\n","gbc_feat_df = pd.DataFrame({'Feature': features, 'Importance': feature_importance})\n","gbc_feat_df = gbc_feat_df.sort_values(by=\"Importance\", ascending=False)\n","gbc_feat_df = gbc_feat_df[gbc_feat_df[\"Importance\"] > 0.1]\n","plt.figure(figsize=(10, 8))\n","plt.barh(gbc_feat_df[\"Feature\"], gbc_feat_df[\"Importance\"], align='center')\n","plt.xlabel('Feature Importance')\n","plt.ylabel('Feature')\n","plt.title('Feature Importance for Gradient Boosting Classifier')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## SHAP\n","\n","The summary plot provides a global view of feature importance across all predictions. The SHAP value represents the impact of each feature on the model's output. \n","\n","Positive SHAP values push the prediction towards one class (mean reversion in your case), while negative SHAP values push it towards the other class (random walk).\n","\n","The magnitude of the SHAP value indicates the strength of the impact. Larger absolute values mean a stronger impact on the prediction."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import shap\n","\n","MAX_FEATURES_COUNT = 16\n","\n","explainer = shap.Explainer(gbc, X_train)\n","shap_values = explainer(X_train)\n","\n","shap.summary_plot(shap_values, X_train)\n","shap.decision_plot(explainer.expected_value, shap_values.values[0], X_test.iloc[0])\n","\n","shap_importance = np.abs(shap_values.values).mean(axis=0)\n","shap_importance_df = pd.DataFrame({\n","    'Feature': X_train.columns,\n","    'Importance': shap_importance\n","})\n","\n","shap_importance_df = shap_importance_df.sort_values(by='Importance', ascending=False)\n","shap_importance_df = shap_importance_df[shap_importance_df['Importance'] > 0.001]\n","top_shap_features = shap_importance_df.head(MAX_FEATURES_COUNT)\n","print(top_shap_features)\n","\n","fig, ax = plt.subplots(figsize=(10, 6))\n","top_shap_features.plot(kind='barh', x='Feature', y='Importance', legend=False, ax=ax)\n","ax.set_title(f\"SHAP Values\")\n","ax.set_xlabel(\"Mean Absolute SHAP Value\")\n","ax.set_ylabel(\"Feature\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## PCA"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","MAX_VARIANCE = 0.99\n","\n","pca = PCA()\n","xdata = pca.fit_transform(X_train)\n","\n","cum_var_exp = np.cumsum(pca.explained_variance_ratio_)\n","num_components = np.argmax(cum_var_exp >= MAX_VARIANCE) + 1\n","print(f\"Max components for {MAX_VARIANCE*100}% variance: {num_components} out of {X_train.shape[1]}\")\n","\n","pca = PCA(num_components)\n","xdata = pca.fit_transform(X_train)\n","eigenvectors = pca.components_\n","\n","top_features = np.abs(eigenvectors).sum(axis=0).argsort()[::-1]\n","selected_features = X_train.columns[top_features[:num_components]]\n","loadings_df = pd.DataFrame(eigenvectors[:, top_features[:num_components]], columns=selected_features).T\n","\n","summed_loadings = np.sum(np.abs(eigenvectors[:, top_features[:num_components]]), axis=0)\n","summed_loadings_df = pd.DataFrame(summed_loadings, index=selected_features, columns=[\"Sum\"]).sort_values(by=\"Sum\", ascending=False)\n","\n","fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18, 10), gridspec_kw={'height_ratios': [1, 2]})\n","\n","summed_loadings_df.plot(kind=\"bar\", legend=False, ax=axes[0])\n","axes[0].set_title(\"Summed Loadings Across Top Principal Components\")\n","axes[0].set_ylabel(\"Summed Loadings\")\n","axes[0].set_xlabel(\"Features\")\n","axes[0].tick_params(axis='x', labelrotation=45)\n","\n","loadings_df.plot(kind=\"bar\", legend=False, ax=axes[1])\n","axes[1].set_title(f\"Loadings for Top Principal Components\")\n","axes[1].set_ylabel(\"Loadings\")\n","axes[1].set_xlabel(\"Features\")\n","axes[1].tick_params(axis='x', labelrotation=45)\n","\n","loadings_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import seaborn as sns\n","from scipy.stats import spearmanr\n","from sklearn.feature_selection import mutual_info_regression\n","\n","ic = {}\n","for column in X_train.columns:\n","    corr, p_val = spearmanr(y_train, X_train[column])\n","    ic[column] = [corr, p_val]\n","\n","ic_df = pd.DataFrame(ic, index=[\"IC\", \"p-value\"]).T\n","\n","mi = mutual_info_regression(X=X_train, y=y_train)\n","mi_series = pd.Series(mi, index=X_train.columns)\n","metrics = pd.concat(\n","    [\n","        mi_series.to_frame(\"Mutual Information\"),\n","        ic_df[\"IC\"].to_frame(\"Information Coefficient\"),\n","    ],\n","    axis=1,\n",")\n","\n","\n","top_mi_features = metrics.sort_values(by=\"Mutual Information\", ascending=False).head(num_components)\n","top_ic_features = metrics.sort_values(by=\"Information Coefficient\", ascending=False).head(num_components)\n","\n","fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18, 8))\n","\n","top_mi_features.plot.bar(ax=axes[0], rot=45)\n","axes[0].set_xlabel(\"Features\")\n","axes[0].set_ylabel(\"Scores\")\n","axes[0].set_title(\"Top Features by Mutual Information\")\n","axes[0].tick_params(axis='x', labelrotation=45)\n","sns.despine(ax=axes[0])\n","\n","top_ic_features.plot.bar(ax=axes[1], rot=45)\n","axes[1].set_xlabel(\"Features\")\n","axes[1].set_ylabel(\"Scores\")\n","axes[1].set_title(\"Top Features by Information Coefficient\")\n","axes[1].tick_params(axis='x', labelrotation=45)\n","sns.despine(ax=axes[1])\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features_pca = summed_loadings_df.head(MAX_FEATURES_COUNT).index.tolist()\n","features_miic = (metrics.head(MAX_FEATURES_COUNT).index.tolist())\n","features_shap  = (shap_importance_df.head(MAX_FEATURES_COUNT)['Feature'].tolist())\n","features_gbc = (gbc_feat_df.head(MAX_FEATURES_COUNT)['Feature'].tolist())\n","\n","print(F\"Top {MAX_FEATURES_COUNT} PCA Loadings: {features_pca}\")\n","print(F\"Top {MAX_FEATURES_COUNT} MI/IC: {features_miic}\")\n","print(F\"Top {MAX_FEATURES_COUNT} SHAP: {features_shap}\")\n","print(F\"Top {MAX_FEATURES_COUNT} GBC: {features_gbc}\")\n","\n","DIMREDUC_FEATURES = list(set(features_pca) | set(features_miic) | set(features_shap) | set(features_gbc))\n","\n","print(f\"Selected {len(DIMREDUC_FEATURES)} features: {DIMREDUC_FEATURES}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Dimension Reduction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_dimreduc = X_train[DIMREDUC_FEATURES]\n","X_test_dimreduc = X_test[DIMREDUC_FEATURES]\n","if PARAM_SEARCH:\n","    gbc_dimreduc = param_search()\n","else:\n","    gbc_dimreduc = GradientBoostingClassifier(random_state=42, learning_rate=0.01, max_depth=3, n_estimators=300)\n","    gbc_dimreduc.fit(X_train_dimreduc, y_train)\n","print_classification_metrics(X_test_dimreduc, y_test, gbc_dimreduc)"]},{"cell_type":"markdown","metadata":{},"source":["# Rebalancing Dataset"]},{"cell_type":"markdown","metadata":{},"source":["## SMOTE"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from imblearn.over_sampling import SMOTEN\n","\n","X_train_resampled, y_train_resampled = SMOTEN().fit_resample(X_train, y_train)\n","if PARAM_SEARCH:\n","    gbc_smote = param_search()\n","else:\n","    gbc_smote = GradientBoostingClassifier(random_state=42, learning_rate=0.01, max_depth=3, n_estimators=300)\n","    gbc_smote.fit(X_train_resampled, y_train_resampled)\n","print_classification_metrics(X_test, y_test, gbc_smote)"]},{"cell_type":"markdown","metadata":{},"source":["## Balanced Bagging"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from imblearn.ensemble import BalancedBaggingClassifier\n","\n","def bagging_param_search(best_model):\n","    model = BalancedBaggingClassifier(estimator=best_model,\n","                                        sampling_strategy='auto',\n","                                        replacement=False,\n","                                        random_state=42,\n","                                        n_estimators=50,\n","                                        n_jobs=-1)\n","    param_grid = {\n","        'n_estimators': [50, 150, 300],\n","    }\n","\n","    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='precision', verbose=1, n_jobs=-1)\n","    grid_search.fit(X_train, y_train)\n","    print(f\"Best parameters found: {grid_search.best_params_}\")\n","    print(f\"Best precision score: {grid_search.best_score_}\")\n","\n","    best_model = grid_search.best_estimator_\n","    return best_model\n","\n","if PARAM_SEARCH:\n","    bbc = bagging_param_search(gbc)\n","else:\n","    bbc = BalancedBaggingClassifier(estimator=gbc,\n","                                    sampling_strategy='auto',\n","                                    replacement=False,\n","                                    random_state=42,\n","                                    n_estimators=150,\n","                                    n_jobs=-1)\n","    bbc.fit(X_train, y_train)\n","print_classification_metrics(X_test, y_test, bbc)"]},{"cell_type":"markdown","metadata":{},"source":["# Validation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SELECTED_MODEL = gbc"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def backtest_gbc(df, gbc_model, model_features=DIMREDUC_FEATURES):\n","    position = 0\n","\n","    df[META_LABEL] = gbc_model.predict(df[model_features])\n","    df['Ret'] = 0\n","    df['Closed'] = 0\n","    df['Position'] = 0\n","    df['Unreal_Ret'] = 0\n","    df['cRets'] = 0\n","    df['SCancelled'] = 0\n","\n","    for i, row in tqdm(df.iterrows(), desc=\"gbc_backtest\"):\n","        if ((row['SBS'] == -1 and position == 1) or \\\n","            (row['SSB'] == 1 and position == -1) or \\\n","            (position == 1 and row['Close'] <= entry * (1 - 0.1)) or \\\n","            (position == -1 and row['Close'] >= entry * (1 + 0.1))):\n","\n","            if position == 1:\n","                df.loc[i, 'Ret'] = (row['Close'] - entry) / entry\n","                df.loc[i, 'Closed'] = 1\n","            else:\n","                df.loc[i, 'Ret'] = (entry - row['Close']) / entry\n","                df.loc[i, 'Closed'] = -1\n","            position = 0\n","        elif ((row['SBS'] == 1 and position == 0) or (row['SSB'] == -1 and position == 0)):\n","            # it was cancelled.\n","            df.loc[i, 'SBS'] = df.loc[i, 'SSB'] = 0\n","\n","        if ((row['SB'] == 1 and position == 0) or (row['SS'] == -1 and position == 0)):\n","            if row[META_LABEL] :\n","                entry = row['Close']\n","                position = 1 if row['SB'] == 1 else -1\n","            else:\n","                df.loc[i, 'SB'] = df.loc[i, 'SS'] = 0\n","                df.loc[i, 'SCancelled'] = 1 # use LIME to understand\n","\n","        df.loc[i, 'Position'] = position\n","        if position != 0:\n","            # Unrealized for continuous returns tracking.\n","            df.loc[i, 'Unreal_Ret'] = (entry - row['Close']) / entry\n","\n","            if not row[META_LABEL] :\n","                if position == 1:\n","                    df.loc[i, 'Ret'] = (row['Close'] - entry) / entry\n","                    df.loc[i, 'Closed'] = 1\n","                    df.loc[i, 'SBS'] = 1\n","                else:\n","                    df.loc[i, 'Ret'] = (entry - row['Close']) / entry\n","                    df.loc[i, 'Closed'] = -1\n","                    df.loc[i, 'SSB'] = 1\n","                position = 0\n","\n","    df['cRets'] = (1 + df['Ret']).cumprod() - 1\n","    return df\n","\n","# Since we stacked the same timeseries, we can unstack to get the first actual future TS.\n","# Also descale it to compare, and rescale it for classification.\n","fut_df = futs_df.filter(regex=f\"{LEANHOG_FUT}_.*\")\n","fut_df.columns = fut_df.columns.str.replace(f\"{LEANHOG_FUT}_\", \"\", regex=False)\n","fut_df = pd.concat([fut_df, futs_exog_df], axis=1)\n","fut_df = fut_df.loc[:, ~fut_df.columns.duplicated(keep=\"first\")]\n","\n","single_test_df, scaler = augment_ts(fut_df.iloc[TRAIN_SIZE:], \"Close\", \"High\", \"Low\", \"Volume\", INTERVAL)\n","single_test_df = single_test_df.loc[:, ~single_test_df.columns.duplicated(keep=\"first\")]\n","print(single_test_df.shape)\n","\n","bt_df = backtest_gbc(single_test_df, SELECTED_MODEL, model_features=CLEAN_FEATURES)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot(bt_df):\n","    fig, axs = plt.subplots(4, gridspec_kw={'height_ratios': [4, 4, 1, 1]}, figsize=(18, 8))\n","\n","    buy_signals = bt_df[bt_df['SB'] > 0]\n","    sell_signals = bt_df[bt_df['SS'] < 0]\n","    long_closed = bt_df[bt_df['Closed'] > 0]\n","    short_closed = bt_df[bt_df['Closed'] < 0]\n","\n","    axs[0].plot(bt_df['Close'].index, bt_df['Close'], label=f'{AGRI_FUTS[0]} Price', color='blue')\n","    axs[0].scatter(buy_signals.index, buy_signals[f'Close'], color='green', marker='^', label='Buy', alpha =0.7)\n","    axs[0].scatter(sell_signals.index, sell_signals[f'Close'], color='red', marker='v', label='Sell', alpha =0.7)\n","    axs[0].scatter(long_closed.index, long_closed[f'Close'], color='green', marker='x', label='Buy Close', alpha =0.5)\n","    axs[0].scatter(short_closed.index, short_closed[f'Close'], color='red', marker='o', label='Sell Close', alpha =0.5)\n","    axs[0].set_title(f'{AGRI_FUTS[0]} Close Prices')\n","    axs[0].grid(True)\n","    axs[0].legend()\n","\n","    axs[1].plot(bt_df.index, bt_df['Filtered_X'], label='Spread', alpha=0.7, linestyle='--')\n","    axs[1].scatter(buy_signals.index, buy_signals[f'Filtered_X'], color='green', marker='^', label='Buy', alpha =0.7)\n","    axs[1].scatter(sell_signals.index, sell_signals[f'Filtered_X'], color='red', marker='v', label='Sell', alpha =0.7)\n","    axs[1].scatter(long_closed.index, long_closed[f'Filtered_X'], color='green', marker='x', label='Buy Close', alpha =0.5)\n","    axs[1].scatter(short_closed.index, short_closed[f'Filtered_X'], color='red', marker='o', label='Sell Close', alpha =0.5)\n","    axs[1].fill_between(bt_df.index,\n","                        bt_df['Filtered_X'] - bt_df['Uncertainty'],\n","                        bt_df['Filtered_X'] + bt_df['Uncertainty'],\n","                        label='Uncertainty', color=\"gray\", alpha=0.5)\n","    axs[1].axhline(y=1., color='black', alpha=0.7)\n","    axs[1].axhline(y=0.5, color='black', alpha=0.7)\n","    axs[1].axhline(y=0., color='black', alpha=0.7)\n","    axs[1].set_title(f'{TARGET_FUT} Actual vs Kalman Filtered Spread')\n","    axs[1].legend()\n","    axs[1].grid(True)\n","\n","    axs[2].plot(bt_df.index, bt_df['cRets'], label='Returns')\n","    axs[2].set_title(f'Cummulative Returns')\n","    axs[2].grid(True)\n","    axs[2].legend()\n","\n","    axs[3].scatter(bt_df.index, bt_df[META_LABEL], label='Signal')\n","    axs[3].set_title(f'Model Signal')\n","    axs[3].grid(True)\n","    axs[3].legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot(bt_df)"]},{"cell_type":"markdown","metadata":{},"source":["## Why Decisions?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["samples = single_test_df.iloc[460:480]\n","sample_cancelled = samples[samples[\"SCancelled\"] == 1][CLEAN_FEATURES]\n","samples = samples[CLEAN_FEATURES]\n","\n","print(sample_cancelled.shape)\n","sample_cancelled"]},{"cell_type":"markdown","metadata":{},"source":["# LIME Explanations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from lime.lime_tabular import LimeTabularExplainer\n","\n","explainer = LimeTabularExplainer(samples.values, feature_names=samples.columns, class_names=['RW', 'MR'], discretize_continuous=True)\n","exp = explainer.explain_instance(sample_cancelled.values[0], SELECTED_MODEL.predict_proba, num_features=len(single_test_df.columns))\n","exp.show_in_notebook(show_table=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Other Future"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":4755137,"sourceId":8061237,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
