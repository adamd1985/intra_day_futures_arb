{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"143f9c02-8d86-484a-b4c2-eb1317289f2a","_uuid":"068dcfb3-c368-468d-8410-aea88bc0b181","id":"oaDoHbxVH0CW"},"source":["# Statistical Algos"]},{"cell_type":"markdown","metadata":{"_cell_guid":"b5ba98a0-9590-4ccd-b238-cfae63d19770","_uuid":"6a6076dd-8ce5-47e2-8913-74dcaa2eacf0","id":"z_cBqdYOoY5S"},"source":["## Notebook's Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"44c8b09f-6f40-410d-aa3c-89b119fb2456","_uuid":"56c0c199-418e-4fa2-a71a-30d54c3a8b2c","collapsed":false,"id":"eETPYJLiMU-b","jupyter":{"outputs_hidden":false},"outputId":"49f77cf0-e6a3-44d8-9dae-05a929fa4804","trusted":true},"outputs":[],"source":["INSTALL_DEPS = False\n","if INSTALL_DEPS:\n","  %pip install matplotlib==3.8.3\n","  %pip installnumpy==1.26.4\n","  %pip installpandas==2.2.1\n","  %pip installpandas_market_calendars==4.4.0\n","  %pip installpytz==2024.1\n","  %pip installscipy==1.12.0\n","  %pip installta==0.11.0\n","  %pip installyfinance==0.2.37\n","\n","!python --version"]},{"cell_type":"markdown","metadata":{},"source":["## Cloud Environment Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cf2e55fb-0872-49df-ae06-aa49505f9474","_uuid":"ccc8fcee-37e2-48b5-8501-6285d13e13cd","collapsed":false,"id":"Q4-GoceIIfT_","jupyter":{"outputs_hidden":false},"outputId":"7dcb11f2-d20e-4714-e4fe-f9895dc22aac","trusted":true},"outputs":[],"source":["import os\n","import sys\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","IN_KAGGLE = IN_COLAB = False\n","try:\n","    # https://www.tensorflow.org/install/pip#windows-wsl2\n","    import google.colab\n","    from google.colab import drive\n","\n","    drive.mount(\"/content/drive\")\n","    DATA_PATH = \"/content/drive/MyDrive/EDT dataset\"\n","    MODEL_PATH = \"/content/drive/MyDrive/models\"\n","    IN_COLAB = True\n","    print(\"Colab!\")\n","except:\n","    IN_COLAB = False\n","if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and not IN_COLAB:\n","    print(\"Running in Kaggle...\")\n","    for dirname, _, filenames in os.walk(\"/kaggle/input\"):\n","        for filename in filenames:\n","            print(os.path.join(dirname, filename))\n","    MODEL_PATH = \"./models\"\n","    DATA_PATH = \"/kaggle/input/\"\n","    IN_KAGGLE = True\n","    print(\"Kaggle!\")\n","elif not IN_COLAB:\n","    IN_KAGGLE = False\n","    MODEL_PATH = \"./models\"\n","    DATA_PATH = \"./data/\"\n","    print(\"running localhost!\")"]},{"cell_type":"markdown","metadata":{},"source":["# Instruments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class YFinanceOptions:\n","    INDEX = \"Datetime\"\n","    MIN1_RANGE = 7 - 1\n","    MIN15_RANGE = 60 - 1\n","    HOUR_RANGE = 730 - 1\n","    DAY_RANGE = 7300 - 1\n","    D1=\"1d\"\n","    H1=\"1h\"\n","    M15=\"15m\"\n","    M1=\"1m\"\n","    DATE_TIME_FORMAT = \"%Y-%m-%d\"\n","    DATE_TIME_HRS_FORMAT = '%Y-%m-%d %H:%M:%S %Z'\n","\n","INTERVAL = YFinanceOptions.M15\n","\n","# Metals\n","GOLD_FUT = \"GC=F\"\n","SILVER_FUT = \"SI=F\"\n","COPPER_FUT = \"HG=F\"\n","PLATINUM_FUT = \"PL=F\"\n","PALLADIUM_FUT = \"PA=F\"\n","\n","METALS_FUTS = [GOLD_FUT, SILVER_FUT, COPPER_FUT, PLATINUM_FUT, PALLADIUM_FUT]\n","\n","# Energy\n","CRUDEOIL_FUT = \"CL=F\"\n","NATURALGAS_FUT = \"NG=F\"\n","HEATINGOIL_FUT = \"HO=F\"\n","RBOB_FUT = \"RB=F\"\n","\n","\n","ENERGY_FUTS = [CRUDEOIL_FUT, NATURALGAS_FUT, HEATINGOIL_FUT, RBOB_FUT]\n","\n","# Agri\n","CORN_FUT = \"ZC=F\"\n","SOYOIL_FUT = \"ZL=F\"\n","KCWHEAT_FUT = \"KE=F\"\n","SOYBEAN_FUT = \"ZS=F\"\n","SOYBEANMEAL_FUT = \"ZM=F\"\n","WHEAT_FUT = \"ZW=F\"\n","LIVECATTLE_FUT = \"LE=F\"\n","LEANHOG_FUT = \"HE=F\"\n","FEEDERCATTLE_FUT = \"GF=F\"\n","MILK_FUT = \"DC=F\"\n","\n","AGRI_FUTS = [CORN_FUT, SOYOIL_FUT, KCWHEAT_FUT, SOYBEAN_FUT, SOYBEANMEAL_FUT, WHEAT_FUT, LIVECATTLE_FUT, LEANHOG_FUT, FEEDERCATTLE_FUT, MILK_FUT]\n","\n","FUTS = AGRI_FUTS + METALS_FUTS + ENERGY_FUTS\n","\n","TARGET_FUT=KCWHEAT_FUT.replace(\"=F\", \"\")"]},{"cell_type":"markdown","metadata":{},"source":["## Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","filename = f\"{DATA_PATH}{os.sep}futures_{INTERVAL}.csv\"\n","print(filename)\n","futs_df = pd.read_csv(filename, index_col=\"Date\", parse_dates=True)\n","\n","print(futs_df.shape)\n","print(futs_df.columns)\n","futs_df.head(2)"]},{"cell_type":"markdown","metadata":{},"source":["# Problem Defintion\n","\n","Utilize meduim-frequency trade data for a set of 6 Future contracts listed on CME during a 250-day trading periods between 01/01/2023 and 01/01/2024. Future contracts are cross-sectional from metals, equities and volatiliyy. Trading is 23hours, with minute durations."]},{"cell_type":"markdown","metadata":{},"source":["## Intra-Day Mean Reversion\n","\n","https://learning.oreilly.com/library/view/algorithmic-trading-winning/9781118746912/OEBPS/9781118746912_epub_c02.htm#c02-sec1-0001\n","\n","Using Augmented Dickey-Fuller (ADF) for stationarity:\n","\n","$\\Delta y(t) = \\alpha + \\beta t + \\gamma y(t-1) + \\delta_1 \\Delta y(t-1) + \\cdots + \\delta_{p-1} \\Delta y(t-p+1) + \\epsilon_t$\n","\n","where:\n","- $\\Delta y(t) = y(t) - y(t-1)$ represents the first difference of the series.\n","- $y(t-1)$ is the lagged value of the series.\n","- $\\beta t$, often set to zero in price series analyses, accounts for any deterministic time trend.\n","- $\\delta_1, \\delta_2, ..., \\delta_{p-1}$ are coefficients for the lagged differences, adjusting for serial correlation.\n","- $\\epsilon_t$ is the error term.\n","\n","\n","The null hypothesis $H_0: \\gamma = 0$ suggests the presence of a unit root, indicating non-stationarity if the test statistic is $< 0.5$.\n","\n","$\\text{Test Statistic} = \\frac{\\hat{\\gamma}}{\\text{SE}(\\hat{\\gamma})}$\n","\n","where $\\text{SE}(\\hat{\\gamma})$ is the standard error of $\\hat{\\gamma}$. \n","\n","Given a significance of $\\alpha=0.05$, having critical values of $-2.86 \\text{ to} -3.45$.\n","- $\\text{SE} < 0 $ and $ p < 0.05 = \\text{Mean reverting}$\n","- $\\text{SE} > 0 $ and $ p < 0.05 = \\text{Trending}$\n","\n","From our EDA:\n","| Series     | Lag | Coefficient | P-Value          |\n","|------------|-----|-------------|------------------|\n","| NQ_Close   | 1   | 0.999967    | 0.000000e+00     |\n","| NQ_Close   | 5   | -0.017068   | 8.802106e-30     |\n","\n","A the 5 min lag, the series shows a **weak** mean-reversion $-0.017068 > -2.86$.\n","At the 1 min lag it trends, the AR is strong towards the direction.\n","\n","### Mean Reversion Probability"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nq_fut_df = futs_df[[f\"{TARGET_FUT}_Close\"]].copy()\n","nq_fut_df.index = pd.to_datetime(nq_fut_df.index)\n","nq_fut_df.sort_index(inplace=True)\n","\n","nq_fut_df.head(2)"]},{"cell_type":"markdown","metadata":{},"source":["### Ornstein-Uhlenbeck (OU) \n","\n","Typically, a Hurst exponent significantly different from 0.5 indicates long-term memory (H > 0.5) or anti-persistent behavior (H < 0.5).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from statsmodels.tsa.stattools import adfuller\n","from statsmodels.regression.linear_model import OLS\n","from statsmodels.tools.tools import add_constant\n","import statsmodels.api as sm\n","from hurst import compute_Hc\n","\n","NOISE_THRESHOLD = 5.\n","LAGS_IN_MINS =  [1]\n","\n","def get_ou(df, col):\n","    log_prices = np.log(df[col])\n","\n","    h, _, _ = compute_Hc(log_prices, kind='price', simplified=True)\n","    spread_lag = log_prices.shift(1).bfill()\n","\n","    spread_ret = (log_prices - spread_lag).bfill()\n","    spread_lag2 = sm.add_constant(spread_lag)\n","\n","    model = sm.OLS(spread_ret, spread_lag2)\n","    res = model.fit()\n","    hl = int(round(-np.log(2) / res.params[1], 0))\n","\n","    return hl, h\n","\n","HALF_LIFE, HURST = get_ou(nq_fut_df, f\"{TARGET_FUT}_Close\")\n","\n","print(\"Half-Life:\", HALF_LIFE)\n","print(\"Hurst:\", HURST)"]},{"cell_type":"markdown","metadata":{},"source":["The variance ratio (VR) test is used to determine if a time series follows a random walk and its scaling. The variance ratio is defined as:\n","\n","`z(t)` is the value of the time series at time \\( t \\).  \n","`z(t-\\tau)` is the value of the time series at time \\( t - \\tau \\), where \\( \\tau \\) is a lag.  \n","`Var(x)` denotes the variance of the variable \\( x \\).\n","\n","The formula is:\n","\n","$$\n","\\frac{\\text{Var}(z(t) - z(t-\\tau))}{\\tau \\, \\text{Var}(z(t) - z(t-1))}\n","$$\n","\n","- VRâ‰ˆ1: Consistent with a random walk.\n","- VR>1: Indicates positive serial correlation (momentum).\n","- VR<1: Indicates negative serial correlation (mean reversion).\n","\n","and test statistics:\n","- Is heteroskedasticity (variance changing over time).\n","- Is autocorrelation (returns at different times being correlated)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.stats import norm\n","\n","def var_ratio(df, col, k=100):\n","    # https://mingze-gao.com/posts/lomackinlay1988/#source-code\n","    log_prices = np.log(df[col])\n","\n","    rets = np.diff(log_prices)\n","    T = len(rets)\n","    mu = np.mean(rets)\n","    var_1 = np.var(rets, ddof=1, dtype=np.float64)\n","    rets_k = (log_prices - np.roll(log_prices, k))[k:]\n","    m = k * (T - k + 1) * (1 - k / T)\n","    var_k = 1/m * np.sum(np.square(rets_k - k * mu))\n","\n","    # Variance Ratio\n","    vr = var_k / var_1\n","    # Phi1\n","    phi1 = 2 * (2*k - 1) * (k-1) / (3*k*T)\n","    # Phi2\n","\n","    def delta(j):\n","        res = 0\n","        for t in range(j+1, T+1):\n","            t -= 1  # array index is t-1 for t-th element\n","            res += np.square((rets[t]-mu)*(rets[t-j]-mu))\n","        return res / ((T-1) * var_1)**2\n","\n","    phi2 = 0\n","    for j in range(1, k, 2):\n","        phi2 += (2*(k-j)/k)**2 * delta(j)\n","\n","    # Test statistics\n","    ts1 = (vr - 1) / np.sqrt(phi1)\n","    ts2 = (vr - 1) / np.sqrt(phi2)\n","\n","    # P-values\n","    p_value1 = 2 * (1 - norm.cdf(np.abs(ts1)))  # two-tailed test\n","    p_value2 = 2 * (1 - norm.cdf(np.abs(ts2)))  # two-tailed test\n","\n","    return vr, ts1, p_value1, ts2, p_value2\n","\n","vr, ts1, p_value1, ts2, p_value2 = var_ratio(nq_fut_df, f\"{TARGET_FUT}_Close\")\n","print(f\"Variance Ratio: {vr}\")\n","print(f\"Test Statistic 1: {ts1} at P-Value 1: {p_value1:.12f}\")\n","print(f\"Test Statistic 2: {ts2} at P-Value 2: {p_value2:.12f}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Probabilities of Reversion"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for lag in LAGS_IN_MINS:\n","    trend_col = f'T_{lag}M'\n","    mr_col = f'MR_{lag}M'\n","\n","    nq_fut_df[trend_col] = nq_fut_df[f'{TARGET_FUT}_Close'].diff(lag).bfill()\n","    nq_fut_df[trend_col] = nq_fut_df[trend_col].apply(lambda x: np.sign(x))\n","\n","    nq_fut_df[mr_col] = (nq_fut_df[trend_col] != nq_fut_df[trend_col].shift(lag).bfill()).astype(int)\n","\n","nq_fut_df.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mr_proba = {}\n","for col in nq_fut_df.columns:\n","    if col.startswith('MR_'):\n","        mr_proba[col] = nq_fut_df[col].mean()\n","\n","probabilities_df = pd.DataFrame(list(mr_proba.items()), columns=['Lag', 'Probability'])\n","probabilities_df['Lag'] = probabilities_df['Lag'].str.extract('(\\d+)').astype(int)\n","probabilities_df.sort_values('Lag', inplace=True)\n","probabilities_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["MAX_TRENDS = 15\n","revert_proba = {}\n","for lag in LAGS_IN_MINS:\n","    trend_reversion_prob = {}\n","    trend_col = f'T_{lag}M'\n","    mr_col = f'MR_{lag}M'\n","\n","    for x in range(0, MAX_TRENDS + 1):\n","        rolling_trends = nq_fut_df[trend_col].rolling(window=x).sum() == x\n","        still_trending = rolling_trends.shift(1)\n","\n","        reverted = nq_fut_df[mr_col]\n","        reverting_after_trends = (reverted & still_trending)\n","        total_trends = still_trending.sum()\n","\n","        if total_trends > 0:\n","            trend_reversion_prob[x] = reverting_after_trends.sum() / total_trends\n","\n","    revert_proba[lag] = trend_reversion_prob\n","\n","assert len(revert_proba) > 0\n","\n","rows = []\n","for lag, probs in revert_proba.items():\n","    for trend, probability in probs.items():\n","        rows.append({'Lag': lag, 'Trends': trend, 'Probability': probability})\n","\n","temp_df = pd.DataFrame(rows)\n","pivot_df = temp_df.pivot(index='Trends', columns='Lag', values='Probability')\n","pivot_df = pivot_df.reindex(sorted(pivot_df.columns, key=int), axis=1)\n","pivot_df.T"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","pivot_df.plot(kind='line', marker='o', figsize=(8, 4))\n","\n","plt.title('Mean Reversion Probability after X Trends', fontsize=16)\n","plt.xlabel('X Trends', fontsize=14)\n","plt.ylabel('Mean Reversion %', fontsize=14)\n","plt.legend(title='Lag (in minutes)', fontsize=12, title_fontsize=14)\n","plt.grid(True)\n","plt.xticks(range(1, MAX_TRENDS + 1))\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.stats import norm\n","\n","distribution_params = pd.DataFrame(columns=['Lag', 'Mean', 'Std'])\n","for lag in pivot_df.columns:\n","    data = pivot_df[lag].dropna()\n","    if not data.empty:\n","        mu, std = norm.fit(data)\n","        new_row = pd.DataFrame({\n","            'Lag': [lag],\n","            'Mean': [mu],\n","            'Std': [std]\n","        })\n","        distribution_params = pd.concat([distribution_params, new_row], ignore_index=True)\n","\n","        plt.figure(figsize=(8, 4))\n","        plt.hist(data, bins=10, density=True, alpha=0.6, color='g')\n","        xmin, xmax = plt.xlim()\n","        x = np.linspace(xmin, xmax, 100)\n","        p = norm.pdf(x, mu, std)\n","        plt.plot(x, p, 'k', linewidth=2)\n","        plt.title(f\"Gaussian for Lag {lag}: mean = {mu:.2f}, std = {std:.2f}\")\n","        plt.show()\n","\n","distribution_params"]},{"cell_type":"markdown","metadata":{},"source":["# Bollinger Bands"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tqdm import tqdm\n","import itertools\n","import math\n","\n","def calculate_std_factor(hurst, base_std=2.0, adjustment=0.5):\n","    if hurst < 0.5:\n","        # Reverting, increase this band\n","        return base_std + (0.5 - hurst) * adjustment\n","    elif hurst > 0.5:\n","        # Trending, decrease band\n","        return base_std - (hurst - 0.5) * adjustment\n","    else:\n","        return base_std\n","\n","def kalman_filter_step(price, m_prev, R_prev, delta, ve):\n","    m_pred = m_prev\n","    R_pred = R_prev + delta\n","\n","    K = R_pred / (R_pred + ve)\n","    m_curr = m_pred + K * (price - m_pred)\n","    R_curr = (1 - K) * R_pred\n","\n","    return m_curr, R_curr\n","\n","def bollinger_band_backtest(df, target_col, window, std_factor, delta=1e-3, ve=1e-2):\n","    n = len(df)\n","    m = np.zeros(n)\n","    R = np.zeros(n)\n","\n","    initial_prices = df[target_col].values[:window]\n","    m[:window] = initial_prices.mean()\n","    R[:window] = initial_prices.var()\n","    for t in range(window, n):\n","        m[t], R[t] = kalman_filter_step(df[target_col].values[t], m[t-1], R[t-1], delta, ve)\n","\n","    df['MA'] = m\n","    df['SD'] = np.sqrt(R)\n","\n","    df['U'] = df['MA'] + (df['SD'] * std_factor)\n","    df['L'] = df['MA'] - (df['SD'] * std_factor)\n","\n","    df['SB'] = (df[target_col] < df['L']).astype(int).diff().clip(0) * +1\n","    df['SS'] = (df[target_col] > df['U']).astype(int).diff().clip(0) * -1\n","    df['SBS'] = (df[target_col] > df['MA']).astype(int).diff().clip(0) * -1\n","    df['SSB'] = (df[target_col] < df['MA']).astype(int).diff().clip(0) * +1\n","\n","    df['Closed'] = 0\n","    df['Position'] = 0\n","    df['Ret'] = 0.\n","    entry = position = 0\n","\n","    for i, row in df.iterrows():\n","        if (row['SBS'] == -1 and position == 1) or (row['SSB'] == 1 and position == -1):\n","            if position == 1:\n","                df.loc[i, 'Ret'] = (row[target_col] - entry) / entry\n","                df.loc[i, 'Closed'] = 1\n","            else:\n","                df.loc[i, 'Ret'] = (entry - row[target_col]) / entry\n","                df.loc[i, 'Closed'] = -1\n","            position = 0\n","\n","        if (row['SB'] == 1 and position == 0) or (row['SS'] == -1 and position == 0):\n","            entry = row[target_col]\n","            position = 1 if row['SB'] == 1 else -1\n","        df.loc[i, 'Position'] = position\n","\n","    df['cRets'] = (1 + df['Ret']).cumprod() - 1\n","\n","    variance = df['Ret'].var()\n","    df['Drawdown'] = (1 + df['Ret']).cumprod().div((1 + df['Ret']).cumprod().cummax()) - 1\n","    max_drawdown = df['Drawdown'].min()\n","    drawdown_length = (df['Drawdown'] < 0).astype(int).groupby(df['Drawdown'].eq(0).cumsum()).cumsum().max()\n","    negative_returns = df['Ret'][df['Ret'] < 0]\n","    sortino_ratio = df['Ret'].mean() / negative_returns.std() * np.sqrt(24192)  # 60/15 * 24 * 252 Intervals\n","    trades = (df['Position'].diff().ne(0) & df['Position'].ne(0)).sum()\n","\n","    stats_df = pd.DataFrame({\n","        \"Window\": [window],\n","        \"Standard_Factor\": [std_factor],\n","        \"Cumulative_Returns\": [df['cRets'].iloc[-1]],\n","        \"Max Ret\": [df['Ret'].max()],\n","        \"Max Loss\": [df['Ret'].min()],\n","        \"Variance\": [variance],\n","        \"STD\": [np.sqrt(variance)],\n","        \"Max_Drawdown\": [max_drawdown],\n","        \"Drawdown_Length\": [drawdown_length],\n","        \"Sortino_Ratio\": [sortino_ratio],\n","        \"Trades_Count\": [trades],\n","        \"Trades_per_Interval\": [trades / len(df)],\n","        \"Trading_Intervals\": [len(df)],\n","    })\n","\n","    return df, stats_df\n","\n","def param_search_bbs(df, target_col, initial_window=HALF_LIFE, window_factor = 1.5, window_min = 4, intial_std_adjustment=0.5, hurst=HURST):\n","    assert initial_window > window_min\n","\n","    num_steps = int(math.log(initial_window / window_min, window_factor)) + 1\n","    windows = [int(initial_window // (window_factor**i)) for i in range(num_steps)]\n","    std_adjustments = [intial_std_adjustment/2, intial_std_adjustment, intial_std_adjustment * 1.5, intial_std_adjustment * 2]\n","    combinations = list(itertools.product(windows, std_adjustments))\n","\n","    best_sortino = -float('inf')\n","    best_sortino_stats = None\n","    best_rets = -float('inf')\n","    best_rets_stats = None\n","    best_mdd = -float('inf')\n","    best_mdd_stats = None\n","\n","    for window, adjustment in tqdm(combinations, desc=\"param_search_bbs\"):\n","        std_factor = calculate_std_factor(hurst, intial_std_adjustment, adjustment)\n","        _, stats_df = bollinger_band_backtest(df, target_col, window,std_factor)\n","\n","        stat = stats_df['Sortino_Ratio'].iloc[0]\n","        if stat > best_sortino:\n","            best_sortino = stat\n","            best_sortino_stats = stats_df\n","\n","        stat = stats_df['Cumulative_Returns'].iloc[0]\n","        if stat > best_rets:\n","            best_rets = stat\n","            best_rets_stats = stats_df\n","\n","        stat = stats_df['Max_Drawdown'].iloc[0]\n","        if stat > best_mdd:\n","            best_mdd = stat\n","            best_mdd_stats = stats_df\n","\n","    results_df = pd.concat([best_sortino_stats.assign(Metric='Sortino'),\n","                            best_rets_stats.assign(Metric='Cumulative Returns'),\n","                            best_mdd_stats.assign(Metric='Max Drawdown')],\n","                           ignore_index=True)\n","\n","    return results_df\n","\n","\n","stats_df = param_search_bbs(futs_df, f'{TARGET_FUT}_Close', initial_window=HALF_LIFE, hurst=HURST)\n","cumret_df= stats_df[stats_df[\"Metric\"] == \"Cumulative Returns\"]\n","BEST_WINDOW = cumret_df[\"Window\"].values[0]\n","BEST_STD_FACTOR = cumret_df[\"Standard_Factor\"].values[0]\n","\n","stats_df"]},{"cell_type":"markdown","metadata":{},"source":["## Results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["backtest_df = futs_df[[f'{TARGET_FUT}_Close']].copy()\n","backtest_df, stats_df = bollinger_band_backtest(backtest_df, f'{TARGET_FUT}_Close', BEST_WINDOW, BEST_STD_FACTOR)\n","backtest_df['cRets'] = (1 + backtest_df['Ret']).cumprod() - 1\n","print(f\"Cumulative returns from the strategy: {backtest_df['cRets'].iloc[-1]*100.:.02f}%\")\n","\n","# Visualization should be easier\n","backtest_df = backtest_df.tail(300)\n","\n","plt.figure(figsize=(18, 8))\n","buy_signals = backtest_df[backtest_df['SB'] > 0]\n","sell_signals = backtest_df[backtest_df['SS'] < 0]\n","long_closed = backtest_df[backtest_df['Closed'] > 0]\n","short_closed = backtest_df[backtest_df['Closed'] < 0]\n","\n","ax1 = plt.subplot2grid((6, 1), (0, 0), rowspan=4, colspan=1)  # Main plot gets more space\n","ax1.plot(backtest_df[f'{TARGET_FUT}_Close'], label=f'{TARGET_FUT} Close', color='blue', alpha=0.6, linestyle='--')\n","ax1.plot(backtest_df['MA'], label='Moving Average', color='red', linestyle='-.')\n","ax1.plot(backtest_df['U'], label='Upper Bollinger Band', color='green')\n","ax1.plot(backtest_df['L'], label='Lower Bollinger Band', color='green', alpha=0.7)\n","ax1.scatter(buy_signals.index, buy_signals[f'{TARGET_FUT}_Close'], color='green', marker='^', label='Buy')\n","ax1.scatter(sell_signals.index, sell_signals[f'{TARGET_FUT}_Close'], color='red', marker='v', label='Sell')\n","ax1.scatter(long_closed.index, long_closed[f'{TARGET_FUT}_Close'], color='green', marker='x', label='Buy Close')\n","ax1.scatter(short_closed.index, short_closed[f'{TARGET_FUT}_Close'], color='red', marker='o', label='Sell Close')\n","\n","ax1.set_title(f'Bollinger Bands for {TARGET_FUT} Close')\n","ax1.set_xlabel('Date')\n","ax1.set_ylabel('Price')\n","ax1.legend()\n","ax1.grid(True)\n","\n","ax2 = plt.subplot2grid((6, 1), (4, 0), rowspan=2, colspan=1, sharex=ax1)\n","ax2.plot(backtest_df['cRets'], label='Cumulative rets', color='purple')\n","ax2.set_title('Cumulative rets')\n","ax2.set_xlabel('Date')\n","ax2.set_ylabel('Cumulative rets')\n","ax2.grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Test all Futs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_metrics = pd.DataFrame()\n","for fut in FUTS:\n","    target=fut.replace(\"=F\", \"\")\n","\n","    fut_df = futs_df[[f\"{target}_Close\"]].copy()\n","    hl, h = get_ou(fut_df, f\"{target}_Close\")\n","    stats_df = param_search_bbs(fut_df, f'{target}_Close', initial_window=hl, hurst=h)\n","\n","    all_metrics = pd.concat([all_metrics, stats_df.assign(Fut=target, Hurst=h, Halflife=h)], ignore_index=True)\n","\n","    mr_metrics = all_metrics[all_metrics.Hurst < 0.5]\n","    mr_metrics = mr_metrics.sort_values(by=\"Cumulative_Returns\", ascending=False)\n","    mr_metrics[[\"Cumulative_Returns\", \"Window\",\"Standard_Factor\",\"Metric\",\"Fut\"]]"]}],"metadata":{"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"datasetId":4755137,"sourceId":8061237,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
